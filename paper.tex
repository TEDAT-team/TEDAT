\documentclass[sigconf]{acmart}

\usepackage[english]{babel}
\usepackage{blindtext}


% Copyright
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

\usepackage{subfigure}
 \usepackage{tabulary}
  \usepackage{array,booktabs,longtable,tabularx}

% \usepackage{tabulary}
%   \usepackage{array,booktabs,longtable,tabularx}
%   \usepackage{multirow} 
% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

% inlined bib file
\usepackage{filecontents}
\usepackage{multirow}

\usepackage{pifont}
\usepackage{amssymb}
\newtheorem{definition}{\noindent{\bf Definition}}
%\newtheorem{lemma}{\noindent{\bf Lemma}}
\newtheorem{theorem}{\noindent{\bf Theorem}}
%\newtheorem{proposition}{\noindent{\bf proposition}}
\usepackage{appendix}
\usepackage[ruled, vlined , linesnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{amsmath,bm}
\usepackage{enumerate}
\usepackage{footnote}
\makesavenoteenv{table}

%\newcommand{\ballnumber}[1]{\tikz[baseline=(myanchor.base)] \node[circle,fill=.,inner sep=1pt] (myanchor) {\color{-.}\bfseries\footnotesize #1};}
\settopmatter{printacmref=false, printccs=false, printfolios=true}

% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
\acmConference[Submitted to SIGCOMM]{}
\acmYear{2021}
%\copyrightyear{}

%% {} with no args suppresses printing of the price
\acmPrice{}

\setlength{\textfloatsep}{3.8pt}

\begin{document}
\title{\bf $\mathsf{BATE}$: Boosting Bandwidth Availability Over Inter-DC WAN}

%\titlenote{Produces the permission block, and copyright information}
%\subtitle{Extended Abstract}

\author{Paper \# 56, 17 pages}
% \author{Firstname Lastname}
% \authornote{Note}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{Affiliation}
%   \streetaddress{Address}
%   \city{City} 
%   \state{State} 
%   \postcode{Zipcode}
% }
% \email{email@domain.com}

% The default list of authors is too long for headers}
%\renewcommand{\shortauthors}{X.et al.}

\begin{abstract}
Inter-DataCenter Wide Area Network (Inter-DC WAN) that connects geographically distributed data centers is becoming one of the most critical network infrastructures.
%Network failures are common in WANs, so the provision of high availability has been a major focus of network research.
Due to the limited bandwidth resources and inevitable link failures, it is highly challenging to guarantee network availability for services, especially those with stringent bandwidth demands, over inter-DC WAN.
%While modern datacenter services increasingly call for high availability, network failures in WANs remain a fairly common phenomenon.
%Most traffic engineering schemes only consider connectivity-based availability and this severely degrades quality of service, thus revenue.
We present $\mathsf{BATE}$,
a novel Traffic Engineering (TE)  framework that 
%contains admission control, online scheduling and failure recovery.
aims for \textit{bandwidth availability} (BA) provision, where a service level agreement (SLA) defines that a demand on certain bandwidth should be satisfied with a stipulated probability,  when subjected to the network capacity and possible failures of the inter-DC WAN.
%Also, $\mathsf{BATE}$ is able to maximize revenue when network fails.
%$\mathsf{BATE}$ tries to guarantee the bandwidth-based availability of demands . 
%$\mathsf{BATE}$ generates a probabilistic failure model from empirical data and maximizes the profit that can be achieved under certain constrains on  bandwidth-based availability. 
%For the failure model, we propose a pruning algorithm to reduce the problem size.
%For the optimization problem, we prove it is NP-hard and  propose a greedy algorithm to efficiently solve it with only minor profit loss. 
%We design and implement an SDN-based system named $\mathsf{Dionysus}$ to deploy $\mathsf{BATE}$ over inter-DC WAN.
The three core components of  $\mathsf{BATE}$, i.e., admission control, traffic scheduling and failure recovery, are built on different mathematical models and theoretically analyzed. 
They are also extensively compared against state-of-the-art TE schemes, using testbed as well as trace driven simulations across different topologies, traffic matrices and failure scenarios. 
Our evaluations on real network topologies and traces show that, compared with the optimal admission strategy, $\mathsf{BATE}$ can speed up the online admission control by 30$\times$ at the expense of less than 4\% false rejections. 
On the other hand, compared with the latest TE schemes like FFC and TEAVAR, $\mathsf{BATE}$ can meet the bandwidth availability SLAs for 23\%$\sim$60\% more demands under normal loads, and when network failure causes SLA violations, it can retain 10\%$\sim$20\% more profit under a simple pricing and refunding model.
%.
% $\mathsf{BATE}$ can 
%(1) achieve 30 $\times$ speedup in admission control time at the expense of sacrificing 8\% in performance compared to the optimal stragety;
%(2) increase the percentage of demands that satisfy their availability SLAs by up to 40\%;
%(3) hold 30\% more profit for the provider when network fails. 
%The evaluation results demonstrate that $\mathsf{BATE}$ can make 20\%, 25\%, 30\%, 40\% more flows satisfy their bandwidth-based availability demands  than TEAVAR, SMORE, SWAN and FFC, respectively.
\end{abstract}

\maketitle

\section{Introduction}
Nowadays, large scale online services such as finance trading, web search, online shopping, online game and video streaming are posing stringent requirements on the availability and agility of the underlying network infrastructure, where Inter-DataCenter Wide Area Network (Inter-DC WAN) that
connects geographically distributed data centers has been playing a critical role. 
Many service providers, including Amazon, Google, Microsoft, etc., are providing various  optimizations for their global WAN, especially with the help of the emerging software-defined networking techniques \cite{swan,hong2018b4,evole,Teavar,calendaring,FFC,bwe,dynamic,SMORE,B4,OWAN}.  


%In modern data centers, the delay-sensitive interactive services, elastic services and background services share WAN networks\cite{swan}.
%Providing uninterruptible network service is becoming increasingly important as customers are becoming fastidious.

%Recently, deploying multiple virtual private clouds over public clouds (e.g., Amazon AWS) or deploying multiple services over private clouds (e.g., Google internal DCs) has become a common practice for tenants or content providers  \cite{Guaranteeings}.
%Services have different bandwidth demands and availability demands \cite{B4,hong2018b4,bwe}.
%%For example, a common practice is running MapReduce operations across geo-distributed data centers and many connections from mappers to reducers are maintained.
%%These instances belonging to diverse service%Tenants might launch multiple services' instances with different bandwidth demands and each service have different availability targets.
%Maintaining high availability has been, and continues to be a major focus of inter-DC WAN as customers are becoming more fastidious.
Among various optimization targets, high network availability has been, and will continue to be a major focus. On the one hand, it supports critical uninterrupted services and satisfies fastidious users, while on the other hand, it helps to build a good reputation and improves the competitiveness of network providers. However, guaranteeing network availability for services, especially those with stringent bandwidth demands, over inter-DC WAN is very challenging, 
since failures may arise from various network components, from data plane to control plane, and could happen anytime \cite{evole,california,understanding}.
For example, Microsoft reports links in their WAN could fail as often as every 30 minutes \cite{FFC}.
Once a link fails, traffic has to be rescaled and rerouted, resulting in transit or long lasting congestions. 
%the ingress switch normally rescales traffic to the remaining tunnels and could cause transit sever congestion.
%Flows can be routed on multiple paths from source to the destination over inter-DC WAN and application performance is susceptible to network failures.
%Although switches can quickly increase the rate of other surviving ones to ensure the resource provisions,  some links might get more bandwidth than their capacity, which will lead to congestion and jeopardize applications.
%While centralized traffic engineering is effective, it is unable to quickly react to faults.
%To mitigate performance loss due to network failures, TE controller can be employed  to reconfigure the networks, which is 
%However, it takes at least several seconds to update all rules after failures happen\cite{FFC},
%which is too much for high-availability service as congestion may have already started.
%The reactive interventions take too much time and the network might have already experienced congestion on failures.
Such negative impacts on inter-DC WAN services will ultimately translate into monetary loss (e.g., more refund to customers in the short term, and low customer stickness in the long term).
At the same time, as more businesses move to cloud, 
%bandwidth is becoming a more valuable and congested resource, 
there are inevitable competitions over the scarce inter-DC WAN bandwidth  \cite{swan,Guaranteeings,calendaring}.
Therefore, the design and optimization of inter-DC WANs have to take competitions, heterogeneities and economic interests into consideration.
%Therefore,
%we take both network risk and service competitions into WAN resource optimization and jointly maximize bandwidth-based availability of requests belonging to diverse services in this paper .
%Operators actually have high insight into applications bandwidth and availability demands.
%Apart from performance requirements, such as throughput and latency \cite{appdriven,tail},
%applications also have availability demands.
%SWAN\cite{swan} allocates bandwidth in strict precedence across the priority of flows.
%The delay-sensitive services can be efficiently scheduled, but they are susceptible to performance loss when there are faults.
%FFC\cite{FFC} guarantees that applications are freedom from congestion under arbitrary combinations of up to $k$ network faults.
%%To accomplish this, FFC keeps link utilization extremely low and applications have poor performance when no failures happen. 
%TEAVAR\cite{Teavar} grasps the probability of failure scenarios and maximizes the bandwidth utilization under the operator-specified available target constraint.
%tries to strike the right utilization-availability balance and maximizes the bandwidth of users subject an operator-specified available target.
%Although some of them, such as FFC\cite{FFC} and TEAVAR\cite{Teavar},  even try to make applications free from congestion when more than one link/node fails simultaneously, 

In this paper, we argue that although existing traffic engineering schemes \cite{FFC,Teavar,R3,swan,PCF,B4,SMORE} have already factored in network risks and aimed for network availability guarantee, they cannot meet the above objectives due to three limitations:
% when facing heterogeneous and competing bandwidth demands over inter-DC WAN:
\textit{First}, most of them \cite{FFC,R3,PCF,Teavar,SMORE} typically make a conservative bandwidth allocation, so that even if a failure occurs, surviving paths could be used and the network can still be free from congestion under traffic rerouting.
%when there are failures, and consequently their bandwidth allocation can often be too conservative \cite{FFC,R3,PCF} and result in inefficient resource usage as well as significant performance gap \cite{failureslarge,Teavar}.
To prevent congestion, links, including those with negligible failure probabilities, must be kept at low utilization, 
resulting in significant waste of network bandwidth (and potentially less accommodated users). 
Such a solution may be fit for existing ISP networks which use over provision to avoid congestion, but for new players, such as content providers that are building their own backbone network (either physically \cite{B4,swan,amazon,evole} or leasing bandwidth from ISPs \cite{cato, aryaka}), this is quite uneconomic \cite{publicore}. 
%and this method is suited for ISP networks which are designed with worst-case assumptions about failures.
%However, content providers can't invest so much in their private backbone (e.g., inter-DC WAN) as public backbone to reduce the risk of congestion  \cite{publicore},
%However, the bandwidth over private backbone (e.g., inter-DC WAN) faces hard provisioning choices, in terms of 
%faces
%hard provisioning choices, in terms of how much bandwidth to buy at
%any particular time: buy too much and they have wasted money; but
%buy too little and they run the risk of congestion on their backbone \cite{publicore}.
%therefore, leaving too much capacity unused most of the time when no link fails over inter-DC WAN implies resource wastage.
%However, bandwidth over inter-DC WAN is expensive and not as sufficient as backbone network \cite{B4, swan,Teavar}, %To prevent congestion when redistributing traffic, links are always kept low utilization.
%However, some links have negligible failure probability  \cite{failureslarge,Teavar}, and lowing their utilization to protect routing is needless and will degrade application performance.
%Firstly, most of them only consider \textit{connectivity-based} availability\cite{hong2018b4}, which is conservative.
%Connectivity-based availability is defined as the service uptime percentage, where a given minute is considered up if the connectivity is successful.
%The connectivity-based availability is insufficient since it only emphasizes the success of connectivity but can't guarantee applications' achievable performance,
%as a result, user experience might be influenced.
%Nowadays, with the development of online service, users' expectations of high quality of experience (e.g., high resolution video, low latency) are continuously improving.
%We show that only considering connectivity in traffic engineering is insufficient since QoE would greatly drop for some online service (e.g., online video) when minimal bandwidth can't be guaranteed (see $\S$\ref{background}). 
\textit{Second}, existing techniques mainly focus on the availability of the whole network, but ignore that users not only have diverse demands on bandwidth,  but also ask for different levels of reliability. Providing reliable bandwidth can be a value-added service for many cloud providers, typically in the form of 
%(e.g., TEAVAR \cite{Teavar}) usually emphasize on network service availability rather than the availability targets of applications deployed over networks, i.e., they normally take a \textit{one-size-fit-all} approach and do not differentiate availability requirements of different classes of traffic, which may impair high availability traffic when there are competitions.
%they don't differentiate diverse service availability demands from different classes of traffic.
%Availability is one of the main items of 
Service Level Agreements (SLAs) \cite{amazon,azure}.
%and cloud providers have to refund customers for any violation of SLAs.
%In reality, availability demands vary with different services.
%In reality, the refunding functions are often staged and differ depending on the type of service \cite{azure}.
For example, Microsoft Azure guarantees its customers at least 99.9\% availability for its backup service and 99.95\% availability for its ExpressRoute service \cite{azure}.
If the availability agreement is violated, a 10\% or 25\% refund will be returned to the customers.
A \textit{one-size-fit-all} approach (e.g., TEAVAR \cite{Teavar})  ignoring these heterogeneities cannot support such SLAs well, and may even hurt critical and uninterruptible applications when there are competitions on bandwidth. 
%For example, the availability demand of search index copies is 99\%, while DNS service is 99.99\% over B4\cite{hong2018b4}.
%TEAVAR \cite{Teavar} can't guarantee the availability of high priority traffic when resource is inadequate.
% that are oblivious of such diverse availability requirements can be either too expensive or insufficient for different services.
\textit{Third}, 
such heterogeneities and competitions are also not considered by current failure recovery approaches, especially those who allocate bandwidth aggressively \cite{Teavar,swan}, since they may run into congestions when traffic is rerouted under network failures. 
Such violations of SLAs will inevitably cause revenue loss, which should be kept as small as possible. 

%their failure recovery schemes might cause much monetary loss when network fails.
%In tunnel-based traffic engineering schemes (e.g., TEAVAR \cite{Teavar}, FFC \cite{FFC}),
%traffic can be redistributed across the surviving tunnels via rerouting when any tunnel becomes unavailable.
%Their rerouting mechanisms via surviving tunnels could lead to much revenue loss when services attempt to restore rate after failures.


%Although they consider rerouting of data plane, their random re-hashing mechanisms fail to capture link utilization and service importance, which could lead to congestion and monetary loss.
%WAN bandwidth remains a constrained resource and schemes (e.g., TEAVAR \cite{Teavar}) ignore diverse availability demands ,
%as a result, services with small availability demands will gain redundant bandwidth and affect other services' performance finally.
%Thirdly, they fail to consider the priority/urgency of different services.
%Thirdly, they lack fine grained priority description 
%In data centers, services have different urgency levels.
%For instance, the event service and the vRoute service, responsible for communication of critical components,  are of the highest level of importance, while data-backup and data-distribution are services running in the background and are of the lowest level of importance \cite{hanzhang}.\
%Although some schemes, such as SWAN \cite{swan}, allocate bandwidth in strict precedence across a small number of priority classes,
%they are too coarse and fine-grained priority description is needed. 
To solve these challenges, in this paper we make the following  three \textbf{contributions}:



Firstly, we advocate traffic engineering with \textit{bandwidth availability} (\textit{BA}) provision: 
 a BA demand $d=(b_d, \beta_d, t_d^s, t_d^e)$ means that  $d$ requests bandwidth $b_d$ for a life duration between $ t_d^s$ and $t_d^e$, and should be guaranteed at least $\beta_d\%$ of the duration, subjected to the network capacity and possible failures. Such a demand is typically represented by a Service Level Agreement (see Table \ref{target} for real world examples), and different users or applications may pose different demands. 
%a tenant's \textit{promised bandwidth of service $j$ should be available at least $\beta_j\%$ time}.  
%It tries to guarantee the bandwidth demands from tenants under network uncertainty.
We show that state-of-the-art traffic engineering schemes fail to meet the heterogeneous bandwidth  availability demands, especially under diverse link failure probabilities that may vary by several orders of magnitude (see $\S$\ref{background}). We note that, although the general concept of bandwidth-based availability has been recognized in some recent TE works \cite{hong2018b4,bwe, B4}, their methodologies and evaluations are actually achieving only a soft guarantee, i.e., a high ratio of the allocated bandwidth to the negotiated one, while we will provide a hard guarantee, i.e., the negotiated bandwidth must be met.


%meet services' bandwidth-based availability targets under arbitrarily link failures (see $\S$\ref{background}).
%This is due to the fact that these schemes either ignore the difference in link failure probabilities that can vary by several orders of magnitude in practice or fail to consider the diverse availability targets of services.
%The stronger availability can impel network to provide stable services.
%Compared with the connectivity-based availability, it makes a further step and tries to guarantee the minimal bandwidth demands under network uncertainty after the success of connections.
%
%Taking the diverse availability demands from different applications and the application importance into consideration, 

Secondly, we design $\mathsf{BATE}$,
a novel traffic engineering  framework that aims for bandwidth availability provision over inter-DC WAN (see $\S$\ref{HATE}).
$\mathsf{BATE}$ is composed of three core components, i.e., admission control, traffic scheduling and failure recovery. 
%The admission control scheme 
%i.e., intent  "service $j$ is able to gain $d_j$ network bandwidth at least $\beta_j\%$ of the time" can be realized , where $\beta_j\%$ is the availability target promised in the SLA of service $j$.
%$\mathsf{BATE}$ framework contains three parts, i.e., admission control, online scheduling and failure recovery.
The admission control step strikes a balance between efficiency and optimality, so that new demands can be admitted and guaranteed as much as possible with negligible delay.
%and our algorithm strikes a good tradeoff between efficiency and optimality.
%After a tenant submits a demand, the admission control will check whether the request can be admitted or not in near real time fashion. 
%We prove that for demands already admitted by our algorithm, there must exist an allocation scheme to satisfy their bandwidth-based availability targets.
%If the demand is admitted, the system will pre-allocate resource to the tenant and pass it to Online Sched- uler module, while the rejected demands can be resubmitted by the tenants latter. The Online Scheduler module performs traffic
%After a tenant's demand arriving at the system, it is firstly checked by the admission control part .
%Tenants' demands can arrive at any time and the  is able to admit as many demands as possible while replying tenants  by a fast rescheduling algorithm.
Then based on a Linear Programming (LP) model, our traffic scheduling algorithm allocates bandwidth for the admitted demands over tunnels.
To cope with the complexity which increases exponentially with the network size,  we also propose a pruning method by ignoring certain failure scenarios that hardly happen.
At last, based on a Mixed-Integer Linear Programming (MILP) model, the failure recovery procedure pre-computes backup bandwidth allocations and reroutes traffic to minimize the revenue loss due to SLA violations, which is proved to be 2-optimal. 
%Our online scheduling algorithm is able to guarantee diverse availability targets of services even when network resource is inadequate.
%Compared with TEAVAR \cite{Teavar},  $\mathsf{BATE}$ can achieve a more reasonable balance between network utilization and availability when network resource is sufficient, and guarantee diverse availability targets of services when network resource is inadequate.
%$\mathsf{BATE}$  also pre-computes backup allocation across each tunnel to maximize revenue when a link fails.
%We model the failure recovery problem as  a 0-1 Mixed-integer linear programming, prove its NP-hardness and propose a 2-approximation algorithm to solve it.
%Therefore, $\mathsf{BATE}$ can maintain profit even network fails without too much extra bandwidth headroom.
%Links are maintained ideal utilization level under 
%The admission control works in the first-come-first-service (FCFS) manner without preemption and we model it as a 0-1 optimization problem when all the admitted demands can be rescheduled. 
%We propose a reschedule algorithm to accommodate as many tenants' demands as possible in polynomial time.
%The traffic engineering models bandwidth allocation as a linear programming problem and is able to guarantee availability targets of admitted demands.
%Our traffic engineering algorithm 
%$\mathsf{BATE}$ is motivated by the observation that empirical link failure traces available to the network operators allow us to estimate link failure probabilities and to reason availability under different network states.
%%is the first formal TE framework that optimizes bandwidth-based availability.
%%We refer the readers to Section \ref{relate} for a discussion of related work on TE, bandwidth guarantee, and other
%%risk-aware approaches to networking.
%%Operators actually have empirical link failures traces of inter-DC WAN and $\mathsf{BATE}$ can estimate the failure probability of links from these data.
%%Also, the probability of different failure scenarios can be  explicitly reasoned with link failure probability.
%%We define $\mathsf{BATE}$ optimization problem which aims to maximize total profit subject to bandwidth-based availability demands. 
%We define $\mathsf{BATE}$ optimization problem to maximize SLA-aware total profit subject to bandwidth-based availability demands.
%$\mathsf{BATE}$ problem is a 0-1 mixed-integer linear program and  it is proven to be NP-hard.
%$\mathsf{BATE}$  aims for bandwidth-based availability provision, where a set of demands on transmission bandwidth in inter-DC WAN should be satisfied with their expected probabilities.
%%$\mathsf{BATE}$ generates a probabilistic failure model from empirical data and maximizes the profit that can be achieved under certain constrains on bandwidth-based availability. 
%For the failure model, we propose a pruning algorithm to reduce the problem size. 
%For the optimization problem, we propose a greedy algorithm to efficiently solve it with only minor profit loss.
% $\mathsf{BATE}$ defines Weighted Availability Profit Maximization (WAPM) problem which optimizes applications' total availability profit.
%Input of $\mathsf{BATE}$ contains two parts: (1) Network states and its corresponding probability reasoned from the empirical data of WAN link/node failure; (2) Application level traffic matrix which is composed of bandwidth and availability demands.
%We define an application's achieved availability as the percentage of network states in which the allocated resource is larger than its demand.
%The availability profit function is positively biased over the achieved availability.
%%If an application's achieved availability is larger than availability threshold, we regard it as eligibility.
%$\mathsf{BATE}$ optimizes the total availability profit.
%TeaVaR
%enables formulating guarantees such as “user i is guaranteed bi network bandwidth at least β% of the time,” and computing bandwidth
%assignments that achieve these guarantees for a operator-specified
%value of β
%$\mathsf{BATE}$ problem is a 0-1 mixed-integer linear program and we prove it is NP-hard.
%We propose network scenario pruning algorithm to reduce problem scale  and a greedy algorithm to attain the approximate solution.

%Under $\mathsf{BATE}$, services are very likely free from congestion, while maintaining high network utilization.
%Compared with other TE schemes (see $\S$\ref{relate}),  it achieves better tradeoff between network utilization and availability.


Thirdly, 
we implement $\mathsf{BATE}$ as a real system, including a centralized controller and multiple brokers (one for each DC),  together with end-host clients (see $\S$\ref{system}).
 We conduct extensive experiments using a small testbed as well as trace driven large scale simulations (see $\S$\ref{evaluation}).
We compare  $\mathsf{BATE}$ with state-of-the-art WAN TE schemes such as TEAVAR \cite{Teavar}, SMORE \cite{SMORE}, SWAN \cite{swan}, B4 \cite{B4} and FFC \cite{FFC}, across different topologies, traffic matrices and failure scenarios.
Our evaluations on real network topologies and traces demonstrate that, $\mathsf{BATE}$ can 
(1) speed up the online admission control by 30$\times $ at the expense of a false rejection ratio that is less than 4\%;
(2) meet the bandwidth availability SLAs for 23\%$\sim$60\% more demands under normal loads;
(3) retain 10\%$\sim$20\% more profit when network failure causes SLA violations, under a simple pricing and refunding model. 
To our knowledge, $\mathsf{BATE}$ is the first to tackle bandwidth availability provision over inter-DC WAN, where heterogeneities of demands and link failures are systematically taken into account for profit maximization.  
 %At last, we show the related works (see $\S$\ref{relate}).
%Traditionally, links are heavily underutilized to achieve a more rigorous availability target. 
%However, in reality, the failure probability of a single link can even differ by three orders of magnitude.
%Therefore, some link failure can be negligible and it makes no sense to reduce the utilization of these links.
%With the link Under BATE, links can maintain high utilization.

%In summary, we make four key contributions:
%
%\begin{itemize}
%\item We analyze deficiency of state-of-the-art availability traffic engineering schemes and  advocate bandwidth-based availability traffic engineering.
%\item We propose $\mathsf{BATE}$
%a novel TE framework that defines WAPM problem to maximize total profit subject to application availability demands with network failure probability.
%We prove WAPM is NP-hard and propose a heuristic algorithm to solve it efficiently.
%\item  We design an SDN-based system named $\mathsf{Dionysus}$ which works in the centralized way to implement  $\mathsf{BATE}$.
%\item We evaluate the performance of $\mathsf{BATE}$ in both real trace driven simulation as well as real testbed. 
%Evaluation  results show  $\mathsf{BATE}$ can make 20\%, 25\%, 30\%, 40\% more flows satisfy their bandwidth-based availability demands than TEAVAR\cite{Teavar}, SMORE\cite{SMORE}, SWAN\cite{swan} and FFC\cite{FFC}, respectively.
%\end{itemize}

%\begin{table}
%\footnotesize
%\centering
%\caption{Service availability demands and their credits.}\label{target}
%\renewcommand{\arraystretch}{1.0}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline
%\setlength{\tabcolsep}{10pt}
%\textbf{Service}&\multicolumn{2}{|c|}{\textbf{Uptime percentage}}&\multicolumn{2}{|c|}{\textbf{Service credit}}&\textbf{Provider}\\
%\hline
%\multirow{2}{*}{Traffic Manager}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{2}{*}{Azure}\\
%&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%}& \\
%\hline
%\multirow{2}{*}{VPN Gateway}&\multicolumn{2}{|c|}{$< 99.95\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{2}{*}{Azure}\\
%&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%}& \\
%\hline
%\multirow{3}{*}{VM Instances}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{3}{*}{Azure}\\
%&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%}& \\
%&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%}& \\
%\hline
%\multirow{2}{*}{Cosmos DB}&\multicolumn{2}{|c|}{$< 99.999\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{2}{*}{Azure}\\
%&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%}& \\
%\hline
%\multirow{3}{*}{Compute}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{3}{*}{AWS}\\
%&\multicolumn{2}{|c|}{$< 99.0\%$}&\multicolumn{2}{|c|}{30\%}& \\
%&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%}& \\
%\hline
%\multirow{3}{*}{Amplify Console}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{3}{*}{AWS}\\
%&\multicolumn{2}{|c|}{$< 99.95\%$}&\multicolumn{2}{|c|}{25\%}& \\
%&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%}& \\
%\hline
%\multirow{2}{*}{Messages}&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{2}{*}{Aliyun}\\
%&\multicolumn{2}{|c|}{$< 90\%$}&\multicolumn{2}{|c|}{30\%}& \\
%\hline
%\multirow{2}{*}{ML PAI}&\multicolumn{2}{|c|}{$< 99.9\%$}&\multicolumn{2}{|c|}{$30\%$}&\multirow{2}{*}{Aliyun}\\
%&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{50\%}& \\
%\hline
%\end{tabular}
%\end{table}





%\begin{figure}[t]
%        \centering
%          \subfigure[Flow completion time]{
%         \includegraphics [width=0.23 \textwidth] {fig/c/compare-selected2.pdf}}
%       \subfigure[Bandwidth ratio]{
%        \includegraphics[width=0.23\textwidth]{fig/b/compare-selected.pdf}}
%     \caption{The Amazon inter-DC WAN performance.}
%    \label{wan-measure}
% \end{figure}

\section{Background and Motivation}\label{background}
In this section, we first briefly introduce network and common availability requirements in inter-DC WAN, then 
we use an example to demonstrate the limitations of state-of-the-art traffic engineering schemes in fulfilling such requirements.
%This section first shows the background.
%We then demonstrate the deficiencies of state-of-the-art traffic engineering schemes.

\subsection{Network failures and availability requirements}

%How to maintain high performance is a big problem for network operators.
%Frequent network failures can result in SLA violations and eventually loss of revenue to service providers \cite{riskbased,analysis-osdi,amazon,azure}.

\begin{figure*}[t]
\centering
\subfigure[Capacity and failure probability]{
\includegraphics[width=0.24\textwidth]{fig/motivation-link.pdf}}
\subfigure[FFC (with one failure) ]{
\includegraphics[width=0.24\textwidth]{fig/motivation-ffc.pdf}}
\subfigure[TEAVAR]{
\includegraphics[width=0.24\textwidth]{fig/motivation-teavar.pdf}}
\subfigure[ $\mathsf{BATE}$]{
\includegraphics[width=0.24\textwidth]{fig/motivation-hate.pdf}}
\caption{A simple example where user1 (red) requires 6Gbps bandwidth for at least 99\% time and user2 (blue) requires 12Gbps bandwidth for at least 90\% time, both from DC1 to DC4. }%Link capacity is 10Gbps everywhere. Neither user gets enough bandwidth or availability guarantees under FFC or TEAVAR.  Both users are in line with their SLAs under $\mathsf{BATE}$. }
\label{motivatio_fig}
\end{figure*}

\begin{table}
\small
\centering
\caption{Services have different availability targets.}\label{target}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{|c|l|l|l|l|l|l|l|} \hline
\setlength{\tabcolsep}{10pt}
\textbf{Service}&\multicolumn{2}{|c|}{\textbf{Availability}}&\multicolumn{2}{|c|}{\textbf{Refund}}\\
\hline
{Traffic Manager \cite{azure}}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}\\
\hline
{VPN Gateway \cite{azure}}&\multicolumn{2}{|c|}{$< 99.95\%$}&\multicolumn{2}{|c|}{$10\%$} \\
\hline
{VM Instances \cite{amazon}}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$} \\
\hline
{Cosmos DB \cite{azure}}&\multicolumn{2}{|c|}{$< 99.999\%$}&\multicolumn{2}{|c|}{$10\%$}\\
(Azure)&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%} \\
\hline
{DMS \cite{amazon2}}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}\\
(AWS)&\multicolumn{2}{|c|}{$< 99.0\%$}&\multicolumn{2}{|c|}{30\%}\\
&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%}\\
\hline
{AppFlow\cite{amazon3}}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}\\
(Amazon)&\multicolumn{2}{|c|}{$< 99.95\%$}&\multicolumn{2}{|c|}{25\%} \\
&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%} \\
\hline
{SMS\cite{alibaba3}}&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{$10\%$}\\
(Alibaba)&\multicolumn{2}{|c|}{$< 90\%$}&\multicolumn{2}{|c|}{30\%}\\
\hline
{Data Transmission\cite{alibaba2}}&\multicolumn{2}{|c|}{$< 99.9\%$}&\multicolumn{2}{|c|}{$15\%$}\\
(Alibaba) &\multicolumn{2}{|c|}{$< 99.0\%$}&\multicolumn{2}{|c|}{30\%} \\
&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%} \\
\hline
\end{tabular}
\end{table}

\textbf{WAN failures are frequent and follow a heavy-tailed distribution.}
%Facing the growing scale and complexity of the WAN, how to maintain high availability is a challenge for data center network service providers\cite{evole, riskbased}. 
Failures could occur anywhere, from control plane to data plane across the network \cite{Teavar}.
They could also last for long durations, 
%Besides, failure events could be persistent with long durations.
as Google reports, more than 80\% of the failures last between 10 mins and 100 mins over their B4 network \cite{evole,riskbased}, 
%For some large failure events, before finding the root cause, they will try to drain services away from affected clusters \cite{evole}.
%The service resumption after such failures can even take one day, 
leading to severe performance degradation and revenue loss.
%Therefore, \textit{when planning for resource allocation, network risks should proactively be taken into consideration}.
On the other hand, 
according to the earlier measurements \cite{california,understanding}, failures often follow a \textit{heavy-tailed distribution},
where a small portion of links contribute to most of the failures, while most links experience only few failures, and the failure rate of
%Failure probability of 
a single link can differ by even more than three orders of magnitude \cite{failureslarge,Teavar}.
Therefore, \textit{network failures, especially their uneven distribution, should be explicitly taken into account by network operators}. 


\textbf{High availability directly translates into profit.}
Nowadays, high availability is nearly always one of the main items in SLAs  \cite{amazon,azure,amazon2}, and customers are eligible for a credit refund  if there are SLA violations. 
We conduct a survey on the SLA claims of different cloud providers, and Table \ref{target} shows their declared availability targets and  corresponding refunding policies,
%when violated.
%We can see that service providers often use a progressive piecewise function to derive refunding credits for violating availability targets. 
where the refunding credit is typically represented by a simple step function. 
For example, Microsoft Azure provides 10\% refund if its Traffic Manager service availability falls between 99.99\% and
99.0\%, and provides 30\% refund for anything below 99.0\% availability \cite{azure}. As more realtime and mission-critical applications (financial trading, online game, video streaming,
instant messaging, live broadcast, etc.) are deployed on the Internet, \textit{providing hard guarantee of high service availability under network failures to retain a good profit is a big challenge}.

%Maximizing network utilization and improving application availability are often contradictory.
%In tunnel-based TE mechanisms, when a failure occurs in a tunnel, the remaining available tunnels can be used to redistribute the traffic.
%%Therefore, some links might be overloaded.
%It is easy to see that the highly utilized networks are usually unable to accommodate traffic shifted from the failed tunnels. 
%%In order to maintain high availability service level, network capacity are always drastically over-provisioned for redundancy.
%However, low network utilization with substantially over-provisioned WAN capacity can lead to performance and profit loss. 
%%can degrade applications' performance and drastically over-provisioned WAN capacity for redundancy spends much money.
%It is a challenge to strike a balance between availability and performance.
%%These historical failures of links/nodes can reflect their reliability and be used in WAN resource allocation.
%%To guarantee links free from congestion, the network is proactively XXX.
%Therefore, \textit{some low failure-probability links can retain high utilization to guarantee the high throughput of applications, without necessarily leading to SLA violations}.



%\textbf{Inter-DC WAN is expensive:}
%Bandwidth on the Inter-DC WAN is valuable and important resource.
%According to one study of Microsoft, they even invest more than 100 millions of dollars on Inter-DC WAN.

%For example, Microsoft Azure network provider should guarantee their DNS queries will receive a valid response at least 99.99\% of the time, while  the availability demand of  Virtual WAN is  99.95\%.

%Therefore, providing adequate resources for each service can 



%
%\begin{figure*} 
%   \begin{minipage}[t]{0.48\linewidth} 
%         \subfigure[Insufficient bandwidth]{
%         \includegraphics [width=0.45 \textwidth] {fig/motivation-1/1.pdf}}
%       \subfigure[Guarantee bandwidth]{
%        \includegraphics[width=0.45\textwidth]{fig/motivation-1/2.pdf}}
%     \caption{QoS comparison for the same video.}
%    \label{wan-measure-2}
% \end{minipage}% 
%  \begin{minipage}[t]{0.48\linewidth} 
%    \centering 
%       \subfigure[Completion time]{
%         \includegraphics [width=0.48 \textwidth] {fig/c/compare-selected2.pdf}}
%       \subfigure[Bandwidth ratio]{
%        \includegraphics[width=0.48\textwidth]{fig/b/compare-selected.pdf}}
%     \caption{The Amazon Inter-DC WAN performance.}
%    \label{wan-measure}
%      \end{minipage}% 
%\end{figure*}










\textbf{A one-size-fit-all network availability target is not enough.}
%Availability has attracted major attention both in the industry and research community.
%Firstly, most of them only consider \textit{connectivity-based} availability\cite{hong2018b4}, which is conservative.
%The connectivity-based availability\cite{hong2018b4} is defined as the service uptime percentage, where a given minute is considered up if the connection is reachable and  it tries to quickly detour failed network devices, but \textit{they only focus on the reachability of networks and do not provide performance guarantees}.
%as a result, user experience might be influenced.
%Bandwidth on InterDC-WAN is a valuable resource and Microsoft \cite{swan} even spends millions of dollars on WAN investment.
In recent years, there has been a rapid increase in rapid and agile deployment of services over clouds.
Many studies have shown that users will quickly abandon sessions if the qualify of service is not guaranteed, leading to significant losses in revenue for content providers \cite{video1,video2,video3}.
Multiple services might be simultaneously launched over the global infrastructure operated by the same content provider or cloud provider. 
They might also pose different availability requirements, and  will contend for the  
% multiple service instances over different data centers globally at any time.
inter-DC WAN bandwidth.
% is a valuable resource and can become bottleneck \cite{swan} when  services \textit{contend} for network resource.
%In reality, availability targets vary with services, 
As shown in  Table \ref{target}, the minimal availability demands of the Data Transmission Service \cite{alibaba2} and the Short Message Service\cite{alibaba3} are 95\% and 90\%, respectively.
\textit{Such heterogeneous availability demands cannot be well captured and handled by a one-size-fit-all approach}, where all users get the same level of availability guarantee (e.g, TEAVAR  \cite{Teavar} only considers guaranteeing all users' bandwidth at least  $\beta\%$  time) . 
%Therefore, TEAVAR's guarantee such as "Users' bandwidth should be guaranteed at least $\beta\%$ time " \cite{Teavar}  is not enough,
%since it fails to differentiate availability targets of services.

%Traditionally, the availability is defined as the service uptime percentage, where a given minute is considered up if the connectivity is successful.
%We call this connectivity-based availability.
%%individual services have vastly different bandwidth, latency and loss requirements.
%\textit{The connectivity-based availability is insufficient since it only emphasizes on the success of connectivity but can't guarantee bandwidth demands and QoE will drop significantly if minimal bandwidth is unable to be provisioned}.
%However, modern public cloud providers can't always guarantee applications' bandwidth over their inter-DC WAN.
%We perform measurement on Amazon global inter-DC WAN to prove this.
%We select 6 data centers: NYC (US east), PAR (Europe), HK (Asia Pacific), TKY (Asia Pacific), SG (Asia Pacific), London (Europe).
%In each data center, we purchase one VM with high network performance metric. 
%We transfer 5GB data from London to other VMs at different times and Figure  \ref{wan-measure} (a) shows ratio of maximal and minimal flow completion time. 
%We can see the FCT variation can even be as large as 3 $\times$.
%To investigate this problem, we next start TCP connections from London to other data centers and compute the ratio of maximal bandwidth and minimal bandwidth every 10 minutes.
%The measurement lasts one day and Figure \ref{wan-measure} (b) demonstrates the result.
%We can see that bandwidth ratio varies from 2 to 7, which suggests the inter-DC WAN is unable to ensure the timely data delivery of online services.


%The way of transmitting bandwidth utilizes redundant resources. 
%However, without proper resource allocation, when an error occurs in a certain path, even if the switch quickly detects the error and expands the bandwidth on other backup paths, it will cause congestion in the expanded network, which will affect the performance of the application. 
%Current network operation and maintenance personnel can calculate the error probability of each link based on . 
%. , Can jointly optimize network utilization and availability.

%
%Most current data center WAN resource allocation mechanisms focus on improving network resource utilization, or optimizing the performance of data center WAN, and rarely consider how to provide continuous and uninterrupted service guarantee for applications. 
%At present, although there have been a series of work in resource allocation, considering the availability requirements of network services, but there are some problems, we will introduce in the next section.






\subsection{A motivating example for $\mathsf{BATE}$}
Now we use a simple example to illustrate why existing traffic engineering algorithms cannot 
meet the heterogeneous bandwidth availability demands well. The toy topology we use is depicted 
in Figure \ref{motivatio_fig}(a), where there are 4 data centers and the links connecting them 
are annotated with their corresponding capacities and failure probabilities. Suppose we have two 
bandwidth demands for inter-DC transmission from DC1 to DC4, i.e., user1 (red) requires 6Gbps bandwidth 
with at least 99\% availability, and user2 (blue) requires 12Gbps bandwidth with at least 90\% availability. 
There are two paths from DC1 to DC4, i.e., DC1$\to$ DC2 $\to$ DC4, and DC1 $\to$ DC3 $\to$ DC4, whose available probabilities are $(1-4\%) \times (1-0.0001\%)=95.999904\%$ and $(1-0.1\%) \times (1-0.0001\%) = 99.8999001\%$, respectively. We apply FFC \cite{FFC} and TEAVAR \cite{Teavar}, two latest WAN traffic engineering schemes that take network failures into account, to this scenario. 

FFC \cite{FFC} guarantees a total bandwidth from DC1 to DC4 under at most $l$ concurrent node/link failures, and here we simply use $l=1$. Figure \ref{motivatio_fig}(b) shows FFC can support 10Gbps bandwidth from DC1 to DC4 in 99.996\% time even with one failure (the probability that the two paths fail simultaneously is $(1-95.999904\%) \times (1-99.8999001\%) = 0.004004092096\%$. Since user1 and user2 can get respectively 3.34Gbps and 6.66Gbps, which are evenly distributed on the two paths from DC1 to DC4, 
and neither of their bandwidth demands can be satisfied. This shows \textit{FFC does not differentiate between paths with different availabilities.} The lower path has a much smaller failure probability, and it is wasteful without utilizing it as much as possible. 

On the other hand, TEAVAR \cite{Teavar} exploits the different link failure probabilities and maximizes the network utilization, subject to meeting a \textit{single} desired availability. Figure \ref{motivatio_fig}(c) illustrates the bandwidth allocation result of TEAVAR, where user1 and user2 can get their demanded 6Gbps and 12Gbps bandwidth, both in about 95.9\% time. However, this falls below 
user1's availability demand, i.e., 99\%, and will cause a SLA violation. 
This shows  \textit{TEAVAR does not consider the heterogeneous user demands on availability.} 
Since user1 requires a higher availability, it is better to use a path with a lower failure probability. 

\iffalse
FFC \cite{FFC} seeks to assign a total bandwidth $f_k$ to each node pair $k$.
It achieves this by reserving bandwidth on each tunnel between $k$ and ensuring the total bandwidth of all tunnels is larger than $f_k$ under $l$ concurrent node/link failure scenarios.
TEAVAR \cite{Teavar} ensures that all scenarios meeting bandwidth demand $d_k$ has a total probability no less than the required availability target.
%considers the probability of each scenario and regards the sum of the probabilities for scenarios where bandwidth demand $d_k$ between each node pair $k$ is fully satisfied as the achieved availability.
TEAVAR maximizes the network utilization subject to meeting a \textit{single} desired availability target (e.g., 99.99\%).
In this part, we demonstrate that reasoning about  availability in terms of
the number of concurrent failures is often too conservative while meeting the same availability target for diverse services is often not enough.
%FFC\cite{FFC} uses the concurrent node/link failures as the proxy for availability;
%TEAVAR\cite{Teavar} tries to leverage the financial risk theory and optimize bandwidth allocation subject to meeting a desired availability target (e.g., 99.99\%).
%In this part, we will use a simple example to demonstrate the flaws of FFC and  TEAVAR.


Figure \ref{motivatio_fig}(a) shows a network topology, where DC1 is connected to DC4 via DC3 and DC2.
The capacity of each link is 10Gbps and we also show the link failure probability in this figure.
There are two paths from DC1 to DC4:
(1) DC1$\to$ DC2 $\to$ DC4, (2) DC1 $\to$ DC3 $\to$ DC4, whose available probabilities are 95.999904\% (i.e., 96\% $\times$ 99.9999\%) and 99.8999001\% (i.e., 99.9\% $\times$ 99.9999\%).
%We can derive the uptime fraction of path $(1)$ and $(2)$, i.e., 95\% and 99\%, respectively.
%There are two tenants, where tenant1 (red) deploys service1 with 6Gbps bandwidth demand from DC1 to DC4 and tenant2 (blue) deploys service2 with 12Gbps bandwidth demand from DC1 to DC4
There are two services deployed by a tenant, where service1's (red) minimal bandwidth demand of 6Gbps from DC1 to DC4 should be guaranteed at least 99\% time and service2's (blue) minimal bandwidth demand of 12Gbps from DC1 to DC4 should be guaranteed at least 90\% time.
%whose bandwidth demands are 6Gbps and 12Gbps.
%Availability demands of App1 and App2 are  99\% and 90\%, respectively.
%Assume App1 and App2 need to maintain the minimal bandwidth demands 99\% and 90\% time.
%App1's  is  with availability SLO target and App2's bandwidth demand is  with  availability SLO target.
%We take FFC-1 that can tolerate one network failure as the example and Figure \ref{motivatio_fig}(b) demonstrates the allocation result.
We demonstrate the bandwidth allocation under FFC-1 scheme (which can tolerate one network failure) in Figure \ref{motivatio_fig}(b).
We can see that FFC can support total 10Gbps bandwidth almost 99.9999\% time (unless path (1) and (2) fail simultaneously), where service1 and service2 can gain 3.34Gbps and 6.66Gbps, respectively.
%Indeed, 10Gbps can be guaranteed unless path (1) and (2) fail simultaneously.
Neither service is able to get enough bandwidth, because FFC treats the two paths indifferently without their individual failure rates.
However, path (2) has much smaller failure probability and lowing its utilization does not help improve availability and is wasteful.
TEAVAR exploits the different failure probability of network and advocates to fully utilize links under a loss-based availability target constraint (e.g., 99\%).
%Given the availability target 99.9\%, Figure \ref{motivatio_fig}(c) demonstrates the solution of TEAVAR.
TEAVAR's solution, shown in Figure \ref{motivatio_fig}(c), can guarantee service1 and service2 attaining 6Gbps and 12Gbps about 95.9\% time.
TEAVAR fails to satisfy availability target of service1, since it allocates network resources under a single availability target and ignores the different expectations of the two services.
As a result, it can't meet service1's availability demand.
%This suggests that TEAVAR ignores demands of different applications, as a result,  applications with higher availability demands might be unable 
\fi

\textbf{Our approach:}
%$\mathsf{BATE}$ exploits network failure probability and attempts to satisfy applications'  bandwidth-based availability demands.
Taking into account the diverse link failure probabilities and user bandwidth availability demands, 
Figure \ref{motivatio_fig}(d) shows a better bandwidth allocation, 
where user1 can get 6Gbps over 99.8999001\% time (via the lower path that has a lower failure probability) and user2 can get 12Gbps over 95.999904\% time (via both the upper and the lower path), 
satisfying both of their bandwidth demands. 

%The minimal bandwidth demands of applications are able to be met with high probability under $\mathsf{BATE}$.
%Low failure probability links are fully utilized to satisfy the high availability demands of applications.
%We elaborate the details of $\mathsf{BATE}$ in the following section.



\begin{table}[h!]
\small
\centering
\caption{Key Notations for $\mathsf{BATE}$}\label{Notation}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{ccccccc}  \toprule \toprule 
\setlength{\tabcolsep}{5pt}
&\textbf{Input Variables}\\
\hline
$G(V,E)$ &\multicolumn{6}{|l}{inter-DC WAN with nodes $V$ and Links $E$}\\
\hline
$k\in K$ &\multicolumn{6}{|l}{a s(ource)-d(est) pair in the set of all s-d pairs}\\
\hline
$T_{k}$ &\multicolumn{6}{|l}{the set of tunnels for a s-d pair $k$}\\
%\multirow{3}{*}{VM Instances}&\multicolumn{2}{|c|}{$< 99.99\%$}&\multicolumn{2}{|c|}{$10\%$}&\multirow{3}{*}{Azure}\\
%&\multicolumn{2}{|c|}{$< 99\%$}&\multicolumn{2}{|c|}{25\%}& \\
%&\multicolumn{2}{|c|}{$< 95\%$}&\multicolumn{2}{|c|}{100\%}& \\
\hline
\multirow{3}{*}{$d\!=\!(\mathbf{b}_d, \beta_d)$}&\multicolumn{6}{|l}{a BA demand $d$, requiring bandwidth $\mathbf{b}_d$ with }\\
&\multicolumn{6}{|l}{ availability $\beta_d$, where $\mathbf{b}_d$ is a vector $<\mathbf{b}^1_d, \mathbf{b}^2_d, ...>$}\\
&\multicolumn{6}{|l}{of bandwidth demands over all s-d pairs} \\
\hline
$D, \hat{D}$&\multicolumn{6}{|l}{the set of arrived and admitted demands\footnote{When an admitted demand finishes, it will be removed from $\hat{D}$.} }\\
%$d_i=(\mathbf{b_i}, \beta_i)$ &\multicolumn{6}{|l}{the $i$-th user demand, requiring bandwidth $\mathbf{b_i}$ with availability $\beta_i$, where $\mathbf{b_i}$ is a vector $<\mathbf{d}_i^1, \mathbf{d}_i^2, ...>$ of bandwidth demands over all s-d pairs\footnote{Here we omit the start and end time of this demand, but they will be implicitly considered in our online admission and scheduling.}}\\
%\hline
%$D, \hat{D}$&\multicolumn{6}{|l}{the set of arrived demands and admitted demands\footnote{$\hat{D}$ will update every time slot and an admitted demand will be removed if it finishes.} }\\
%\hline
% $j\in J$ &\multicolumn{6}{|l}{the $j$-th availability target in the level set}\\
%\hline
%$\mathbf{b}^k_d$ &\multicolumn{6}{|l}{Bandwidth demand through node pair $k$ of $i$}\\
%\hline
%$\gamma_{i}$ &\multicolumn{6}{|l}{Service type (i.e., $j$) of demand $i$}\\
%\hline
%$ \beta_{j}$ &\multicolumn{6}{|l}{Bandwidth-based availability target of service $j$}\\
\hline
$t$ &\multicolumn{6}{|l}{a tunnel for transmitting traffic\footnote{Multiple tunnels may exist for a single s-d pair.}}\\
\hline
$u_t^e$ &\multicolumn{6}{|l}{whether tunnel $t$ passes link $e \in E$}\\
\hline
$c_e, c_t$ &\multicolumn{6}{|l}{the remaining capacity on link $e$ or a tunnel $t$}\\
\hline
$\mathbf{z}\in Z$ &\multicolumn{6}{|l}{a network failure scenario in the scenario set}\\
\hline
$p_{\mathbf{z}}$ &\multicolumn{6}{|l}{the probability that a failure scenario $\mathbf{z}$ occurs}\\
\hline
$v_t^{\mathbf{z}}$ &\multicolumn{6}{|l}{whether tunnel $t$ is available under scenario $\mathbf{z}$}\\
 \hline
$w_e^{ \mathbf{z}}$ &\multicolumn{6}{|l}{whether link $e$ is available under scenario $ \mathbf{z}$}\\
% $\{1,\beta_j^1,\beta_j^2....\}$ &\multicolumn{6}{|l}{The availability target list for service $j$}\\
%  \hline
% $\{1,\alpha_j^1,\alpha_j^2...\}$ &\multicolumn{6}{|l}{The profit list for service $j$}\\
\hline

&\textbf{Output Variables}\\
\hline
$g_d$& \multicolumn{6}{|l}{whether demand $d$ is admitted}\\
\hline
%$f_{d}^t$& \multicolumn{6}{|l}{bandwidth allocated for the $i$-th demand over tunnel $t \in T_{k}$ }\\
$f_d^t$& \multicolumn{6}{|l}{bandwidth allocated for demand $d$ over tunnel $t$}\\
\hline
$r_d$& \multicolumn{6}{|l}{profit (after refunding)  for demand $d$}\\
%\hline
%\hline
%&\textbf{Auxiliary Output Variables}\\
%\hline
%% $S_j$ &\multicolumn{6}{|l}{Network service achieved availability of $j$}\\
%%\hline
% $A_d^{\mathbf{z}}$&\multicolumn{6}{|l}{Whether scenario $\mathbf{z}$ is safe or not for $i$ }\\
% \hline
% $S_i$&\multicolumn{6}{|l}{Achieved availability of $i$ }\\
\bottomrule
\bottomrule
\end{tabular}
\end{table}

\section{$\mathsf{BATE}$ framework}\label{HATE}
In this section, we discuss the details of $\mathsf{BATE}$, which contains three parts, i.e., admission control, traffic scheduling and failure recovery, using notations summarized in Table \ref{Notation}. 
The framework intends to achieve the following objectives: 
 \begin{itemize}
\item \textbf{High admission ratio and low admission latency:}  
  Bandwidth availability demands might arrive at anytime. 
  The system should be able to efficiently accommodate as many BA demands as possible under the constraint of network capacity and failure probabilities, as this would increase service agility and bring more revenue. %availability and bandwidth targets.
\item \textbf{High availability for allocated bandwidth:}
The system should be able to optimize its achieved availability in a probabilistic manner, 
as this would, in the long term, reduce potential penalties (i.e., refund due to SLA violations) and retain a good reputation. This can be achieved by making a good match between demands on higher availability and paths with lower failure probability.
%make services with stringent availability requirements (e.g., 99.999\%) pass links with high reliability.
%Therefore, their bandwidth demands can be guaranteed with high probability.
\item \textbf{Automatic and economical failure recovery:}
If any link failure really happens, the system should reroute traffic away from that link, 
while minimizing any possible collateral damage to normal traffic, i.e., congestion and lower bandwidth availability due to contention from the rerouted traffic.
%\item \textbf{ \& scalability:}
%
\end{itemize}

\subsection{Abstraction of bandwidth availability}
%Recently, deploying multiple virtual private clouds over public clouds (e.g., Amazon AWS) or deploying multiple services over private clouds (e.g., Google internal DCs) is becoming  a common practice for content providers.
%Tenants could deploy diverse service instances over Inter-DC WAN.
In reality, a customer demanding inter-DC WAN bandwidth resources could be any application or tenant  spanning multiple data centers, in either a public or a private cloud. 
%a tenant of a public cloud (e.g., Amazon AWS), who uses this substrate to build his own virtual private cloud over multiple DCs, or could be a service team who launches multiple VM instances in a private cloud (e.g., Google internal DCs).
%The cloud providers are providing many services and each service contains numerous instances across geo-distributed data centers.
%For example, a common practice is running MapReduce operations across geo-distributed data centers and many connections from mappers to reducers are maintained.
%Services have different bandwidth demands as well as availability targets.
%Currently, there is no interface for tenants to specify bandwidth availability.
Our abstractions on network failure scenarios and bandwidth availability demands in $\mathsf{BATE}$ are as follows.

\textbf{Network failure scenario model:} 
%$\mathsf{BATE}$ relies on network operators to provide link failure probabilities.
%A series of failure events (e.g., power outage, configuration mistakes, firmware bugs) directly lead to link failures.
%%According to the report of Google \cite{evole}, more than 90\% of the failure events can lead to high packets losses, or blackholes to entire data centers and they might occur anytime.
%Operators maintain the network empirical data, which can be used to estimate the link risks\cite{jointfailure,overview,evole}.
%For each link $e$, operators can examine historical data and track whether $e$ was up or down in a measured time epoch (e.g., 1min).
%The link up probability $p_i$ of link $i$ can be given by the up epoch percentage and its failure probability is $1-p_i$.
%Inspired by the site reliability engineering (SRE) technology\cite{jointfailure,overview}, we propose a general failure model.
The inter-DC WAN is modeled as a directed graph $G(V,E)$, where the set of nodes $V$ represent the data centers, 
and the set of links $E$ represent directed links between them. 
A network scenario $\mathbf{z}=\{\mathbf{z}_1,\mathbf{z}_2,...,\mathbf{z}_{|E|}\}$ is a vector of link states, 
where each element $\mathbf{z}_i \in \{0,1\}$ denotes whether the $i$-th link is up ($\mathbf{z}_i=1$) or down ($\mathbf{z}_i=0$). 
%Operators maintain the network empirical data, which can be used to estimate the link risks\cite{jointfailure,overview,evole}.
We assume network operators can use historical data to estimate the failure probability $x_i$ for this link, which are statistically indenpendent. 
%can examine historical data and track whether $e$ was up or down in a measured time epoch (e.g., 1 min).
%probability $p_i$ of link $i$ can be given by the up epoch percentage and its failure probability is $1-p_i$.
Let $Z$ denote the network scenario set, %and $p_{\mathbf{z}}$ is the probability that a scenario $\mathbf{z}\in Z$ may happen.
%Each link is up with the probability of $p_i$. \cite{Teavar}
%Let $z_i'$ denote the value of $z_i$, and assume link failures are independent.
then the expected probability that a network scenario  $\mathbf{z} \in Z$ will happen is given by \cite{Teavar} 
\begin{eqnarray*} \label{state}
%p_{\mathbf{z}}&=&p\left( z_1=z_1',z_2=z_2',z_3=z_3',...,z_{e}=z_{e}'\right)\nonumber \\
p_{\mathbf{z}}&=&\prod_{i=1}^{|E|}\big(\mathbf{z}_i \times (1-x_i)+ (1-\mathbf{z}_i) \times x_i\big)
\end{eqnarray*}

%\textbf{To be updated, follow Fig.1 }
Use the simple Inter-DC WAN topology in Figure \ref{motivatio_fig} as an example, where $E=\{e_1,e_2,e_3,e_4\}$.
Network scenario $\mathbf{z}=\{1,1,0,1\}$ means $e_1$ ,$e_2$, $e_4$ are working fine and  $e_3$ is down.
The expected availabilities of $e_1,e_2,e_3,e_4$ are 96\%, 99.9999\%, 99.9\%, 99.9999\%, respectively.
Then the probability that $\mathbf{z}$ happens is $p_\mathbf{z}=p_{\{1,1,0,1\}} =0.96\times0.999999\times0.001\times0.999999\simeq 0.000959998$.

\textbf{BA demand model:}
%We model the inter-DC WAN as a  graph $G=(V,E)$,  where $V$ and $E$ are nodes and link sets.
Let $K$ denote the set of all source-destination (s-d) DC pairs.
%Each link $e\in E$ is associated with a link capacity $c_e$ (e.g., in bps).
%DC pair set is $K$ and  $\beta_j$ presents the desired availability target  of service $j \in J$.
%\multirow{3}{*}{$d_i=(\mathbf{b}_i, \beta_i)$}&\multicolumn{6}{|l}{the $i$-th user demand, requiring bandwidth $\mathbf{b}_i$ with }\\
%&\multicolumn{6}{|l}{ availability $\beta_i$, where $\mathbf{b}_i$ is a vector $<\mathbf{b}_i^1, \mathbf{b}_i^2, ...>$}\\
%&\multicolumn{6}{|l}{of bandwidth demands over all s-d pairs\footnote{Here we omit the start and end time of this demand, but they will be implicitly considered in our online admission and traffic scheduling.}} \\
%A bandwidth availability demand $d$ is in the form of $(\mathbf{b}_d, \beta_d,t_d^s,t_d^e)$,
A bandwidth availability demand $d$ is in the form of $(\mathbf{b}_d, \beta_d)$,
where $\mathbf{b}_d$ is a vector $<\mathbf{b}_d^1, ..., \mathbf{b}_d^k, ...>$ of bandwidth demands on each s-d pair $k \in K$ \footnote{Here we omit the start and end time of this demand, but they will be implicitly considered in our online admission and traffic scheduling.}.
%where $\alpha$ and $\delta$ are the arrival and departure of the demand, $\gamma$ denotes the service type and $\theta =\{d_{1}, d_{2}, ...,d_{K} \}$ presents bandwidth over each node pair.
%We assume one tenant can only start one service and if a tenant wants to boot multiple services, we can use multiple tuples to describe it.
%The system should reserve enough bandwidth to guarantee all \textit{admitted} requests' availability demands.
%When all services'  \textit{achieved} bandwidth-based availability are larger than their corresponding desired availability target, the system is able to accommodate this service request, otherwise, the request will be denied.

%If the achieved availability (i.e., $S_j$) is larger than the highest desired availability target (i.e., $\beta_j^1$), the profit is 1.


\textbf{Traffic engineering model:}
Our Traffic Engineering uses tunnel-based forwarding \cite{FFC,swan,Teavar}.
%where traffic is carried over a set of tunnels.
%Consider an undirected graph $G = (V,E)$ and a set of $k$ pairs: $s_1t_1, s_2t_2,..., s_kt_k$,
For each source-destination node pair $k\in K$ of the inter-DC WAN, we pre-compute a set of tunnels $T_{k}$ with different routing schemes (e.g., k-shortest paths, edge disjoint paths \cite{Bruno2013Dynamic}, oblivious routing \cite{SMORE}, etc.). 
%In Section \ref{evaluation}, we evaluate the impact of routing schemes  on performance.
Each tunnel $t\in T_{k}$ contains a sequence of links and $u_t^e$ denotes whether tunnel $t$ passes a specific link $e \in E$ or not.
%The network scenario set is $\mathbf{z}$ and the corresponding probability is $p_{\mathbf{z}}, \forall \mathbf{z}\in Z $.
We use $D$ and $\hat{D}$ to represent \textit{arrived} demands and \textit{admitted} demands, respectively.
Given a new demand $d$, our admission control scheme will decide whether to admit it. 
The bandwidth that will be allocated for an admitted demand $d \in \hat{D}$ 
over tunnel $t$ is denoted by $f_d^t$.

With the above network failure scenario model, we also use 
$v_t^{\mathbf{z}}$ to denote whether tunnel $t$ is available (i.e., $v_t^{\mathbf{z}}=1$)  or not  (i.e., $v_t^{\mathbf{z}}=0$) under network scenario $\mathbf{z}$. 
Given a BA demand $d\!=\!(\mathbf{b}_d, \beta_d)$, an allocation result $\{f_d^t\}$ and a network scenario $\mathbf{z}$, 
for \textit{every} s-d pair $k$, 
if the total allocated bandwidth on available tunnels under $\mathbf{z}$, 
i.e., $\sum_{t \in T_k} f_d^t v_t^{\mathbf{z}}$,  
is no less than the bandwidth demand $\mathbf{b}_d^k$, 
then we call $\mathbf{z}$ a \textit{qualified scenario} for allocation $\{f_d^t\}$ with respect to demand $d$, 
and denote this by $\mathbf{z} \propto <d, \{f_d^t\}>$.
The sum of the probabilities of all such qualified scenarios, i.e., $\sum_{\mathbf{z} \propto <d, \{f_d^t\}>} p_{\mathbf{z}}$, is the expected probability that the bandwidth target $\mathbf{b}_d$ will be satisfied.
Now we can formally define when a bandwidth availability demand is satisfied: a demand $d$ is satisfied by an allocation $\{f_d^t\}$, if and only if 
\[
\sum_{\mathbf{z} \propto <d, \{f_d^t\}>} p_{\mathbf{z}} \ge \beta_d
\].

\iffalse
Given an allocation result $\{f_d^t\}$, 
(i) if the total bandwidth allocated for \textbf{each} s-d pair $k$ through the total surviving tunnels under network scenario $\mathbf{z}$, i.e., $\sum_{\forall t \in T_k} f_d^tv_t^{\mathbf{z}}$, 
is not smaller than demand $\mathbf{b}_d^k$ for that pair under under scenario $\bm{z}$,  then we call the scenario as a qualified one;
(ii) The sum of qualified scenario probability can be seen as the achieved availability and if the achieved availability
%(ii) for every tunnel $t$ that is used in the allocation (i.e., $f_d^t>0$), 
%its expected availability under all possible network failure scenarios, 
%i.e., $\sum_{\mathbf{z}} \in Z p_{\mathbf{z}} \times v_t^{\mathbf{z}} $, 
is no less than the availability demand $\beta_d$, then we say this BA demand is \textbf{satisfied} by this exact allocation result. 
\fi

If a failure indeed occurs, our failure recovery scheme will try to reroute traffic that is affected by this failure. If any availability target is violated, a refund will be given back to the customer %(see $\S$\ref{background}) 
according to the achieved availability, and we use $h_d$ to denote the profit (after refunding) for serving demand $d$.


%\begin{enumerate}
%    \item  How to accommodate as many requests as possible subject to network failure model and diverse service availability targets  (see $\S$ \ref{admission_control}).
%    \item  How to route all admitted requests with minimum induced network overload under the constraint of network failure model and diverse service availability targets (see $\S$ \ref{TE}).
%    \item  How to reroute requests with minimum cost when network fails (see $\S$ \ref{backup}).
%\end{enumerate}

%\subsection {Modeling network scenarios}
%%We now discuss how to model network failure scenarios.


%Under network scenario $\mathbf{z}$, a tunnel $t$ is available only when all the links it passes by are up.



%Consider a set of failure event set $H$ can influence the network reliability.
%If an event $h\in H$ occurs, the network device will become unavailable.
%Similar to \cite{Teavar}, we also assume all the events are uncorrelated and use $p_h$ to denote the probability of occurrence for event $h$.
%Vector $\bm{q}=\{q_1,q_2,...,q_{|H|}\}$ denotes one state, where $q_h\in \{0,1\}$ presents whether event $h$ will happen($q_h=1$) or not ($q_h=0$).
%$q_h'$ is the value of $q_h$.
%The probability of state $\bm{q}$ can be derived from the occurrence probability of events:
%
%\begin{eqnarray} \label{q}
%p(\bm{q})&=&p\left( q_1=q_1',q_2=q_2',q_3=q_3',...,q_{h}=q_{h}'\right)\nonumber \\
%&=&\prod_{h\in H}(q_h'p_h+(1-q_h')(1-p_h))
%\end{eqnarray}
%
%For instance,  we consider four events $H=\{h_1,h_2,h_3,h_4\}$ for link $e$.
%The occurrence probability of the four events are $p_{h_1}=0.2, p_{h_2}=0.3,p_{h_3}=0.4,p_{h_4}=0.5$, respectively.
%$\bm{q}=\{1,0,0,0\}$ presents only event $h_1$ happens.
%Then the probability of state $\bm{q}$ is: $p(\bm{q})=0.2\times0.7\times0.6\times 0.5=0.042$.
%Consider all events will not happen, we can also derive the available probability of link $e$: $p(\{0,0,0,0\})=0.8\times0.7\times0.6\times 0.5=0.168$.



%\subsection {Traffic Engineering for Diverse Availability Targets }\label{BATE}
%Based on the the network scenarios, we design our traffic engineering algorithm that optimizes bandwidth-based availability for diverse services.
%To this end, we introduce the problem formulation of $\mathsf{BATE}$ problem and 
 \begin{algorithm}
\KwIn{Input parameters shown in Table \ref{Notation}} 
\KwOut{Whether the new demand can be admitted.}
%$g_d=0, s_d=1, \forall d \in D$; \\
%$f_{d}^t = 0, \forall d \in D ,  k\in K: t\in T_{k}$;\\
\While{$true$}{
$d=\arg_{d'\in D}min\{\sum_{k\in K}b_{d'}^k\times \beta_{d'}\}$;\\
\For{$k\in K$}{
\If{$b_{d}^k >$  remaining capacity of s-d pair $k$}{
    \Return $ False$;\\
}
$T'_k=T_k$;\\
\While{$b_{d}^k>0$}{
$t=\arg_{t\in T'_k}{\min\{c_t*p_t\}}$;\\
 $f_{d}^t=\min$\{$c_t$, $b_{d}^k$\};\\
$ T'_k=T'_k \setminus t$;\\
 $s_d=s_d*p_t$;\\
 $b_{d}^k = b_{d}^k- f_{d}^t;$\\
  update the remaining capacities of links and tunnels;\\
  %$c_t= c_t- f_{d}^t;$\\
}
%\For{$t\in T_k$}{
%   $f_{d}^t=Allocation(t, \mathbf{b}^k_d)$;\\
%   \If{$f_{d}^t>=0$}{
%   $s_i=s_i*p_t$;\\
%   $\mathbf{b}^k_d = \mathbf{b}^k_d- f_{d}^t;$\\
%   }
%   \If{$\mathbf{b}^k_d<=0$}{
%   \textbf{break};\\
%   }
%}
}
  \If{$s_d <   \beta_{d}$}{
      \Return $False$;\\
  }
  $D=D\setminus d$;\\
}

\Return $True$;\\
\caption{Admission Conjecture}
\label{greedy-1}
\end{algorithm}
\subsection {Admission control} \label{admission_control}
User demands are served in a first-come-first-service (FCFS) manner without preemption.
When a new demand $d$ arrives, we have $D= \hat{D} \cup {d}$.
%The Admission control  \textit{determines whether availability target of $\forall d \in D$ can be supported simultaneously subject to network failure model}.
%If this is true, the new arrival demand can be admitted, otherwise, it is rejected.
To accommodate as many demands as possible, the optimal admission strategy would be that, 
if every demand in $D$ can meet its availability target, then $d$ should be admitted. 
Otherwise, it should be rejected.
This can be modeled by a 0-1 Mixed-Integer Linear Programming (MILP) problem which  
maximizes the number of demands whose availability targets can be satisfied.
Appendix \ref{admission_problem} shows the formulation of this problem, 
which can be proved to be NP-hard by reducing the all-or-nothing multi-commodity flow problem \cite{inproceedings2} to a special case of it.
%The details are omitted for brevity.
%If availability target of $\forall d \in D$ can be supported simultaneously subject to network failure model, the new arrival demand can be admitted, otherwise, it is rejected.
%If , we can model the 
%The Admission control part decides whether a new demand submitted by a tenant can be admitted or not.

%Solving the optimization problem shown in Appendix \ref{admission_problem} can determine whether the new coming tenant's demand can be admitted.
%However,  the large size of network scenarios and 
However, in order to support agile deployment of new applications and services, 
user demands should be admitted as fast as possible, 
while the time needed to exactly solve this NP-hard problem may be prohibitive. 
Therefore, we need a better tradeoff between efficiency and optimality.
The final admission control strategy we use is as follows:

\iffalse
If we assume all the admitted demands are fixed and can't be rescheduled, then we can derive the solution fast but the new arrival one might be unable to be accommodated;
If we derive the optimal solution  by solving the optimization problem shown in Appendix \ref{admission_problem}, then we can accommodate more demands, however, this is time-consuming.
To achieve a tradeoff, we would like to address the following three steps:
\fi

\begin{enumerate}
    \item When a new demand $d$ arrives, we \textit{fix} the bandwidth allocation for all admitted demands in $\hat{D}$, then we check whether $d$ can be satisfied by the remaining network capacity and failure  probability. If the answer is positive, then admit $d$ and make a first-time bandwidth allocation for it.
    \item Otherwise, run a greedy algorithm (Algorithm \ref{greedy-1}) to \textit{conjecture} whether the admitted demands can potentially be rescheduled to accommodate $d$. If the answer is positive, then admit $d$ and make a temporary bandwidth allocation for it, using the remaining network capacity as far as needed \footnote{It's possible that the temporarily allocated bandwidth falls below the demanded bandwidth, but a new allocation strategy satisfying all demands does exist (see Theorem \ref{NP-222}), and will be computed later in our periodical traffic scheduling.}. 
	\item If $d$ still cannot be accommodated, reject the demand.
%    \item Pre-allocate bandwidth to the demand with remaining network capacity and pass all the admitted demands to the traffic engineering algorithm.
\end{enumerate}

%We now introduce the admission control problem when all the admitted demands can be rescheduled.
%
%We begin with the constraints.
The greedy algorithm tries to conjecture, in an efficient way, whether an allocation strategy satisfying all demands (i.e., including $d$) exists. It works iteratively as follows. In each iteration, 
It finds the demand which has the smallest product of bandwidth target and availability target (i.e., $\sum_{k \in K}\mathbf{b}_d^k \times \beta_d$) at first (line 2), 
and tries to allocate bandwidth for each of its s-d pairs one by one. 
If the remaining network capacity cannot satisfy this demand, we will give up (line 4-5). 
Otherwise, it allocates tunnel bandwidth for this demand, where a tunnel with a smaller product of 
remaining capacity and availability has a higher priority (line 7-13). 
After this, if the availability target cannot be roughly satisfied, we will give up  (line 14-15), 
otherwise, we go for the next iteration.

The time complexity of Algorithm \ref{greedy-1} is $O(|D|*|K|*max(|T_k|))$. 
\textit{It is also worth to note that, there is no false positive in conjectures made by Algorithm \ref{greedy-1}}, 
as indicated by the following theorem, whose proof can be found in Appendix \ref{appendix-proof}:


\iffalse
Therefore, we propose Algorithm \ref{greedy-1}, which gains the solution in a fast manner.
%The key idea of the algorithm is to place requests with small bandwidth and availability demands to tunnels with small remaining capacity and availability , the algorithm chooses demand with probability.
The algorithm will choose demand with smallest bandwidth times availability targets each iteration (Line 4).
Then it checks whether the remaining network capacity can satisfy its bandwidth demand.
If this is false, then the network is unable to support the request (Line 6-8), otherwise,  it prioritizes tunnels with small remaining capacity and up probability (Line 10).
It allocates bandwidth with the remaining capacity of the tunnel until bandwidth demand between the node pair is fulfilled (Line 11-13).
The algorithm will return true when all demands' bandwidth availability targets in $D$ are satisfied (Line 19).
\fi



%The system should \textit{determine whether the new arrival and all admitted requests' availability targets can be supported simultaneously subject to network failure model} with near real-time fashion.
%If all the admitted requests can be rescheduled, more requests can be admitted.




\begin{theorem}\label{NP-222}
If a new demand $d$ can be admitted by Algorithm \ref{greedy-1}, then
there must exist an allocation result $\{f_d^t\}$ to satisfy the bandwidth availability targets of all demands $D=\hat{D} \cup {d}$.
\end{theorem} 
%\begin{proof}
%We use the contradiction method to prove, i.e., there is a demand that is admitted by Algorithm \ref{greedy-1} but the network is unable to satisfy its bandwidth-based availability.
%There are two cases: (i) network bandwidth is insufficient; (ii) The availability provided by the network is not enough. 
%Case (i) is impossible, because if bandwidth is insufficient (i.e., $\mathbf{b}^k_d> Capacity (T_k)$) , Algorithm \ref{greedy-1} won't admit the demand (Line 6-8).
%Case (ii) is also impossible, because if the availability is smaller than its target (i.e., $s_i <   \beta_{\gamma_{i}}$) , Algorithm \ref{greedy-1} will reject the demand (Line 14-16).
%This completes the proof. 
%\end{proof}
%We can reduce the NP-hard all-or-nothing multi-commodity flow problem \cite{inproceedings2} to a special case of admission control problem, and the detailed proof can be found in Appendix \ref{appendix1}.


%
% \begin{algorithm}
%\KwIn{Input parameters shown in Table \ref{Notation}} 
%\KwOut{ $\{f_{jtk}\}$}
%Sort $I$ in non-decreasing order with $\sum_{k\in K}\mathbf{b}^k_d\beta_i$;\\
%Sort all $T_k$ in non-decreasing order with failure probability;\\
%$g_i=0, s_i=1, \forall d \in D$; \\
%\For{$d \in D$}{
%\For{$k\in K$}{
%\If{$\mathbf{b}^k_d> Capacity(T_k)$}{
%    $g_i=0;$\\
%    \Return $\{g_1, g_2,..,g_i,....\}$;\\
%}
%\While{$\mathbf{b}^k_d>0$}{
%$t=\arg_{t\in T_k}{\min\{c_t*p_t\}}$;\\
% $f_{d}^t=Allocation(t, \mathbf{b}^k_d)$;\\
% $s_i=s_i*p_t$;\\
% $\mathbf{b}^k_d = \mathbf{b}^k_d- f_{d}^t;$\\
%}
%%\For{$t\in T_k$}{
%%   $f_{d}^t=Allocation(t, \mathbf{b}^k_d)$;\\
%%   \If{$f_{d}^t>=0$}{
%%   $s_i=s_i*p_t$;\\
%%   $\mathbf{b}^k_d = \mathbf{b}^k_d- f_{d}^t;$\\
%%   }
%%   \If{$\mathbf{b}^k_d<=0$}{
%%   \textbf{break};\\
%%   }
%%}
%}
%  \If{$s_i <   \beta_{\gamma_{i}}$}{
%  $g_i=0$;\\
%      \Return $\{g_1, g_2,..,g_i,....\}$;\\
%  }
%  $g_i=1$;\\
%}
%
%\Return $\{g_1, g_2,..,g_i,....\},\{f_{d}^t\}$;\\
%\caption{Greedy algorithm for admission control}
%\label{greedy}
%\end{algorithm}



%The output variables $g_i$ are chosen from \{0,1\},  so that the admission control problem is a 0-1 Mixed-integer linear programming.
%
%\begin{lemma}\label{NP-hard}
%The admission control problem is NP-hard.
%\end{lemma}
%We can reduce the NP-hard all-or-nothing multi-commodity flow problem \cite{inproceedings2} to a special case of admission control problem, and the detailed proof can be found in Appendix \ref{appendix1}.






% \begin{algorithm}
%\KwIn{$\{\mathbf{b}^k_d\},G, \{\beta_j\}$} 
%\KwOut{ $g_i$}
%$s_i=1;$\\
%\For{$k \in K$}{
%\If{$\mathbf{b}^k_d> Capacity(T_k)$}{
%    $g_i=0;$\\
%    \Return  $g_i$;\\
%}
%\Else{
%$p_k = BandwidthProbability(\mathbf{b}^k_d,G);$\\
%$s_i=s_i*p_k;$
%}
%}
%\If{$s_i>=  \beta_{\gamma_{i}}$}{
%  update network remaining capacity;\\
%  $g_i=1;$\\
%}
%    \Return  $g_i$;
%  \caption{Fixed admission control for $\mathsf{BATE}$}
%  \label{admission}
% \end{algorithm}
 
 

%We now describe Traffic Engineering for Diverse Availability Targets ($\mathsf{BATE}$) in details.


%Like other WAN TE schemes,  we model the inter-DC WAN as a  graph $G=(V,E)$, where the node set $V$ represents data center site and
%edge set $E$ represents links between them.
%$c_e$ is the capacity of link $e \in E$.
%The pre-selected tunnels $T_{k}, \forall k \in K$ should be given.
%and each pair $(a,b)$ corresponds to a flow to be sent from the source node $a$ to the destination node $b$.
%We now introduce the admission control model.



%Each request $d \in D$ of service $j \in J$  needs $d_{ijk}$ bandwidth over DC pair $k$ and the availability target for service $j$ is $\beta_{j}\%$.
%The importance of service $j$ is denoted as $w_j$.
%As Table \ref{Notation} shown, input includes the following parts:
%(1) Network topology and network scenarios.
%The input includes two parts:
%(1) Network states and topology.
%For a given inter-DC WAN,  $c_e$ is the capacity of link $e$.
%The network state $\mathbf{z}\in Z$ is an uncertainty parameter with probability $p_{\mathbf{z}}$; 
%(2) Application level traffic matrix (TM) and tunnel sets for each flow pair.
%In traditional TE, a traffic matrix is a two dimensional array that describes the bandwidth demands from one point to another over the network.
%$\mathsf{BATE}$ extends this from two aspects:
%Firstly, it adds a dimension to identify applications.
%Let $J$ denote the flow identifiers and $j \in J$ is a three tuple $<AppID, Src,Dst>$, where $AppID, Src, Dst$ are  application identifier, source data center and destination data center, respectively.
%Secondly, each item in the traffic matrix contains minimal bandwidth demand $d_j$ , desired availability target vector $\bm{\beta}_j$ with size $u$, profit vector $\bm{\alpha}_j$ and importance $w_j$.
%In $\mathsf{BATE}$ framework,  each flow's tunnels set $T_j$ is also part of the input.
%Given the network topology, we can derive the tunnel sets between site pair in advance. 
%In Section \ref{evaluation}, we evaluate the impact of routing schemes (e.g., k-shortest paths, edge disjoint paths\cite{Bruno2013Dynamic}, oblivious routing\cite{SMORE}) on performance.
%Let $u_t^e$ present whether tunnel $t \in T_j$ passes link $e$ or not.
%We use $v_t^{\mathbf{z}}$ to denote whether tunnel $t$ is available ($v_t^{\mathbf{z}}=1$)  or not  ($v_t^{\mathbf{z}}=0$) under network state $\mathbf{z}$.

% \begin{figure}
%\begin{center}
%\includegraphics [width=0.9\columnwidth] {fig/return.pdf}
%\caption{Transfer return under different network states.}
%\label{Time-fig}
%\end{center}
%\end{figure}

% $S_j$ &\multicolumn{6}{|l}{Network service achieved availabitlity of $j$}\\
%\hline

%Besides whether the new arrival demand can be admitted, Algorithm \ref{greedy-1} also outputs pre-allocation results.


%Firstly, it sorts all the services j ∈ J in non-decreasing order according to the ratio of importance to efficient bandwidth demands, where the efficient bandwidth demands are derived as ∑k∈K d jk β j (Line 1).  It next sorts network scenarios and tunnels of each node pair (Line 2-3). For each service j ∈ J, the algorithm calls Algorithm 3 to check whether the remaining network capacity can satisfy its bandwidth-based availability (Line 7). If the network is unable to support its bandwidth-based availability and the current service is more profit than the total previous items (Line 8), the algorithm will try to recycle total resources and test that if allocating total network resources can support j (Line 10). If this is true, then algorithm will use the new allocation results (Line12-14). At last, for the remaining network resource, the algorithm will solve the following LP problem (Line 15):



\subsection{Traffic scheduling} \label{TE}

For \textit{admitted} demands (including the newly admitted ones), 
we carry out traffic scheduling to further optimize the bandwidth allocation periodically (e.g., every 10 minutes).
We model this as a linear programming problem, which maximizes the overall availability to be achieved (in a probabilistic sense), under the constraint that all bandwidth target must be met. Specifically, 

\begin{eqnarray} \label{Smaller}
 \sum_{t\in T_{k}} f_{d}^t \ge \mathbf{b}^k_d, \quad  \forall d \in \hat{D}, k \in K
\end{eqnarray}
%To satisfy each BA demand's availability target and we model the traffic scheduling as a linear programming.
%(2) To provide high availability, networks are over-provisioned,  and the traffic engineering should route requests with route all admitted requests with small induced network overload, so that more network resources could be used to accommodate future requests;
%(3) The algorithm should be fast enough to derive the routing solution.
%We begin with the constraints.



%To provider high availability with minimum induced overload, we restrict each demand's total allocated bandwidth smaller than its real need, i.e., 



For an s-d pair $k$ of BA demand $d$, we use $R_{dk}^{\mathbf{z}}$ to denote the ratio of the effective bandwidth under network scenario $\mathbf{z}$ to the demanded bandwidth, which is defined as:
 \begin{eqnarray} \label{R2}
R_{dk}^{\mathbf{z}}=\frac{\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d}, \quad \forall  d \in \hat{D}, k \in K, \mathbf{z}\in Z
\end{eqnarray}
%Figure \ref{Time-fig} gives an illustration on resource allocation return.
%For a given bandwidth allocation vector, it plots the sorted return and the corresponding probability.
%$R_{dk}^{\mathbf{z}}$ describes the allocation return under network scenario $\mathbf{z}$.
Here, our consideration is tunnel $t$ might be unavailable (i.e., $v_t^{\mathbf{z}} =0$) under a network scenario, but if $R_{dk}^{\mathbf{z}} \ge 1$ holds, then this scenario is still \textit{qualified}, i.e., 
\[
\mathbf{z} \propto <d, \{f_d^t\}> \quad \Leftrightarrow \quad \forall k, R_{dk}^{\mathbf{z}} \ge 1
\] 
%If for every source-destination pair $k$, the total reserved bandwidth through all available tunnels is larger than $\mathbf{b}^k_d$, then the bandwidth demand can still be satisfied, and network scenario $\mathbf{z}$ can be regarded as \textit{qualified}, i.e., 
%$R_{dk}^{\mathbf{z}} \ge 1, \forall k \in K$.
To meet the availability target, we should guarantee the total probability of the qualified scenarios 
is no less than the availability target, i.e., $\sum_{R_{dk}^{\mathbf{z}} \ge 1} p_{\mathbf{z}} \ge \beta_d$.
%for each s-d pair $k$ of demand $d$.
%$\forall d \in \hat{D}, k \in K$.
%This is a step function and we define the following potential function:
% 
%  \begin{eqnarray} \label{B3}
%\sum_{R_{dk}^{\mathbf{z}} \ge 1} p_{\mathbf{z}} \ge \beta_i, \forall d \in \hat{D}
%\end{eqnarray}
%$R_{dk}^{\mathbf{z}}$ is the ratio of allocated bandwidth to  demand over node pair $k$ under network scenario $\mathbf{z}$.
However, this condition will result in an mixed interger linear programming problem, 
and we choose to relax it and sovle the following linear programming problem.

Let $B_d^{\mathbf{z}}$ denote the lower bound of $R_{dk}^{\mathbf{z}}$ over all s-d pairs, i.e.,
\begin{eqnarray} \label{B2}
B_d^{\mathbf{z}} \le R_{dk}^{\mathbf{z}}, \quad  \forall  d \in \hat{D}, k \in K, \mathbf{z}\in Z
\end{eqnarray}
%It is obvious  $B_d^{\mathbf{z}} \le 1$ and $B_d^{\mathbf{z}}=1$ presents scenario $\mathbf{z}$ is qualified, therefore, 
and use $B_d^{\mathbf{z}}\times p_{\mathbf{z}}$ to roughtly represent the availability that can be achieved   
under network scenario  $\mathbf{z}$, which is set to be no smaller than the availability target, i.e., 
\begin{eqnarray} \label{Achieved}
\sum_{\mathbf{z}\in Z}B_d^{\mathbf{z}}\times p_{\mathbf{z}} \ge \beta_{d},  \quad {\forall d \in \hat{D}}
\end{eqnarray}

Besides, the bandwidth allocation result $f_{d}^t$ should be non-negative and limited by the network capacity, i.e.,
\begin{equation}
f_{d}^t \ge 0, \quad\forall d \in D ,  k\in K,   t\in T_{k}
\label{constraint-f2}
\end{equation}
%Then total traffic through link $e$ should not be larger than its capacity $c_e$:
and
\begin{equation}
 \sum_{d \in \hat{D}}\ \sum_{k\in K, t\in T_{k}}f_{d}^tu_t^e \le c_e, \quad \forall e \in E
\label{constraint-e2}
\end{equation}

Finally, our traffic scheduling will minimize the overall bandwith allocated to all admitted demands 
under the above constraints, i.e., 
%thus,  we can finally give the formulation of traffic engineering formulation: 
\begin{equation}
\small
\begin{aligned} \label{BATE-TE}
&minimize \sum_{d \in \hat{D}, k \in K, t \in T_k} f_d^t  \\
&\begin{array}{r@{\quad}r@{}l@{\quad}l}
s.t.  (\ref{Smaller}), (\ref{R2}), (\ref{B2}), (\ref{Achieved}), (\ref{constraint-f2}), (\ref{constraint-e2})
%\begin{cases}
% \sum_{d \in \hat{D}}\ \sum_{k\in K}\sum_{t\in T_{k}}f_{d}^tu_t^e \le c_e, &\text{$\forall e \in E $.}\\
%R_{dk}^{\mathbf{z}}=\frac{\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d} , &\text{$\forall  d \in \hat{D}, \mathbf{z}\in \mathbf{z},  k \in K$.}\\
%R_{dk}^{\mathbf{z}}>=B_d^{\mathbf{z}},&{\forall d \in \hat{D}, \mathbf{z} \in Z, k\in K.} \\
%\sum_{\mathbf{z}\in \mathbf{z}}B_d^{\mathbf{z}}\times p_{\mathbf{z}} \ge \beta_{\gamma_{i}},&{\forall d \in \hat{D}} .\\
% \sum_{t\in T_{k}} f_{d}^t \le \mathbf{b}^k_d, &{  \forall d \in \hat{D}, k \in K}.\\
%f_{d}^t \ge 0,&{\forall d \in \hat{D} , \forall k\in K,\forall t\in T_{k}}.\\
%\end{cases}\\
\end{array}
\end{aligned}
\end{equation}

% \begin{figure}[t]
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/pic.pdf}
%\caption{An example of concurrent failure number based pruning tree, where the $i$-th layer denotes concurrent $i$ link failures could happen. Network scenarios (red) located in layer 3, 4 are pruned in this example.}
%\label{pic-fig}
%\end{center}
%\end{figure}

Solving this LP problem directly is possible, but as it considers every possible network scenario, 
the complexity will increase exponentially with the network size.
%For example, we consider a small inter-DC WAN which contains 3 links and  5 events.
%Each link has $2^5=32$ states and the network includes $32^3=32768$ states in total. 
%In reality, network scale is much larger and  the states volume will increase exponentially.
For instance, the B4 topology \cite{B4} has 12 nodes and 38 links,
so there are totally $2^{38}$ network failure scenarios (when only link failures considered).
%The explosive network scenarios make it impossible to solve $\mathsf{BATE}$ problem.
Therefore, an important question is \textit{how to effectively reduce the problem size without 
affecting the result significantly}. 
TEAVAR \cite{Teavar} prunes a scenario if its probability is smaller than a threshold. However, such a threshold is difficult to choose, and an enumeration of all 
possible scenarios is still needed. Instead, we use a much faster pruning method, 
where at most two concurrent link failures will be considered, 
and all the remaining scenarios will be aggregated into one special \textit{unqualified} scenario. 
In this way, the set of scenarios $Z$ and the corresponding probabilities $\{p_\mathbf{z}\}$ can be efficiently computed. 

\iffalse
This method is easy but the accuracy and efficiency tradeoff threshold is hard to decide.
A large threshold could significantly accelerate the algorithm since large amount of network scenarios are cut off, however, the accuracy might decrease.
Instead, we propose to cut off network failure scenarios in which \textit{concurrent link failure number} is larger than $l$.
We think this method is much more practical since most scenarios could hardly happen (e.g., more than two links fail simultaneously). 
Therefore, we can ignore these scenarios.
To provide an upper bound of the traffic engineering problem, we collapse all the pruned scenarios into a single scenario with probability equal to the sum of probabilities of the pruned scenarios and regard that scenario as an unsafe one.
\fi
%Figure \ref{pic-fig} depicts an example of a simple network with four links when $l=2$. 
%The root node denotes all
%links are available (i.e., $[1111]$). 
%The $i$-th layer denotes concurrent $i$ link failures could happen. 
%Network scenarios (red) located in layer 3, 4 are pruned (the root node is regarded as layer 0).
%\begin{equation}
%\small
%\begin{aligned} \label{BATE-TE}
% &maximize  \sum_{i\in I} \sum_{\mathbf{z}\in \mathbf{z}}B_d^{\mathbf{z}}\times p_{\mathbf{z}}\\
%&\begin{array}{r@{\quad}r@{}l@{\quad}l}
%s.t.
%\begin{cases}
% \sum_{d \in D}\ \sum_{\forall k\in K: t\in T_{k}}f_{d}^tu_t^e \le c_e, &\text{$\forall e \in E $.}\\
%R_{dk}^{\mathbf{z}}=\frac{\sum_{k\in K}\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d} , &\text{$\forall  d \in \hat{D}, \mathbf{z}\in \mathbf{z},  k \in K$.}\\
%R_{dk}^{\mathbf{z}}>=B_d^{\mathbf{z}},&{\forall d \in \hat{D}, \mathbf{z} \in Z, k\in K.} \\
%\sum_{\mathbf{z}\in \mathbf{z}}B_d^{\mathbf{z}}\times p_{\mathbf{z}} \ge \beta_{\gamma_{i}},&{\forall d \in \hat{D}} .\\
% \sum_{t\in T_{k}} f_{d}^t \le \mathbf{b}^k_d, &{  \forall d \in \hat{D}, k \in K}.\\
%f_{d}^t \ge 0,&{\forall d \in \hat{D} , \forall k\in K: t\in T_{k}}.\\
%\end{cases}\\
%\end{array}
%\end{aligned}
%\end{equation}

%When all demands have the same availability targets , similar to TEAVAR \cite{Teavar},  $\mathsf{BATE}$ is also able to strike a good balance between network utilization and availability.
%Compared with TEAVAR,  $\mathsf{BATE}$ has the following two advantages:
%Firstly, $\mathsf{BATE}$  is not as aggressive as TEAVAR. 
%Flows can gain more bandwidth than their needs under TEAVAR \cite{Teavar}, while they can't exceed demands under  $\mathsf{BATE}$.
%Therefore, a more reasonable balance between network utilization and availability can be achieved when network is much over-provisioned.
%Secondly, when network is under-provisioned, $\mathsf{BATE}$ can make demands with stringent availability requirements pass links with high reliability, therefore, their availability targets can be guaranteed, but TEAVAR is unable differentiate demands from tenants.
%Compared with TEAVAR \cite{Teavar}, our traffic engineering has the following two advantages:
%(1)




\subsection{Failure recovery} \label{backup}
When failures occur and any tunnel becomes unavailable, traffic can be redistributed across the surviving tunnels. To reduce recovery time, $\mathsf{BATE}$ proactively computes backup allocation strategies for potential failure scenarios, so that the surviving tunnels can be used immediately, and packet loss 
can be mitigated \footnote{Here we only consider backup allocations for one link, while this scheme can be easily extended to deal with concurrent failures.}. 
%Previous tunnel-based traffic engineering schemes (such as FFC) keep link utilization low to prevent congestion when there are failures.
%Different from them, links have less vacant capacity under $\mathsf{BATE}$, therefore, when there are failures, re-routing might cause congestion.
%When a link $e'$ fails, all tunnels through this link  fails.

 
%Each tunnel belongs to one flow pair.
%We can divide all node pairs into two groups, i.e.,  victim node pairs and normal node pairs, which is similar to \cite{sentinel}.
%Let $H$ denote all the victim node pairs when link $e'$ fails.
%Assume link $e'$ will fail, the normal tunnel set is $Q_{k}$ and the backup tunnel set is $Q'_{k}$.
%In reality, the backup tunnel set can be pre-computed by the network operators beforehand.
For example, in Figure \ref{recovery}, there are two users, and the link capacity is 1 everywhere.  One user requests a bandwidth of 1 from DC1 to DC2, while the other one requests a bandwidth of 1 from DC1 to DC4.
Figure \ref{recovery}(a) shows the original bandwidth allocation when no failures occur, 
%if we assume the link between DC2 and DC4 is down, then DC1$\to$DC4 is victim node pair and DC1$\to$ DC2 is normal pair. 
%the backup tunnel set $Q'$=\{DC1$\to$DC2$\to$DC3$\to$DC4\} and normal tunnel set is 
%$Q$ =\{DC1$\to$DC3$\to$DC4\} for the victim node pair DC1$\to$DC4.
%Let $M_e$ denote the used bandwidth of link $e$ by normal node pairs,
and Figure \ref{recovery}(b) depicts the backup allocation pre-computed for a failure of link DC2$\to$DC4.
%and the result will be put into practice when link DC2 $\to$ DC4 failure is detected.

However, service providers have to refund credits to users if their bandwidth availability targets according the agreed SLAs are violated (see $\S$\ref{background}), both for users affected \textit{directly} by the failure and for users \textit{collaterally} affected by the recovery procedure. 
Our failure recovery scheme takes this economic factor into account in the way as follows.
%
%
%
\begin{figure}[t]
\centering
\subfigure[Original allocation]{
\includegraphics[width=0.2\textwidth]{fig/recovery-1.pdf}}
\subfigure[Backup allocation]{
\includegraphics[width=0.2\textwidth]{fig/recovery-2.pdf}}
%\caption{There are two services from DC1 to DC4 and link capacity is 1 everywhere. (a) shows the original TE allocation. (b) shows the backup allocation result when assuming link DC2 $\to$ DC4 is broken.  }
\caption{Bandwidth allocation for DC2$\to$DC4 failure}
\label{recovery}
\end{figure}
%\begin{equation}
%\small
%\begin{aligned} \label{BATE-recovery}
% &maximize  \sum_{i\in I_a} \sum_{\mathbf{z}\in \mathbf{z}}B_d^{\mathbf{z}}\times p_{\mathbf{z}}\\
%&\begin{array}{r@{\quad}r@{}l@{\quad}l}
%s.t.
%\begin{cases}
%\sum_{i\in I_a}\sum_{k\in K}\sum_{t\in Q_{h}} f'_{ikt}u_t^e+ \sum_{i\in I_a}\sum_{k\in K}\sum_{t'\in Q_{h}} f''_{ikt'}L_{t'e} \le c_e, &\text{$\forall e \in E \setminus e' $.}\\
%R_{dk}^{\mathbf{z}}=\frac{\sum_{k\in K}\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d} , &\text{$\forall  d \in \hat{D}, \mathbf{z}\in \mathbf{z},  k \in K$.}\\
%R_{dk}^{\mathbf{z}}>=B_d^{\mathbf{z}},&{\forall d \in \hat{D}, \mathbf{z} \in Z, k\in K.} \\
%\sum_{\mathbf{z}\in \mathbf{z}}B_d^{\mathbf{z}}\times p_{\mathbf{z}} \ge \beta_{\gamma_{i}},&{\forall d \in \hat{D}} .\\
% \sum_{t\in T_{k}} f_{d}^t \le \mathbf{b}^k_d, &{  \forall d \in \hat{D}, k \in K}.\\
%f_{d}^t \ge 0,&{\forall d \in \hat{D} , \forall k\in K,\forall t\in T_{k}}.\\
%\end{cases}\\
%\end{array}
%\end{aligned}
%\end{equation}

For a specific network scenario $ \mathbf{z}$ (where one link failure occurs) in consideration, 
%let
%$f^{\mathbf{z}}_{ikt}$ denote bandwidth of $i$ over tunnel $t\in T^{ \mathbf{z}}_k$, where 
%$T^ { \mathbf{z}}_k$ denote the set of surviving tunnels for the s-d pair $k$ under scenario $ \mathbf{z}$.
the ratio of allocated bandwidth to a user's demanded bandwidth is 
\footnote{This is the same as equation (\ref{R2}), but we omit the superscript $\mathbf{z}$ of $R_{dk}^\mathbf{z}$.}

\begin{small}
\begin{eqnarray} \label{B-return}
R_{dk}=\frac{\sum_{t\in T_k} f^{t}_{d} v_t^{\mathbf{z}}} {\mathbf{b}^k_d}, \quad \forall  d \in \hat{D},  k \in K
\end{eqnarray}
\end{small}



If for every $k$, $R_{dk}$ is larger than its demand (i.e., $R_{dk} \ge 1$), 
then there is no problem since the demanded availability is still satisfied. 
However, if any $R_{dk}$ falls below 1, then the corresponding SLA will be violated. 
For simplicity, here we assume a simple pricing and refunding model, 
where the charge for serving a user demand $d$ is $g_d$, and if the SLA is violated, 
a fraction $\mu_d$ of $g_d$ will be refunded\footnote{There are also more complicated multi-stage SLAs, 
where $\mu_d$ can even be set to 1 under low availability. Our preliminary results show $\mathsf{BATE}$ will perform even better than the others under such more complicated SLAs.}. 
We use $r_d$ to denote the profit of demand $d$ with refunding, such that
%If it is between $\beta_j^{k}$ and $\beta_j^{k+1}$, then the profit is $\alpha_j^k$, where $1>\alpha_j^1>\alpha_j^2>...>\alpha_j^k$.
\begin{equation*}
r_d=
\begin{cases}
g_d &\text{if $R_{dk} \ge 1$ for every $k \in K$}\\
(1-\mu_{d})g_d &\text{Otherwise}
\end{cases}
\label{availability222}
\end{equation*}

%\begin{equation}
%A_d^{\mathbf{z}}=
%\begin{cases}
%1 &\text{$\forall k \in K:R_{dk}^{\mathbf{z}}\ge 1$}\\
%0&\text{Otherwise}
%\end{cases}
%,\forall  d \in D, \mathbf{z}\in \mathbf{z}.
%\label{CA}
%\end{equation}


We use an auxiliary integer variable $y_d \in \{0, 1\}$ to denote the violation condition, 
where $y_d=1$ means no violation. 
Then the profit $r_d$ can be rewritten as 
%\begin{equation}
%\label{A-SSS2}
%y_d \in \{0,1\} &{\forall d \in \hat{D}} \\
%r_d  =  g_d \times \Big(y_d+(1-\mu_{d})\times (1-y_d)\Big) &{\forall d \in \hat{D}} \\
%R_{dk} <  M\times y_d+1-y_d &{\forall d \in \hat{D},  k\in K} \\
%R_{dk} \ge   y_d &{\forall d \in \hat{D},  k\in K}
%\end{equation}
\iffalse
\begin{equation}
\label{A-SSS2}
\begin{split}
&y_d \in \{0,1\}, \quad  \forall d \in \hat{D}\\
&r_d  =  g_d \times \Big(y_d+(1-\mu_{d})\times (1-y_d)\Big), \quad {\forall d \in \hat{D}}\\
&R_{dk} <  M\times y_d+1-y_d ,\quad{\forall d \in \hat{D},  k\in K} \\
&R_{dk} \ge   y_d,\quad {\forall d \in \hat{D},  k\in K}
\end{split}
\end{equation}
\fi
\iffalse
\begin{equation}
\label{A-SSS2}
\begin{array}{rcll}
y_d &\in &\{0,1\} &{\forall d \in \hat{D}} \\
r_d & = & g_d \times \Big(y_d+(1-\mu_{d})\times (1-y_d)\Big) &{\forall d \in \hat{D}} \\
R_{dk}& < & M\times y_d+1-y_d &{\forall d \in \hat{D},  k\in K} \\
R_{dk}& \ge &  y_d, &{\forall d \in \hat{D},  k\in K}
\end{array}
\end{equation}
\fi
\begin{equation}
\label{A-SSS2}
\begin{array}{rll}
y_d \in &\!\!\{0,1\} &{\forall d \in \hat{D}} \\
r_d  = &\!\! g_d \times \Big(y_d+(1-\mu_{d})\times (1-y_d)\Big) &{\forall d \in \hat{D}} \\
R_{dk} < &\!\! M\times y_d+1-y_d &{\forall d \in \hat{D},  k\in K} \\
R_{dk} \ge &\!\!  y_d, &{\forall d \in \hat{D},  k\in K}
\end{array}
\end{equation}
where $M$ is a constant large enough (e.g., at least larger than the upper bound of $R_{dk}$).

\iffalse
\begin{equation}
\begin{cases}
h_d=y_d+\mu_{d}*(1-y_d),&{\forall d \in \hat{D}.} \\
R_{dk} < M*y_d+1-y_d,&{\forall d \in \hat{D},  k\in K.} \\
R_{dk} >= y_d,&{\forall d \in \hat{D},  k\in K.} \\
y_d \in\{0,1\},&{\forall d \in \hat{D}.} \\
\end{cases}
\label{A-SSS2}
\end{equation}
\fi

Besides, the bandwidth allocation result $f_t^d$ should be nonnegative
and limited by the available network capacity. 
Let $w_e^{ \mathbf{z}}$ denote whether link $e$ is available under scenario $ \mathbf{z}$, 
then we have 
\begin{equation}
f^{t}_{d} \ge 0, \quad\forall d \in \hat{D} ,  k\in K,  t\in T_k.
\label{constraint-f3}
\end{equation}
and 
\begin{small}
\begin{eqnarray} \label{B-constraint}
\sum_{d\in \hat{D}}\sum_{k\in K, t\in T^{ \mathbf{z}}_k} f^{t}_{d}u_t^e \le c_e \times w_e^{ \mathbf{z}}, \quad \forall e \in E
\end{eqnarray}
\end{small}

Finally, the failure recovery scheme tries to maximize the total profit (after refunding) by 

\begin{equation}
\small
\begin{aligned} \label{BATE-recovery}
 &maximize  \sum_{d\in \hat{D}} r_d\\
&\begin{array}{r@{\quad}r@{}l@{\quad}l}
s.t.  (\ref{B-return}), (\ref{A-SSS2}), (\ref{constraint-f3}), (\ref{B-constraint})
\end{array}
\end{aligned}
\end{equation}

The above mixed-integer linear programming  (MILP) problem can be proved to be NP-hard, 
and the proof details can be found in Appendix \ref{appendix1}.
To efficiently solve this problem, we further propose a 2-approximation greedy algorithm.
The key idea is to prioritize demands by the ratio of profit to the allocated bandwidth in a non-decreasing order.
Due to space limitations, the detailed algorithm, as well as the proof on its optimality, 
are put into Appendix \ref{B}.

\iffalse
\begin{lemma}\label{NP-hard}
The failure recovery problem is NP-hard.
\end{lemma}
The detailed proof can be found in Appendix \ref{appendix1}.

%For the second challenge, although the powerful mathematical optimization solver such as Gurobi \cite{gurobi} can derive the solution, it might be too slow to converge before next TE scheduling epoch.
In this case,  we propose a greedy algorithm to attain an approximate solution whose pseudocode is shown in Appendix \ref{B}.
The key idea of the greedy algorithm is to give priority to demands in non-decreasing order by the ratio of profit to its efficient bandwidth demands.
\textit{We also prove the approximation of the greedy algorithm is 2  in Appendix \ref{B}}. 
%The greedy algorithm can solve $\mathsf{BATE}$  problem in  $O(|J||Z||T_k||E|)$ time.
%We further explore the accuracy of greedy algorithm in Section \ref{evaluation}.
\fi



\section{system implementation} \label{system}
%In this section, we show the design and some implementation details of $\mathsf{BATE}$ prototype.
%In this section, we show the design of $\mathsf{BATE}$ (Traffic Engineering for Diverse Availability Targets) system, which is able to provide high bandwidth-based availability to services.
%$\mathsf{BATE}$ system tries to achieve the following objectives:
We have implemented $\mathsf{BATE}$ on the Linux platform. Figure \ref{pic-overview} shows the whole system architecture, which contains one controller, multiple brokers (one for each DC) and multiple clients (one for each host). The controller is responsible for most decision work of $\mathsf{BATE}$, including admission control, traffic scheduling, and failure recovery, while the brokers and clients are responsible for bandwidth enforcement.
It works as follows:
When a user submits a demand to the controller, the admission control module determines whether the demand can be admitted or not (see $\S$ \ref{admission_control}).
If the demand is admitted, this module will also allocate its demanded bandwidth on appropriate paths for the first time, and notify the brokers for enforcement. The online scheduler module performs traffic scheduling (see $\S$ \ref{TE}) periodically  (e.g., every 10 minutes) to further optimize the availability expectation of all active demands. In addition, 
for potential link failures, it also pre-computes backup allocation strategies that will be activated if any link failure indeed happens (see $\S$ \ref{backup}). These central decisions are distributed to the brokers for bandwidth enforcement. The brokers in each DC monitor link status and bandwidth consumption, report these statistics to the central controller, and ask the clients to implement appropriate rate limit on end hosts. 

%The controller performs traffic engineering algorithm every $T_a$ (e.g., 10) minutes to derive the bandwidth allocation results.

% then it limits rate according to the responses from its data center broker.


\textbf{Controller} is the brain of the whole system. It is responsible for allocating WAN level bandwidth, and orchestrates all activities with a global view. The four main components in Controller are as follows.
(1) Offline Routing.
This module maintains the WAN level network topology, and computes TE tunnels between each node pair
 (i.e., $T_k,\forall \text{s-d pair} \ k \in K$), using certain routing algorithms (oblivious routing \cite{SMORE}, k-shortest path \cite{swan}, etc.). 
 %When topology changes (e.g., remove a node) are reported to controller, the TE Routing module will perform routing algorithm (oblivious routing \cite{SMORE}, k-shortest path \cite{swan}, etc.) to derive all the possible tunnels between each node pair (i.e., $T_k,\forall k \in K$) offline.
These tunnels are used by the admission control module and the online scheduler module as input variables;
(2) Admission Control.
When a BA demand is submitted, this module uses the admission control algorithm (see $\S$ \ref{admission_control}) to reject it, or accept it and allocate bandwidth over the tunnels in nearly real-time.
The results are sent to the corresponding brokers.
(3) Online Scheduler. 
Periodically, This module performs traffic scheduling (see $\S$ \ref{TE}) according to the bandwidth availability demands submitted by users, so that the availability can be optimized in a probabilistic sense. 
It also pre-computes backup allocation (see $\S$ \ref{backup}) for some potential link failures.
For each user demand, the normal bandwidth and backup bandwidth allocated over each tunnel (i.e., $f_i^t$) are then sent to the corresponding brokers. 
In addition, our system also supports several other TE algorithms including SWAN \cite{swan}, FFC \cite{FFC} and TEAVAR \cite{Teavar}.
(4) Communication Channel.
This module is responsible for communication with brokers, where we use long-lived TCP connections to avoid unnecessary delay. 
In addition, controller failures can be remedied by using multiple replications, 
where the master controller is elected by the Paxos \cite{lamport1998the} algorithm.
%To handle controller failure, the controller also replicate controllers and places them in different data centers, then Paxos \cite{lamport1998the} is used to elect one controller as the master.

\begin{figure}
\begin{center}
\includegraphics [width=0.7\columnwidth] {fig/inter-DC-WAN-V3.pdf}
\caption{System architecture of $\mathsf{BATE}$}
\label{pic-overview}
\end{center}
\end{figure}

\textbf{Broker} takes care of the data center it resides in. It consists of three modules:
%(1) Demand  Aggregator.
%After receiving demands from hosts, brokers will aggregate them according to service over each node pair (i.e., $d_{jk}$).
%They are sent to controller along with the availability targets of each service (i.e., $\beta_j$);
%%The necessary information includes the application identifier, source address, destination address and expected bandwidth demand.
%Demand Collector gathers traffic demands and aggregates them on the basis of  flow identifiers (i.e., $ < AppID,Src,Dst >$);
%(2) \textit{Demand Predictior}.
%In $\mathsf{Dionysus}$ framework, resources should be able to be dynamic allocated according to the demands of applications.
%However, future application bandwidth demands can't be known beforehand, which makes it hard to adjust in real-time.
%The broker keeps the empirical demands collected by Demand Collector.
%With these history data, 
%Demand Predictior will predict the future bandwidth demands with data-driven algorithms such as ARIMA\cite{ARMA}, reinforcement learning \cite{Pensieve}; 
(1) Bandwidth Enforcer.
It receives the bandwidth allocation results (i.e., $f_i^t$) from controller, sends them to the corresponding hosts, and limits the actual traffic rate in each tunnel in case something is wrong on the end hosts;
(2) Network Agent.
We use commodity SDN switches at data center edges to connect DCs into an Inter-DC WAN.
The network agent runs in a SDN controller (we use floodlight \cite{floodlight}), 
and uses the OpenFlow \cite{openflow} protocol to installs and updates forwarding rules 
on the switches in the same DC. 
To reduce rule complexity, our system uses a label-based forwarding scheme, 
where the first 12 bits of a VxLAN ID represent different demands, 
and the last 12 bits represent different tunnels.
Therefore, 4096 demands and 4096 tunnels can be supported simultaneously, 
which can be further expanded if necessary. 
In this way, a flow (i.e., traffic corresponding to a BA demand) is marked with a label at the ingress switch, and the succeeding switches use this label for forwarding.
Group tables in the switch pipelines are used for flow splitting (i.e., traffic corresponding to a BA demand 
can be split into multiple sub-flows and transmitted in multiple tunnels).
%Flow table just maps the packet based on the destination and the group tables consist of bandwidth weight for each label.
Besides, the network agent also tracks the network topology, reports any change or failure to the central Controller module, and monitors the actual traffic rate. 
(3) Communication Channel.
This component is responsible for communication with the central Controller.
%In reality, many switches are able to detect link failure  \cite{FDT,olfd}.
%Brokers exploit this and send the failure information to controller after the edge switches detect link failures.

\textbf{Client} sits on each host to carry out rate limiting. 
%A shim in the host OS estimates its demand to each remote DC for the next $T_b$ minutes 
%and asks the broker for an allocation.
%Each client has a RPC daemon that submits its SLA to its DC broker every  $T_b$ (e.g., 1) minutes.
It consists of a daemon and a kernel module. The former recieves bandwidth allocation results from the site broker, while the latter sits between the TCP/IP stack and the Linux Traffic Control (TC) module to control outbound traffic rate. 

\iffalse
$\mathsf{BATE}$ system adopts OpenFlow\cite{openflow}  to install and update forwarding rules.
To reduce forwarding complexity, our prototype uses label-based forwarding.
%Traditionally,  VLAN header is tagged on the ingress switches and VLAN ID is often used to differentiate tunnels \cite{swan}.
%We abandon this as VLAN ID only contains 12 bits, which is insufficient to present bandwidth demand between each DC pair.
%Instead, 
We adopt VxLAN ID which contains 24 bits, where the first 12 bits to present demands and the remaining 12 bits are used to distinguish tunnels between each node.
Therefore, $2^{12}=4096$ tenants' demands and $2^{12}=4096$ tunnels can be supported simultaneously.
Flows are marked with a label at the ingress switch and the remaining switches can just read the labels and forward the packets.
To implement unequal bandwidth splitting, our prototype uses group table in its OpenFlow pipeline.
Flow table just maps the packet based on the destination and the group tables consist of bandwidth weight for each label.
\fi


%In reality, service providers often use a piecewise function to derive refunding credits for violating availability targets (see $\S$\ref{background}) and we can modify profit function to model this.
%Let $\{1,\beta_j^1,\beta_j^2....\}$ present the availability target list of service $j$.
%The corresponding profit list can be denoted as $\{1,\alpha_j^1,\alpha_j^2...\}$.
%If the achieved availability (i.e., $S_j$) is larger than the highest desired availability target (i.e., $\beta_j^1$), the profit is 1.
%If it is between $\beta_j^{i}$ and $\beta_j^{i+1}$, the profit is $\alpha_j^i$.
%Otherwise, it is a non-decreasing function  of $f_{jtk}$.
%Finally, the profit function can be formulated as:
%
%
%\begin{equation}
%\forall  j \in J : g_j=
%\begin{cases}
%1 &\text{$1>S_j \ge \beta_j^1$}\\
%\alpha_j^1 &\text{$\beta_j^1>S_j \ge \beta_j^2$}\\
%... &\text{$...$}\\
%\mu(f_{jtk})  &\text{$\beta_j^x>S_j \ge0$}
%\end{cases}
%\label{availability}
%\end{equation}
%
%Table \ref{target} shows how to explicitly model the availability targets of services under $\mathsf{BATE}$.
%For example, the profit for Microsoft Azure Traffic Manager service \cite{azure} shown in the first line of Table \ref{target}, can be modeled as:
%\begin{equation}
%g=
%\begin{cases}
%1 &\text{$1>S \ge 99.99\%$}\\
%0.9 &\text{$99.99\%>S \ge 99\%$}\\
%0.75 &\text{$99\%>S \ge0$}
%\end{cases}
%\label{availability222}
%\end{equation}


 \begin{figure}[t]
\begin{center}
\includegraphics [width=0.6\columnwidth] {fig/testbed.pdf}
\caption{Testbed topology with failure probabilities}
\label{testbed}
\end{center}
\end{figure}
%
%
% \begin{figure}
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/evaluation/testbed-admit.pdf}
%\caption{Admission control comparison.}
%\label{admission-control2}
%\end{center}
%\end{figure}
%
%
%
%\begin{figure}
%\centering
%\subfigure[Availability satisfaction]{
%\includegraphics[width=0.23\textwidth]{fig/evaluation/fake-testbed-target.pdf}}
%\subfigure[Admission time]{
%\includegraphics[width=0.23\textwidth]{fig/evaluation/testbed-public-2.pdf}}
%\caption{Traffic scheduling comparison.}
%\label{testbed-traffic-scheduling}
%\end{figure}




\begin{figure*}
\centering
\subfigure[Admission control]{
\includegraphics[width=0.24\textwidth]{fig/evaluation/testbed-admit.pdf}}
\subfigure[Traffic scheduling]{
\includegraphics[width=0.24\textwidth]{fig/evaluation/fake-testbed-target.pdf}}
\subfigure[Profit loss after failures]{
\includegraphics[width=0.24\textwidth]{fig/evaluation/testbet-loss2.pdf}}
\subfigure[Overall profit gain]{
\includegraphics[width=0.24\textwidth]{fig/evaluation/overall-profit.pdf}}
\caption{Testbed evaluation with Poisson demand arrivals}
\label{e2}
\end{figure*}


%The average error of traffic scheduling is within 8\% and the profit loss of greedy algorithm is less than 10\%. 

% \begin{itemize}
%  \item \textbf{What is the benefit of  $\mathsf{BATE}$ ?}
%  In $\S$ \ref{testbedresults}, we show $\mathsf{BATE}$ proactively considers link failure probability as well as the diverse demands of services proactively and it increases the percentage of services that satisfy their availability SLAs by up to 40\%. 
%  \item \textbf{How does $\mathsf{BATE}$ compare with the state-of-the-art traffic engineering TE schemes?} In $\S$ \ref{scales}, we show the consistent performance of  $\mathsf{BATE}$ under various topologies and traffic. $\mathsf{BATE}$ performs  up to 30\%, 40\%, 50\%, 75\% better than TEAVAR\cite{Teavar}, SMORE\cite{SMORE}, SWAN\cite{swan} and FFC\cite{FFC} on bandwidth-availability guarantee, respectively.
% \item \textbf{ How effect is  $\mathsf{BATE}$ under different parameter settings ?} In  $\S$ \ref{settings}, we show the stable performance of $\mathsf{BATE}$  framework. It doesn't depend on specific profit function, routing schemes, etc.
% \item \textbf{ How robust is the solution? } In $\S$ \ref{purning}, we evaluate the accuracy and efficiency of the solutions. The average error between the optimal and approximate solutions is within 8\% and we also show the algorithm is fast and accuracy enough to use in large scale network. 
% \end{itemize}



%\begin{description}
%  
%\item[fread] 
%
%\item[Fred] a person's name, e.g., there once was a dude named Fred
%  who separated usenix.sty from this file to allow for easy
%  inclusion.
%\end{description}

\section{Evaluation} \label{evaluation}
In this section, we use a small testbed and larger scale trace driven simulations  to evaluate the performance of $\mathsf{BATE}$. 
On the testbed, we also implement another two state-of-the-art TE algorithms considering network availability, i.e., FFC \cite{FFC} and TEAVAR \cite{Teavar}. 
For simulation, we implement more TE algorithms, including SWAN \cite{swan}, SMORE \cite{SMORE} and B4 \cite{B4}. 
All codes for our implementation and evaluation will be published, 
so that interested readers can use them in their own environments or make further improvements. 
Our main results are as follows:

%We begin by describing the experimental framework ($\S$ \ref{setting}). 
%We show the performance of $\mathsf{BATE}$ through answering the following specific questions:
%compared with the latest TE schemes like FFC and TEAVAR, BATE can meet the availability SLAs of 40% more bandwidth demands, and when network failure causes SLA violations, it can retain 30% more profit under a simple pricing and refunding model.
(1) $\mathsf{BATE}$ consistently outperforms latests TE algorithms under various topologies, traffic matrices and failure scenarios. With $\mathsf{BATE}$, 23\%$\sim$60\% more BA demands can be successfully fulfilled under normal loads, and 10\%$\sim$20\% more profit can be retained when failures occur.

(2) $\mathsf{BATE}$ achieves a good tradeoff between efficiency and optimality. 
Compared with the optimal solutions, (i) our admission control algorithm can speed up the admission procedure by 30$\times$ at the expense of less than 4\% false rejections, 
(ii) our pruning-augmented scheduling algorithm runs $10^2 \sim 10^4 \times$ faster while wasting only 6\% total bandwidth, 
and (iii) our greedy failure recovery algorithm can reduce the reaction time by 100$\times$, 
where profit loss is only around 10\%.

(3) $\mathsf{BATE}$ has a stable performance across different network topologies, 
demand matrices and routing schemes. 

% \begin{itemize}
%  \item \textbf{What is the benefit of  $\mathsf{BATE}$ ?}
%  In $\S$ \ref{testbedresults}, we show $\mathsf{BATE}$ proactively considers link failure probability as well as the diverse demands of services proactively and it increases the percentage of services that satisfy their availability SLAs by up to 40\%. 
%  \item \textbf{How does $\mathsf{BATE}$ compare with the state-of-the-art traffic engineering TE schemes?} In $\S$ \ref{scales}, we show the consistent performance of  $\mathsf{BATE}$ under various topologies and traffic. $\mathsf{BATE}$ performs  up to 30\%, 40\%, 50\%, 75\% better than TEAVAR\cite{Teavar}, SMORE\cite{SMORE}, SWAN\cite{swan} and FFC\cite{FFC} on bandwidth-availability guarantee, respectively.
% \item \textbf{ How effect is  $\mathsf{BATE}$ under different parameter settings ?} In  $\S$ \ref{settings}, we show the stable performance of $\mathsf{BATE}$  framework. It doesn't depend on specific profit function, routing schemes, etc.
% \item \textbf{ How robust is the solution? } In $\S$ \ref{purning}, we evaluate the accuracy and efficiency of the solutions. The average error between the optimal and approximate solutions is within 8\% and we also show the algorithm is fast and accuracy enough to use in large scale network. 
% \end{itemize}



%\begin{description}
%  
%\item[fread] 
%
%\item[Fred] a person's name, e.g., there once was a dude named Fred
%  who separated usenix.sty from this file to allow for easy
%  inclusion.
%\end{description}



\subsection{Testbed evaluation}\label{testbed-evaluation}
\textbf{Testbed setup.}
We build a testbed with 6 servers to emulate a small inter-DC WAN connecting 6 DCs, as shown in Figure \ref{testbed}. 
The inter-DC links run at 1Gbps, and we add 100ms delay on each link to emulate a WAN environment.
Each server is equipped with 4 Intel Xeon E5-2620 CPUs, 64GB memory and 4 Ethernet NICs, 
and on each server we start 20 VMs, which are all connected to an Open vSwitch \cite{openvswitch}.
The VMs run CentOS 7 and use Linux v4.15.6 kernel \cite{Kermel} with our rate limiting module plugged in. 
%We start 21 VMs (1 vCPU, 2GB memory, 100GB disk) in each server, where 20 VMs are used to emulate the hosts and 1 VM runs as the broker.
Every second, we emulate link failures by shutting down the corresponding network interfaces 
under the failure probabilities shown in Fig. \ref{testbed}, 
and bring them online after 3 seconds. 
We also deploy our controller and brokers on extra VMs. 
The network agent module in each broker uses Floodlight \cite{floodlight} to control the vSwitch, 
while the latter monitors link status and reports any failure to the former.
If not stated otherwise, we use 4-shortest paths between each source-destination pair as the tunnels in TE algorithms.

%Each VM has a demand and the it varies with a period of 3-minutes pattern just as \cite{swan}.  
%The importance of services are randomly chosen from \{1,2,3,4,5\} 
%All services have the same level of importance and availability targets of services are from the SLAs of Azure \cite{azure}.



%We mainly care about the following metrics:
%(1) Availability.  Bandwidth-based availability is measured as the time qualification ratio. A second is regarded as qualified if the gap between total measured bandwidth and bandwidth demand is less than 1\%;
%(2) Data loss ratio. We measure total bytes loss according to \textit{iperf} server side reports and packet counters in the switches, then finally derive the data loss ratio;
%(3) Demand rejection ration of different admission control algorithms;
%(4) Profit loss after link failures.
%The achieved  availability is computed as the fraction of qualified seconds, where . 
%We perform the experiment for a duration of 100 minutes.
%We deploy 5 applications and each instance will send traffic to other VMs located in the rest DCs.
%The importance of applications are randomly chosen from \{1,2,3,4,5\} with poisson distribution.
% Availability target of applications are randomly chosen from \{90\%, 95\%, 99\%, 99.9\%, 99.99\%\} with uniform distribution.

% \begin{figure}
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/testbed.pdf}
%\caption{Testbed topology. }
%\label{testbed}
%\end{center}
%\end{figure}


%In the following part, we also study the impact of probability distribution on performance.



% \begin{figure}
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/weibull.pdf}
%\caption{Weibull distribution  used in our evaluation, where shape and scale parameters are 0.8 and 0.0001, respectively. }
%\label{weibull}
%\end{center}
%\end{figure}







%


\textbf{Evaluations on continuous demand arrivals.}
We first conduct experiments where user demands are generated from models used 
in some latest inter-DC traffic scheduling algorithms \cite{Guaranteeings,Online-deadline,OWAN,Teavar}. 
For each source-destination pair, the arrival of user bandwidth demands follows a poisson process, 
and the demand duration follows an exponential distribution.  
The mean inter-arrival time we use is 2 minutes, and the mean duration is 5 minutes. 
The demanded bandwidth is uniformly distributed between 10 Mbps and 50 Mbps.
The availability targets and refunding ratio are chosen from the Azure SLAs \cite{azure}, 
and we assume a unit price is charged for 1 Mbps. 
Each experiment lasts 100 minutes and is repeated for 50 times, 
where link failures occur probabilistically, 

%The profit of each tenant (i.e., $w_i$) is proportional to his/her total bandwidth demand.
%Each experiment lasts 100 minutes and is repeated 20 times, where the error bars paints the maximal, minimal and mean value.
\textit{Admission control.} We evaluate how demands can be correctly admitted by $\mathsf{BATE}$, 
comparing with both the optimal admission strategy, 
and a strategy that assumes a \textit{fixed} bandwith allocation for demands that have already been admitted, which is effectively step (1) in $\mathsf{BATE}$.
Figure \ref{e2}(a) demonstrates that $\mathsf{BATE}$ performs closely to the optimal strategy, 
i.e., the false rejection ratio is at most 1\%, while the \textit{fixed} algorithm 
rejects least 10$\times$ more admissible demands. 
%This is because $\mathsf{BATE}$ fully considers the availability diversity of tenants.
%In contrast, \textit{Fixed} admission control can't change tenants' routing during runtime and such inflexibility leads to higher rejection rate. 

\textit{Traffic scheduling.} We evaluate, once a user demand is admitted, 
how often its bandwidth availability target can be met. 
Since we emulate different link failures according to their probabilities in each second, 
we can measure how the bandwidth a user actually uses deviates from its requirement. 
If such a downward deviation is less than 1\%, we regard the bandwidth availability as \textit{satisfied in that second}. Figure \ref{e2}(b) shows the overall fraction of satisfaction, under  different levels of availability requirements (i.e., 95\%, 99\% and 99.99\%). 
We note that, FFC-fixed (or TEAVAR-fixed) in the figure represents applying FFC (or TEAVAR) 
only to demands admitted by the fixed admission control strategy, 
where the total bandwidth required for the admitted demands is much lower. 
$\mathsf{BATE}$ always achieves the highest availability, even compared with FFC-fixed and TEAVAR-fixed. 
In particular, it has a clear advantage for high availability requirements (e.g., >99.99\%). 
%To further illustrate the difference in their mechanisms, 


\textit{Failure Recovery.} We evaluate when failures do occur and cause SLA violations, 
how profit loss can be mitigated by our failure recovery scheme.
Figure \ref{e2}(c) depicts the fraction of profit loss caused by SLA violations under three different admission control strategies, i.e., fixed, BATE-AD (which is the strategy $\mathsf{BATE}$ uses) and optimal, where baseline for each algorithm is the profit it can achieve when no failures occur. 
%\textbf{TBD: $\mathsf{BATE}$ can retain about 15\% more money ratio than FFC and TEAVAR when network fails.}
$\mathsf{BATE}$ always achieves the lowest loss ratio, while 
FFC also has a lower profit loss ratio due to its conservative bandwidth allocation,
and TEAVAR may cause around 5$\times$ higher profit loss.

\textit{Overall Profit.}
Figure \ref{e2}(d) plots the overall profit of $\mathsf{BATE}$, FFC and TEAVAR.
Due to its hard guarantee on bandwidth availability and its  profit maximization, 
$\mathsf{BATE}$ can achieve at least 15\% more profit than the other two.

Due to space limitations, more details of this evaluation, including the the ratio of the
allocated bandwidth to the demanded bandwidth, and the actual data loss ratio, 
are put into  Appendix \ref{testbed-appendix}.

%, where  FFC-1 and FFC-2 
%denote FFC with and without Fixed admission control,  respectively.
%where TEAVAR-1 and TEAVAR-2 are 
%denote TEAVAR with and without Fixed admission control, 
%We can see that 
%Specially, more than 85\% services achieve  95\% availability under $\mathsf{BATE}$, while only about 73\% and 60\%  services achieve this target under TEAVAR and FFC.
%Figure \ref{e2}(c) illustrates three schemes have similar performance for services with small availability target (i.e., $<95\%$).
%However, for services with large availability targets (i.e., $>=99\%)$,  $\mathsf{BATE}$ performs up to  20\% and 30\% better than TEAVAR and FFC.
%The reason for this is that $\mathsf{BATE}$ considers the diverse availability demands, so services with large targets can plunder
%bandwidth from others.
%Compared with TEAVAR, 20\% more applications satisfy their bandwidth demand with $\mathsf{BATE}$.
%Figure  \ref{e2}(c) shows the scheduled results with the same parameters.
%We can see the results approximate to that shown in Figure \ref{e2}(b).
%The gap is narrow, about 5\% on average.
%In the following evaluation, we develop a simulator to perform trace-driven simulations.
%Figure \ref{e2}(d) demonstrates the availability deviation between the testbed and simulation under the same parameters.
%We can see that the error is narrow, i.e., more than 90\% time the difference is less than 5\%.




%
%\begin{equation}
%\forall  j \in J : g_j=
%\begin{cases}
%\alpha &\text{$S_j \ge \beta_j$}\\
%\mu(R_j)  &\text{$S_j < \beta_j$}
%\end{cases}
%\label{availability2}
%\end{equation}




%\textbf{Metrics.}
%We examine the performance of different TE schemes with respect to throughput, availability satisfaction ratio and achieved availability.

%\textbf{Optimization.}
%Our traffic engineering optimization framework uses Gurobi LP solver\cite{gurobi} to solve the optimization problems.


%In this part we will perform experiments on our testbed to evaluate the performance of  $\mathsf{BATE}$.
 \begin{table}
\scriptsize
\centering
\caption{Scheduled results of different schemes.}\label{testbed-1}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{|c|l|l|l|l|ll} \hline
\setlength{\tabcolsep}{10pt}
{Service}&\multicolumn{1}{|c|}{paths}&\multicolumn{1}{|c|}{$\mathsf{BATE}$}&\multicolumn{1}{|c|}{TEAVAR}&\multicolumn{1}{|c|}{FFC}\\
\hline
&{DC1$\to$DC2$\to$DC3}&0&500&0\\
{demand-1}&{DC1$\to$DC4$\to$DC3}&1000&500&250\\
(99.5\%)&{DC1$\to$DC2$\to$DC5$\to$DC4$\to$DC3}&0&0&0\\
&{DC1$\to$DC4$\to$DC5$\to$DC2$\to$DC3}&0&0&0\\
\hline
&{DC1$\to$DC4}&0&250&0\\
{demand-2}&{DC1$\to$DC2$\to$DC5$\to$DC4}&0&0&0\\
(99.9\%)&{DC1$\to$DC2$\to$DC3$\to$DC4}&500&0&250\\
&{DC1$\to$DC6$\to$DC5$\to$DC4}&0&250&250\\
\hline
&{DC1$\to$DC2$\to$DC5}&500&500&750\\
{demand-3}&{DC1$\to$DC4$\to$DC5}&0&250&0\\
(95\%)&{DC1$\to$DC6$\to$DC5}&1000&750&750\\
&{DC1$\to$DC2$\to$DC3$\to$DC4$\to$DC5}&0&0&0\\
%&&&{DC1$\to$DC4$\to$DC5$\to$DC2$\to$DC3}&{750}\\
%&&&{DC1$\to$DC2$\to$DC3}&{0}\\
%&&&{DC1$\to$DC4$\to$DC3}&{0}\\
%{service-1}&&\multirow{2}{*}{{TEAVAR}}&{DC1$\to$DC2$\to$DC3}&{750}\\
%(95\%)&&&{DC1$\to$DC4$\to$DC3}&{750}\\
%&&\multirow{2}{*}{{SWAN}}&{DC1$\to$DC2$\to$DC3}&{1000}\\
%&&&{DC1$\to$DC4$\to$DC3}&{500}\\
%\hline
%&\multirow{6}{*}{500}&\multirow{2}{*}{{$\mathsf{BATE}$}}&{DC1$\to$DC4}&{250}\\
%&&&{DC1$\to$DC2$\to$DC3$\to$DC4}&{250}\\
%{service-2}&&\multirow{2}{*}{{TEAVAR}}&{DC1$\to$DC4}&{250}\\
%(99.99\%)&&&{DC1$\to$DC6$\to$DC5$\to$DC4}&{250}\\
%&&{{SWAN}}&{DC1$\to$DC4}&{500}\\
%\hline
%&\multirow{4}{*}{500}&{{$\mathsf{BATE}$}}&{DC1$\to$DC6$\to$DC5}&{500}\\
%{service-3}&&\multirow{2}{*}{{TEAVAR}}&{DC1$\to$DC6$\to$DC5}&{250}\\
%(90\%)&&&{DC1$\to$DC2$\to$DC5}&{250}\\
%&&{{SWAN}}&{DC1$\to$DC6$\to$DC5}&{500}\\
\hline
\end{tabular}
\end{table}

\textbf{Evaluations on parallel demands.}
Now we use another example with three parallel user demands to illustrate more details of $\mathsf{BATE}$. 
In this evaluation, we also compare with another scheme named BATE-TE, i.e, the traffic scheduling part of $\mathsf{BATE}$, with its fast failure recovery scheme abandoned. 
Demand-1 requires 1000Mbps from DC1 to DC3, demand-2 requires  500Mbps from DC1 to DC4, 
and demand-3 requires 1500Mbps from DC1 to DC5, 
with their availability target set as 99.5\%, 99.9\% and 95\%, respectively. 
We start their traffic simultaneously, assuming all of them have been admitted, 
and their bandwidth on each path, as shown in Table \ref{testbed-1}, 
is determined by different TE algorithms. 
The experiment lasts 100s and is repeated by 100 times.
Figure \ref{e1}(a) shows the fraction of time each bandwidth availability demand is satisfied, 
using the same method as in Figure \ref{e2}(a), i.e, for each second, 
a gap of more than 1\% bandwidth means the demand is not satisfied in that slot. 
It shows that all the three demands can reach their availability targets under $\mathsf{BATE}$,
while TEAVAR and FFC may fail for some users. 
With an investigation on the bandwidth allocation result in Table \ref{testbed-1}, 
we can see that, FFC reserves too much bandwidth for failure recovery, 
so that demand-1 never gets enough bandwidth (250 Mbps allocated v.s. 1500 Mbps demanded), 
and its achieved bandwidth availability is always 0. 
Even it allocates enough bandwidth for demand-2, the achieved bandwidth availability (98.2\%)  
is still lower than required (99.9\%). 
On the other hand, TEAVAR does not make a good match between the link failure probability and the availability users ask for. 
For example, for demand-2, which needs the highest level of availability (99.9\%), 
TEAVAR still allocates 250 Mbps on link L4, which has the highest failure probability (1\%) 
\footnote{In Figure \ref{e1}(b), we plot the actual number of failures that occur in the 100 experiments, 
where L4 fails most frequently.}. 
On the contrary, $\mathsf{BATE}$ matches demands and links well, and does not use L4 for user-2.
%Another factor we must pay attention into is the 

\begin{figure}
\centering
\subfigure[Satisfied BA demands]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/test.pdf}}
\subfigure[Failures of links]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/link-failures.pdf}}
%\subfigure[Scheduled results (SWAN)]{
%\includegraphics[width=0.19\textwidth]{fig/testbed6.pdf}}
%\subfigure[Measured bandwidth (DC1 to DC3)]{
%\includegraphics[width=0.19\textwidth]{fig/evaluation/fake/throughput.pdf}}
%\subfigure[Measured bandwidth (DC1 to DC3)]{
%\includegraphics[width=0.19\textwidth]{fig/evaluation/fake/throughput.pdf}}
%\subfigure[Data loss ratio]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/data-loss.pdf}}
%%\subfigure[After L4 failure ($\mathsf{BATE}$-1)]{
%%\includegraphics[width=0.24\textwidth]{fig/evaluation/time2.pdf}}
%\subfigure[Events after L3 failure]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/fake-failure.pdf}}
\caption{An example with three parallel demands.}
\label{e1}
\end{figure}

%During L4 failure, service2 is unable to gain enough bandwidth. 
%and it will take much time for the controller to retrieve from congestion.
%All links are up under normal circumstances and the probability of this scenario is 98.9\%.
%We now consider a scenario in which DC4-DC5 link fails and other links are up.
%The probability of this failure scenario is about 1\%, which is about 10 $\times$ larger than the probability sum of other failure scenarios.
%We disable the link between DC4 and DC5 at $t=0.2s$ to produce this scenario.
%Figure \ref{e1}(c) shows aggregate bandwidth from DC1 to DC3.
%We can see SWAN drops about 33\% bandwidth between 0.2s and 1.1s.
%The result of service1 is similar.
%\textbf{TBD:Data loss mainly comes from congestion and blackhole, where congestion losses are always link oversubscribed and blackhole losses occur during the time between a link fails and ingress switch rescale.
%Figure \ref{e1}(c) demonstrates $\mathsf{BATE}$ and FFC performs better than   $\mathsf{BATE}$-WT  and TEAVAR for data losses ratio.
%Data losses of Service-2 under TEAVAR is high due to Link L4 failings.
%$\mathsf{BATE}$-WT performs worse than the other three schemes, because it takes much time to derive new resource allocation when network fails and the events time of  $\mathsf{BATE}$-WT shown in Figure \ref{e1}(d) just prove this.}
%We can see that $\mathsf{BATE}$-1 and FFC performs better than   $\mathsf{BATE}$-2  and TEAVAR.
%The losses of TEAVAR mainly comes from link $L4$ failure which is adopt by Service-2.
%When L3 fails, DC1 and DC4 take about 0.4s to detect the failure.
%$\mathsf{BATE}$-1 can  rescale traffic with the backup allocation results, therefore, packet losses will stop immediately.
%Control messages distributing and switch updating take about 0.3s, while the most time-consuming operation is TE computation which takes about 1s, 
%therefore, $\mathsf{BATE}$-2 performs relatively worse than other schemes.
%$\mathsf{BATE}$ considers link failure probability as well as the diverse demands of services \textit{proactively}, as a result, the time consuming events shown in Figure  \ref{e1}(c) rarely happens.






%We can see that SWAN has largest average link utilization, but for links with small failure probability, $\mathsf{BATE}$ is higher used.
%This is because some low probability failure links under $\mathsf{BATE}$ are kept high utilized to guarantee bandwidth demands.

%Figure \ref{topologyutilization}(a) shows the average total link utilization for different schemes when traffic scale is 2.
%We can see that average link utilization of $\mathsf{BATE}$ is about 10\% larger than TEAVAR but 10\% lower than Max-Min.
%Figure \ref{topologyutilization}(b)  shows the average link utilization for links with low failure probability.
%We can see that low failure probability links under $\mathsf{BATE}$ are at least 10\% higher utilized than Max-min scheme.
%This demonstrates that $\mathsf{BATE}$ achieves right balance between link utilization and availability. 
%\begin{figure*}[t]
%\centering
%\subfigure[IBM]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/IBM-admit.pdf}}
%\subfigure[B4 ]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/b4-admit.pdf}}
%\subfigure[ATT]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/att-admit.pdf}}
%\subfigure[Cernet]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/cernet-admit.pdf}}
%\caption{$\mathsf{BATE}$ vs. Fixed admission control scheme under different network topologies.  All requests in (a) and (b) arrive with Poisson Process. Requests in (c) arrive with Gaussian Process. Requests in (d) arrive with Poisson Process whose mean arrival rate is 6.  }
%\label{availability_fig1}
%\end{figure*}

\begin{table}
\centering
\caption{Network topologies used in the simulations}\label{evaluation-topologies}
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline
\setlength{\tabcolsep}{10pt}
\textbf{Topology Name}&\textbf{\#Nodes}&\textbf{\#Links}\\
\hline
IBM&18&48\\
\hline
B4&12&38\\
\hline
ATT&25&112\\
\hline
a national-level backbone&14&32\\
\hline
\end{tabular}
\end{table}

\begin{figure*}[t]
\centering
%\begin{tabular}{c}
%\includegraphics[width=0.7\textwidth]{fig/evaluation/legend.pdf}
%\end{tabular}
%\vfill
\subfigure[Rejection ratio]{
\includegraphics[width=0.3\textwidth]{fig/evaluation/sim-admit.pdf}}
\subfigure[Link utilization]{
\includegraphics[width=0.3\textwidth]{fig/evaluation/link-utilization.pdf}}
\subfigure[Admission delay]{
\includegraphics[width=0.3\textwidth]{fig/evaluation/admission-control-time.pdf}}
\caption{Admission control results in simulations}
\label{availability_fig1}
\end{figure*}


%\begin{figure*}[t]
%\centering
%%\begin{tabular}{c}
%%\includegraphics[width=0.7\textwidth]{fig/evaluation/legend.pdf}
%%\end{tabular}
%%\vfill
%\subfigure[IBM]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/sim-admit.pdf}}
%\subfigure[B4 ]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/att.pdf}}
%\subfigure[ATT]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/cernet3.pdf}}
%\subfigure[Cernet]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/sim-monney.pdf}}
%\caption{$\mathsf{BATE}$  vs. various schemes under different network topologies.}
%\label{availability_fig1}
%\end{figure*}



\begin{figure}[t]
\centering
\subfigure[$\mathsf{BATE}$ v.s. other TE algorithms]{
\includegraphics[width=0.4\textwidth]{fig/evaluation/no-fixed.pdf}}
\subfigure[augmented with \textit{fixed} admission control]{
\includegraphics[width=0.4\textwidth]{fig/evaluation/fixed-shuduling.pdf}}
\caption{BA satisfaction results under simulations}
\label{scheduling-benefit}
\end{figure}




%\begin{figure*} 
%   \begin{minipage}[t]{0.25\linewidth} 
%    \centering 
%    \includegraphics[width=1.8 in]{fig/evaluation/link-utilization.pdf}
%    \caption{Weight difference.} 
%    \label{fig-weight} 
%  \end{minipage}% 
%   \begin{minipage}[t]{0.25\linewidth} 
%    \centering 
%    \includegraphics[width=1.8 in]{fig/evaluation/admission-control-time.pdf}
%    \caption{Weight difference.} 
%    \label{fig-weight} 
%  \end{minipage}% 
%%  \begin{minipage}[t]{0.24\linewidth} 
%%    \centering 
%%    \includegraphics[width=1.7 in]{fig/evaluation/weightnumber.pdf} 
%%    \caption{Important profit.} 
%%    \label{fig-weightnumber} 
%%  \end{minipage}% 
%   \begin{minipage}[t]{0.25\linewidth} 
%    \centering 
%    \includegraphics[width=1.8 in]{fig/evaluation/error.pdf}
%    \caption{Weight difference.} 
%    \label{fig-weight} 
%  \end{minipage}% 
%     \begin{minipage}[t]{0.25\linewidth} 
%    \centering 
%    \includegraphics[width=1.8 in]{fig/evaluation/optimization_time.pdf}
%    \caption{Weight difference.} 
%    \label{fig-weight} 
%  \end{minipage}% 
%  
%\end{figure*}

\subsection{Simulations}
\textbf{Simulation setup.} 
We also conduct simulations on four real network topologies, including B4 \cite{B4}, ATT \cite{Teavar}, IBM\cite{SMORE} and a national-level backbone. 
Table \ref{evaluation-topologies} shows the topology details
\footnote{For B4, ATT and IBM, we get their topologies, link capacities and traffic matrices from the authors of TEAVAR\cite{Teavar}, and for the national backbone, we conduct a direct measurement on it but hide its name due to anonymity requirement.}. 
Following the conclusion on WAN failures in literature studies\cite{Characterization, Teavar}, 
we simulate link failures according to a Weibull distribution with its shape $\lambda=0.8$ and scale $k=0.00001$. 
We generate the demand workload in a similar way to that in the testbed. 
The arrivals of BA demands follow a Poisson process, where the mean arrival rate varies from 1 to 5 in each time slot. 
The duration of each demand follows an exponential distribution, and the mean duration corresponds to 1000 time slots.
The required bandwidth in each user demand is randomly drawn from the traffic metrices (we have collected 200 matrices for each topology) with a proper scale down factor\footnote{We use a factor of 5, and a mean arrival rate around 5 in our simulation corresponds to the normal network load.}, so that between each source-destination pair, multiple users can be served simultaneously. 
The required availability in each user demand is drawn from 10 different SLAs of Azure\cite{azure}.
In our simulations, besides FFC and TEAVAR, we also compare against several other TE algorithms,  
including SWAN \cite{swan}, SMORE \cite{SMORE} and B4 \cite{B4}. 
They have not explicitly considered availability, but pay attention to 
total throughput, link utilization or user fairness. 
These TE algorithms will be activated every 10 time slots.
We assume at most one link failure (i.e., no concurrent failures) in FFC,  
use 99.9\% (which is the maximum value in the user demands) as the default availability target in TEAVAR,
and let SWAN maximize the total throughput of all users. 
With the above settings, each simulation lasts 150,000 slots (corresponding to 100 days if each slot represents 1 minute), and the results achieved by each algorithm on each topology are calculated on 5 independent simulations with different workload traces. 
All the simulations are conducted on a Linux server with a quad-core 2.60GHz processor and 32GB memory.

\iffalse
\textbf{Workload.} 
%We perform trace-driven simulations.
Similar to previous work\cite{OWAN,Guaranteeings,Online-deadline,Teavar}, 
BA demands arrival time is also modeled as Poisson process with arrival rate $\lambda$ per time slot \cite{Guaranteeings,Online-deadline}.
The duration of each demand is modeled as an exponential distribution with a mean of 1000 time slots \cite{Guaranteeings}.
Our collected traffic data contains the capacity of all links as well as 200 bandwidth demands matrices for each topology.
To closely mimic the real traffic, bandwidth of each BA demand is randomly chosen from the real demands matrices, but each scales down 5 $\times$ \cite{Teavar}.

%After a request arriving the system, we run admitted control algorithm to determine whether the request can be accommodated or not.
%As introduced above, $\mathsf{BATE}$ adopts Algorithm \ref{greedy} as its default option.
%We compare Algorithm \ref{greedy} (i.e, Greedy) with fixed algorithm and the optimal algorithm
%The fixed algorithm assumes all admitted requests are fixed and checks whether the remaining network capacity can support the new request.
%The optimal algorithm determines whether the new request can be admitted by solving (\ref{P8}). 
%Every time slot, we randomly generate an integer $p$ between 0 and 10000 for each link.
%If  $p/10000$ is smaller than the failure probability of the link, we make it down to emulate failure.
%Then after 5 time slots, we make the link up again.
%Each experiment lasts 200 time slots.
%The achieved availability of a service is computed as the percentage of qualified time slots, where a slot is regarded as qualified if the total bandwidth is not smaller than demand.
%The achieved availability can be derived by a post-processing simulation composed by network states.
%In each network state, we record the application level throughput ratio as well as the network state probability.
%The sum of network state probability in which the allocated bandwidth larger than the demand is regarded as the achieved availability.

Traffic engineering performs every 10 time slots.
For traffic engineering part, we compare the performance of $\mathsf{BATE}$ against FFC \cite{FFC}, TEAVAR \cite{Teavar}, SWAN \cite{swan}, SMORE \cite{SMORE}, B4 \cite{B4}.
We change them to adapt service level traffic engineering: 
FFC tries to maximize the link utilization with considering 1 fault could occur in our evaluation.
%To fairly compare the ability of the FFC algorithm, we let it
%send the entire demand at the expense of potential degradation in
%availability, unless otherwise stated
%TEAVAR uses the parameter $\beta$ to control the network service availability target
TEAVAR maximizes bandwidth allocation to each demand subject to a single operator-specified availability target.
The default availability target of TEAVAR is $99.9\%$ in our simulations.
SMORE \cite{SMORE} minimizes the maximum link utilization without explicit guarantees on availability.
SWAN \cite{swan}  tries to  maximize the total throughput of all demands in the current slot.
B4 \cite{B4} aims to deliver max-min fair allocation manner to each demand.
\fi

%We compare the failure recovery part with random rerouting mechanism when there are link failures. 
%However, SWAN 
%However, TEAVAR .
%SWAN 
%We can see that DC5 takes about 100ms to detect failure and DC1 hears it within 200ms.
%It takes about 200ms for the controller to derive the new traffic engineering results.
%Switches take about 500ms to update flow rules.
%DC4-DC5 link has large failure probability, so the scenario mentioned above is likely to happen.
%Traditionally,  SDN controller will take reactive interventions when there are failures.
%This will take too much time and QoS might have already been hurt disastrously. 
%This demonstrates that $\mathsf{BATE}$ considers failure scenarios \textit{proactively}.
%Services with large availability target will pass links with low risk probability. 
%Therefore, the availability target can be guaranteed.
%$\mathsf{BATE}$ considers the failure probability of links, thus, the availability demand of applications can be guaranteed.
%\begin{figure*}
%\centering
%\subfigure[Network state coverage]{
%\includegraphics[width=0.32\textwidth]{fig/coverage.pdf}}
%\subfigure[Error]{
%\includegraphics[width=0.32\textwidth]{fig/error.pdf}}
%\subfigure[Solution time]{
%\includegraphics[width=0.32\textwidth]{fig/optimizer_times.pdf}}
%\caption{Impact of pruning and accuracy of solution.}
%\label{purning_fig2}
%\end{figure*}
%As introduced above, $\mathsf{BATE}$ calls for application level volume between each DC pair.
%Our collected traffic data contains the capacity of all links and the  bandwidth demands between each DC pair.
%We don't have real bandwidth demands of each service.
%%We assume there are $|J|=30$ application flows in each DC pair.
%To conform real data center network traffic characters, we divide the original DC-pair bandwidth demands into $|J|$ parts randomly, where the default value of $J$ is 30.
%%Each part can be regarded as an application flow demand.
%Services call for diverse availability demands and we generate them with uniform distribution between 90\% and 99.99\%.  
%There are typically five importance levels in current data centers, i.e., significant,
%important, normal, unimportant and lax.
%We randomly assign weights to services within \{1,2,3,4,5\}with uniform distribution, which is similar to \cite{hanzhang}.
% $\mu(f_{jtk})=\sum_{k\in K, t \in T_k}f_{jtk}/d_j, \forall j \in J$ in our experiments.
%Default  profit is 1 when availability is achieved, otherwise, the profit is $\mu(R_j)=0.01\times R_j$.




%\begin{figure*}[t]
%\centering
%%\subfigure[Weight differentiation]{
%%\includegraphics[width=0.32\textwidth]{fig/evaluation/Geometric.pdf}}
%%\subfigure[Solution comparison]{
%%\includegraphics[width=0.32\textwidth]{fig/evaluation/version.pdf}}
%%\subfigure[Tunnel selection]{
%%\includegraphics[width=0.32\textwidth]{fig/evaluation/routing.pdf}}
%\subfigure[Profit comparison]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/fake/totalprofit.pdf}}
%\subfigure[Target satisfaction percentage]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/weight.pdf}}
%\subfigure[Availability comparison]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/fake/availability-cdf.pdf}}
%%\subfigure[Same importance]{
%%\includegraphics[width=0.24\textwidth]{fig/evaluation/sameweight.pdf}}
%\caption{Performance comparison under different metrics with IBM topology and traffic trace.}
%\label{motivatio_fig2}
%\end{figure*}


%\textbf{Metrics}.
%We evaluate a few different metrics.
%Firstly, we examine request acceptance/reject rate of each request admission control scheme.
%Secondly, we compare the requests satisfied availability targets fraction between $\mathsf{BATE}$ and other engineering schemes.
%Thirdly,  we report other related metrics such as network link utilization, admitted bandwidth, achieved availability and computation overhead. 


%SWAN \cite{swan} divides services into three categories and allocates demands in priority order (i.e., background $<$ elastic $<$ interactive).
%For SWAN, we regard the Significant and Important services as interactive ones, Normal services as elastic ones, and the other services as background ones.
%Nowadays, some ISP networks are designed with worst-case assumptions about failures, so topologies might be over-provisioned. 
%%Therefore, we begin with the real input demands and compute the percentage of applications satisfied availability demands.
%Therefore, we scale up the demands by a factor $s$, which is similar to the technology used in \cite{SMORE, FFC, Teavar}.


%\begin{figure*}
%\centering
%%\begin{tabular}{c}
%%\includegraphics[width=0.7\textwidth]{fig/evaluation/legend.pdf}
%%\end{tabular}
%%\vfill
%\subfigure[Total profit ]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/fake/totalprofit.pdf}}
%\subfigure[Satisfaction fraction details]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/weight.pdf}}
%\subfigure[Profit details ]{
%\includegraphics[width=0.32\textwidth]{fig/evaluation/weightprofit.pdf}}
%\caption{Availability satisfaction fraction of applications with different importance under IBM topology and traffic, where L, U, N, I, S are abbreviations of Lax, Unimportant, Normal, Important and Significant.}
%\label{availability_fig2}
%\end{figure*}


%From the comparison between the scheduled and testbed results, we can see that the scheduled results match the real testbed quite well,
%so in the following part, we will use trace-driven simulation to evaluate the performance of $\mathsf{BATE}$.




%
%
%
%
%
%
%



%\includegraphics[width=0.32\textwidth]{fig/evaluation/fake/totalprofit.pdf}}



%\includegraphics[width=0.24\textwidth]{fig/evaluation/sim-monney.pdf}}




%\begin{figure}[t]
%\begin{center}
%\includegraphics [width=0.9  \columnwidth] {fig/evaluation/fake/availability-cdf.pdf}
%\caption{ $\mathsf{BATE}$ to various TE schemes when network capacity is over-provisioned.}
%\label{sim-importance2}
%\end{center}
%\end{figure}

%\begin{figure}[h]
%\centering
%%\begin{tabular}{c}
%%\includegraphics[width=0.7\textwidth]{fig/evaluation/legend.pdf}
%%\end{tabular}
%\subfigure[Accuracy of greedy algorithm]{
%\includegraphics [width=0.48\columnwidth] {fig/evaluation/link-utilization.pdf}}
%\subfigure[Efficiency comparison]{
%\includegraphics [width=0.48\columnwidth] {fig/evaluation/admission-control-time.pdf}}
%\caption{Error and efficiency comparisons.}
%\label{availability_fig3}
%\end{figure}
%



%\begin{figure}[h]
%\centering
%%\begin{tabular}{c}
%%\includegraphics[width=0.7\textwidth]{fig/evaluation/legend.pdf}
%%\end{tabular}
%%\subfigure[Accuracy of greedy algorithm]{
%%\includegraphics [width=0.48\columnwidth] {fig/evaluation/error.pdf}}
%%\subfigure[Efficiency comparison]{
%%\includegraphics [width=0.48\columnwidth] {fig/evaluation/optimization_time.pdf}}
%%\caption{Error and efficiency comparisons.}
%%\label{availability_fig3}
%%\end{figure}

%
%\begin{figure*} 
%   \begin{minipage}[t]{0.33\linewidth} 
%    \centering 
%    \includegraphics[width=2.4 in] {fig/evaluation/admission-control-time.pdf}}
%    \caption{Weight difference.} 
%    \label{fig-weight} 
%  \end{minipage}% 
%%  \begin{minipage}[t]{0.24\linewidth} 
%%    \centering 
%%    \includegraphics[width=1.7 in]{fig/evaluation/weightnumber.pdf} 
%%    \caption{Important profit.} 
%%    \label{fig-weightnumber} 
%%  \end{minipage}% 
%  \begin{minipage}[t]{0.33\linewidth} 
%    \centering 
%    \includegraphics[width=2.4 in]{fig/evaluation/error.pdf} 
%    \caption{More $\mathsf{BATE}$ versions.} 
%    \label{fig-version} 
%  \end{minipage} 
%   \begin{minipage}[t]{0.33\linewidth} 
%    \centering 
%    \includegraphics[width=2.4 in]{fig/evaluation/optimization_time.pdf}}
%    \caption{More routing schemes.} 
%    \label{routing-version} 
%  \end{minipage} 
%\end{figure*}


%For example, if the bandwidth reservation that is larger than demands in 99.9\% network states, then the sum of the probability for the 99.9\% network states present the achieved availability.
%We care about the following metrics:
%(1) Availability satisfaction fraction. It denotes the percentages of applications whose achieved availability is larger than the corresponding target.
%(2) Total availability profit.  The total availability profit presents the whole performance of the schemes.

\textbf{Evaluation results.}

Figure \ref{availability_fig1} compares, under different demand arrival rates, 
the admission results of $\mathsf{BATE}$  against  
the optimal strategy and the \textit{fixed} one, i.e., step (1) in $\mathsf{BATE}$. 
Figure \ref{availability_fig1}(a) shows that, $\mathsf{BATE}$ falsely rejects at most 4\% demands (using the optimal solution as a baseline), but accepts up to 50\% more demands with a much smaller variance than Fixed. 
It can also utilize at least 10\% higher bandwith than Fixed, as shown in Figure \ref{availability_fig1}(b). 
We also qualify their efficiency by measuring the admission control delay, and Figure \ref{availability_fig1}(c) demonstrates that, $\mathsf{BATE}$ runs at least 30$\times$ faster than directly solving the MILP optimization problem, and always finishes within 1 second. 
%
%\begin{figure}[t]
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/evaluation/sim-monney.pdf}
%\caption{Profit gain after failures}
%\label{sim-profit}
%\end{center}
%\end{figure}

We then compare the traffic scheduling capability of $\mathsf{BATE}$ against FFC, TEAVAR, SWAN, SMORE and B4.
The methodology is similar to the post-processing simulation in TEAVAR \cite{Teavar}, 
where we simulate different failure scenarios according to their probabilities, 
and in each scenario we record the demands that can be satisfied. 
If the \textit{achieved availibility}, i.e., the total posterior probabilities of \textit{qualified} scenarios where a user's bandwidth target is met, is larger than the user's availability target, 
then the BA demand is \textit{satisfied}. 
We plot the overall percentage of satisfied BA demands under each arrival rate (averaged across all simulations) in Figure \ref{scheduling-benefit}(a). $\mathsf{BATE}$ nearly always achieves a satisfaction ratio around 100\%, with a leading margin of at least 23\% (with respect to TEAVAR) under a normal arrival rate (rate=6 in the figure)\footnote{$\mathsf{BATE}$ leads FFC by around 60\%, which is not shown in the figure.}. 
To further demonstrates $\mathsf{BATE}$'s advantage in matching stringent availability requirements  
with reliable links, we further augment each TE algorithm with the \textit{fixed} admission control 
scheme. The satisfaction ratios are plotted in Figure \ref{scheduling-benefit}(b), where $\mathsf{BATE}$
still performs at least 10\% better than the others (rate=6). 

\iffalse
%We write down the bandwidth loss of each scenario for each demand and each scenario's corresponding probability.
The sum of scenario probability where demand is fully satisfied reflects the \textit{achieved availability}.
%If a demand's , then the availability defined in SLA is \textit{satisfaction}.
The term \textit{satisfaction percentage} refers to the fraction of total arrival demands whose achieved availability are higher than their availability targets throughout duration.
Figure \ref{scheduling-benefit}(a) demonstrates that $\mathsf{BATE}$ can even make up to 40\% more demands satisfy SLA availability.
%Demands SLA violation of  $\mathsf{BATE}$ mainly comes from admission control rejection.
%The reason for this is that $\mathsf{BATE}$ adopts admission control and it considers the diverse availability targets of different bandwidth demands.
$\mathsf{BATE}$ can make demands with stringent availability requirements pass links with high reliability, therefore, their availability targets can be guaranteed, but other schemes are unable differentiate demands.
To show this benefit, we adopt \textit{Fixed} admission control for each traffic scheduling algorithm, then Figure \ref{scheduling-benefit}(b) shows $\mathsf{BATE}$ performs at least 10\% better than other algorithms under the \textit{Fixed} admission control method.
\fi
\begin{figure}[t]
\begin{center}
\includegraphics [width=0.8\columnwidth] {fig/evaluation/sim-monney.pdf}
\caption{Profit gain after failures}
\label{sim-profit}
\end{center}
\end{figure}
Figure \ref{sim-profit} shows the average profit after failures occur in the network.
Due to its considration of pricing and refunding,  
$\mathsf{BATE}$ is able to retain 10\%$\sim$20\% more profit than the other algorithms
\footnote{Our preliminary results show $\mathsf{BATE}$ will perform even better than the others under more complicated multi-stage SLAs, especially when the refunding ratio $\mu=1$ under low satisfaction.}.

Remeber that our scaling down factor is 5, so our summarized key results are for a normal network load, where the mean arrival rate in each slot is 5$\sim$6. 
We note that, under heavier loads, $\mathsf{BATE}$ performs even better than its competitors, 
but we regard that as less possible in reality.  


\begin{figure}
\centering
\subfigure[Bandwidth loss in pruning]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/error.pdf}}
\subfigure[Computation time]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/optimization_time.pdf}}
\caption{Impact of pruning on traffic scheduling}
\label{availability_fig3}
\end{figure}

% \begin{figure}
%\begin{center}
%\includegraphics [width=0.7\columnwidth] {fig/evaluation/routing.pdf}
%\caption{Different routing schemes comparison}
%\label{routing-version}
%\end{center}
%\end{figure}
%
% \begin{figure}
%\begin{center}
%\includegraphics [width=0.7\columnwidth] {fig/evaluation/approximation.pdf}
%\caption{Approximation.}
%\label{approximation}
%\end{center}
%\end{figure}


\begin{figure}[t]
   \begin{minipage}[t]{0.5\linewidth} 
    \centering 
    \includegraphics[width=1.6 in]{fig/evaluation/routing.pdf}
    \caption{Routing} 
    \label{routing-version} 
  \end{minipage}% 
   \begin{minipage}[t]{0.5\linewidth} 
    \centering 
    \includegraphics[width=1.6 in]{fig/evaluation/approximation.pdf}
    \caption{Approx ratio} 
    \label{approximation} 
  \end{minipage}
  
\end{figure}



%\begin{figure*} 
%   \begin{minipage}[t]{0.24\linewidth} 
%    \centering 
%    \includegraphics[width=1.6 in]{fig/evaluation/routing.pdf}
%    \caption{Routings.} 
%    \label{routing-version} 
%  \end{minipage}% 
%   \begin{minipage}[t]{0.24\linewidth} 
%    \centering 
%    \includegraphics[width=1.6 in]{fig/evaluation/approximation.pdf}
%    \caption{Approximation.} 
%    \label{approximation} 
%  \end{minipage}
%     \begin{minipage}[t]{0.24\linewidth} 
%    \centering 
%    \includegraphics[width=1.6 in]{fig/evaluation/approximation.pdf}
%    \caption{Approximation.} 
%    \label{approximation} 
%  \end{minipage}
%       \begin{minipage}[t]{0.24\linewidth} 
%    \centering 
%    \includegraphics[width=1.6 in]{fig/evaluation/approximation.pdf}
%    \caption{Approximation.} 
%    \label{approximation} 
%  \end{minipage}
%  
%\end{figure*}

\textbf{Optimality and Robustness.}
In traffic scheduling, $\mathsf{BATE}$ prune scenarios that are unlikely to happen.
%Algorithm \ref{pruning} runs in depth-first manner and when the current network state is smaller than the threshold, the remaining state of the search tree can be ignored.
%Figure \ref{purning_fig2} (a) shows the percentage of network state coverage.
%We can see that even with small cutoff threshold, more than 97\% probability are included.
% In Section \ref{solutions}, we show how to derive the solutions for WAPM problem.
% The errors mainly comes from two parts, i.e., ILP approximate algorithm and pruning.
%In Appendix \ref{B}, we analyze the approximation of the greedy heuristic is 2 in an ideal case.
%Algorithm \ref{greedy} proposes a greedy algorithm to solve $\mathsf{BATE}$ problem.
We compare the bandwidth allocated by $\mathsf{BATE}$ with that allocated by the optimal stragety, i.e., not pruning any scenarios. 
We calculate the bandwidth loss ratio, as well as the running time of $\mathsf{BATE}$, due to such an optimization, under each topology.
%Let $\mathsf{BATE}$ denote the overall allocated bandwidth of all admitted demands with scenario pruning  and $OPT$ denote the allocated bandwidth of optimal solution without scenario pruning. 
%Then the error can be defined as $\frac{|\mathsf{BATE} -OPT|}{|OPT|}$.
Figure \ref{availability_fig3}(a) plots the relative bandwidth loss ratio,  
where the highest number of concurrent link failures varies from 1 to 4. 
This indicates to what extend $\mathsf{BATE}$ will trade accuracy for efficiency. 
We can see the loss ratio is less than 8\% even when no current link failures are considered.
%Traditionally, TE should update the network state every 5-10 minutes.
% and though the powerful mathematical optimization solver such as Gurobi \cite{gurobi} can solve $\mathsf{BATE}$ problem, it might be too slow.
The corresponding computation time is plotted in Figure \ref{availability_fig3}(b), 
where we use Gurobi \cite{gurobi} to solve the pruned LP problem. 
%on the server (4-core, 2.60GHz processor with 32GB ).
We can see that even on a large network (e.g., ATT), 
at most 15 seconds are needed when we consider at most 2 current failures.
%We can see that for large network (e.g., ATT topology) , Gurobi is too slow to converge and the greedy algorithm is necessary.
%While the bruce force algorithm is more than 1000 minutes for each topology and we don't paint them in the picture.
%We can see the result is fast enough to deploy in practice.

By default, we use the K-shortest paths in the network as tunnels for transmission. 
To test the robustness of $\mathsf{BATE}$'s scheduling algorithm,
we further replace K-shortest path routing with oblivious routing\cite{SMORE} and  
edge disjoint path routing\cite{Bruno2013Dynamic}, which have been used by other TE algorithms.  
The BA demand satisfaction ratios are plotted in Figure \ref{routing-version}, 
where there are only minor difference between different tunnel selection algorithms.
Scheduling based on oblivious routing works slightly better than the other two, 
because it finds diverse and low-stretch paths and avoids link over-utilization.

Finally, we compute the approximation ratio of our greedy failure recovery algorithm, 
which is defined as dividing the optimal profit by the profit achieved. 
Figure \ref{approximation} shows that, 
our 2-approximation algorithm achieves a ratio between 1 and 1.25 in real scenarios, 
and the average profit loss is around 10\%. 
This comes with a speedup by at least 100$\times$ under normal load (rate=6). 
 (more details can be seen in Appendix \ref{testbed-appendix}).
%Satisfaction percentage. It denotes the percentages of applications whose achieved availability is larger than the corresponding target.
%Figure \ref{availability_fig1}(b) shows 
%$\mathsf{BATE}$ performs about 20\%, 25\%, 30\%,  40\% better than TEAVAR, SMORE, SWAN, FFC, respectively.
%%Specially, when traffic scale is 3, more than 70\% applications can satisfy their availability demands, which is at least 30\% better than TEAVAR.
%Specially, the advantage of $\mathsf{BATE}$  is more obvious when there are resource competition (e.g., $s=3$).
%%The reason for this is that $\mathsf{BATE}$ considers the bandwidth-based availability demand differentiation of applications.
%This picture also demonstrates that the performance of $\mathsf{BATE}$ doesn't depend on particular network topology and traffic.
%Next, we take IBM trace as the example and perform the experiments.

 
%Figure \ref{motivatio_fig2}(a) shows total profit comparison between $\mathsf{BATE}$ and other schemes.
%We can see that   $\mathsf{BATE}$ performs best and its performance is even up to 30\%, 40\%, 50\%, 75\% better than TEAVAR, SMORE, SWAN and FFC when traffic scale is 3.
%Figure \ref{motivatio_fig2}(b) shows the percentage of services whose availability targets are satisfied for different importance levels when traffic scale is 3. 
%In the picture, L, U, N, I, S are abbreviations of Lax, Unimportant, Normal, Important and Significant, and the error bar paints the mean
%and mean $\pm$ standard deviation value.
%We can see $\mathsf{BATE}$ performs even worse than other TE schemes for the Lax services, but it performs at least 25\% better for the Important and Significant services.
%%We can see that  40\% more important services (i.e., Important, Significant) under $\mathsf{BATE}$ can meet their availability demands.
%%,since the importance of services is taken into consideration under  $\mathsf{BATE}$, while most schemes ignore this.
%Specially, we can see that SWAN \cite{swan} also performs well for the importance services, but it allocates bandwidth only in strict precedence across  three coarse priority classes and  $\mathsf{BATE}$ is  fine-grained priority and can describe the five level of importance.
% $\mathsf{BATE}$ aims to maximize the sum profit of all services, where a service's profit is defined in (\ref{availability}).
%Figure \ref{motivatio_fig2}(c) presents the CDF of achieved availability for different schemes when traffic scale is 3.
%We can see that about 87\%  services's achieved availability are larger than 95\% under  $\mathsf{BATE}$, which is at least 30\% better than other schemes.
%Figure \ref{motivatio_fig2}(a) shows that larger tunnel number can provide slighter higher availability.
%This is because that more alternative tunnels can be used to meet the bandwidth demands.
%Our trace doesn't contain application level traffic matrix, so we divide the original WAN traffic into $|J|=30$ parts randomly.
%Figure \ref{motivatio_fig2}(b) demonstrates the performance with more application division.
%We can see the performance is consistent for various application number.
%





%Figure \ref{availability_fig2} shows availability satisfaction fraction of applications with different importance.
%Figure \ref{availability_fig2}(a) demonstrates the total profit.
%We can see that $\mathsf{BATE}$ gains more 20\% profit than other schemes.
%The reason for this is that more applications under $\mathsf{BATE}$ could meet their availability demands.
%Figure \ref{availability_fig2}(b) and Figure \ref{availability_fig2}(c) show the satisfaction fraction and profit details for applications with different level of importance.


%We use 5-levels to present the importance of applications and we have test the differentiation between the adjacent level.
%In reality, more important levels might be needed to satisfy the user demands.
%Figure \ref{motivatio_fig2}(c) shows the performance with different level number.
%We can see both profit and satisfaction fraction are stale regardless the number of levels.




%In WAN network, the failure probability of a single link differs even by 1000 times.



%\textbf{Path selection.}
%
%
% \begin{figure}
%\begin{center}
%\includegraphics [width=0.8\columnwidth] {fig/evaluation/routing.pdf}
%\caption{Effect of link utilization. }
%\label{routing}
%\end{center}
%\end{figure}




%\begin{figure}
%        \centering
%        \begin{tabular}{c}
%\includegraphics[width=0.45\textwidth]{fig/evaluation/legend2.pdf}
%\end{tabular}  
%       \subfigure[Whole network]{
%         \includegraphics [width=0.23 \textwidth] {fig/topologyutilization.pdf}}
%       \subfigure[Low failure probability]{
%        \includegraphics[width=0.23\textwidth]{fig/topologyutilization2.pdf}}
%     \caption{Link utilization comparison}
%    \label{topologyutilization}
%\end{figure}




%\subsubsection{Performance under different settings}\label{settings}
%%We now evaluate the performance of $\mathsf{BATE}$ under different settings with  IBM topology and trace.
%
%\textbf{Magnitude of weights.}
%In the previous experiments, we set weight within \{1,2,3,4,5\} to present the importance of services. 
%In this part, we evaluate the performance with different magnitude of weights.
%Figure \ref{fig-weight} shows the satisfaction fraction for the Important and Significant services under $\mathsf{BATE}$, when weight difference between the adjacent levels ranges from 1 to 32 (e.g., if the difference is 2, weight set is \{1,3,5,7,9\}).
%We can see that the satisfaction fraction becomes steady after increasing.
%This is because when heightening weight gap,  important services can gain more resources and meet their availability targets with high probability.
%
%%\textbf{Weight number.}
%%Five important levels are used in the experiments shown above.
%%Figure \ref{fig-weightnumber} shows the profit cumulative distribution for important applications (tail 20\%).
%%We can see that more than 80\% important applications achieve their availability targets. 
%%Specially, more important levels can benefit  those important applications.
%
%\textbf{More $\mathsf{BATE}$ solutions.}
%%The default  profit is 1 for applications whose availability demands are satisfied, otherwise the profit is $\mu(R_j)=0.01\times R_j$.
%We denote the $\mathsf{BATE}$ version used in the previous experiment as $\mathsf{BATE}$-A.
%We now define $\mathsf{BATE}$-B, whose gained profit is 1 for $S_j\ge \beta_j$ and 0.75 for other cases.
%$\mathsf{BATE}$-C's gained profit  is 1 for $S_j\ge \beta_j$ and $\mu(f_{jtk})=1-(\beta_j-S_j)$.
%Figure \ref{fig-version} shows the performance comparison for the three schemes.
%We can see that the performance gap between the three algorithms is narrow: less than 5\%.
%This demonstrates $\mathsf{BATE}$ is a general framework that does not depend on the particular profit function.
%
%\textbf{Various routing schemes.}
%So far, we use K-shortest path as the default tunnel selection scheme.
%There are also other tunnel selection schemes, e.g., edge disjoint paths\cite{Bruno2013Dynamic}, oblivious routing\cite{SMORE}.
%Figure \ref{routing-version} shows the comparison with different tunnel selection methods.
%We can see that (1) $\mathsf{BATE}$ performs well regardless the tunnel selection algorithm.
%This indicates that the success of $\mathsf{BATE}$ doesn't reply on the particular routing scheme;
%(2)  Oblivious routing is still slighter better than KSP and edge disjoint paths.
%This is because it is intended to avoid link over-utilization through diverse and low-stretch path selection.
%
%
%\textbf{Link failure probability standard deviation.}
%Weibull distribution is used to generate link failure probability in our simulations.
%Figure \ref{sim-importance2} presents profit comparison under different link failure probability standard deviation.
%We can see that $\mathsf{BATE}$ and TEAVAR perform better under higher link failure probability standard deviation.
%The reason for this is that they both consider link failure probability in their TE formulation. 
%%We can see that all schemes degrade with higher failure probability, but $\mathsf{BATE}$ still performs at least 15\% better than other schemes.
%%Specially, FFC has the smallest influence, since its bandwidth allocation considering at one link failure beforehand.
%
%
%
%
%
%
%
%
%
%
%
%\subsubsection{Solution accuracy}\label{purning}
%Solution error mainly comes from scenario pruning and greedy heuristic algorithm.






\section{Related work}\label{relate}

Optimizing WAN performance is a big challenge. One important topic is on network utilization or fairness. 
For example, early studies focus more on tuning parameters of widely used routing protocols, such as OSPF \cite{OSPF} and MPLS \cite{MATE,Tightrope}, for given traffic matrices. 
Recently, Software defined network (SDN) based technologies, including SWAN\cite{swan}, B4\cite{B4,hong2018b4}, Bwe\cite{bwe} and OWAN\cite{OWAN}, rely on a centralized view to optimize 
bandwidth allocations. 
There are also scheduling algorithms \cite{calendaring,dynamic,D3} using SDN to prioritize WAN traffic.
These work mainly consider aggregated traffic in a macro level, while $\mathsf{BATE}$ handles traffic demands of users. 
%Although these schemes can improve the network utilization, they ignore the \textit{risk} of networks, thus, service availability can't be guaranteed.
%\cite{,evole,Teavar,,FFC,,dynamic,SMORE,,OWAN}.  

As more applications are depolyed in cloud or datacenters, 
many work study how to provide performance guarantee for intra-DC or inter-DC user traffic, 
including flow deadline \cite{D2TCP, LPD, Guaranteeings},  flow rate \cite{appdriven, EyeQ}, etc.
However, they do not provide adquate mechanisms to deal with potential or actual failures. 

Network failures (or uncertainties) have also been considered in various aspects for large scale network evnironments, including design datacenter networks \cite{evole} and optical networks \cite{Optical},  
stochastic models \cite{Stochastic} \cite{bi2019uncertainty-aware}
and failure recovery methods \cite{jointfailure, R3}. 
$\mathsf{BATE}$ studies both proactive and reactive traffic engineering schemes to 
take network failures into account, so that violations on service level agreements can be avoided or mitigated. 
As far as we know, FFC \cite{FFC} and TEAVAR \cite{Teavar} are two pieces of work that are 
most close to $\mathsf{BATE}$, 
in the sense that they also try to provide certain performance guarantee for inter-DC WAN, even under failures. 
However, they have not taken into account the heterogeneities and competitions of user demands, 
and the economic interests of service providers. 


\iffalse
\cite{evole} studies the network failures and proposes the principles to design robust network, but it doesn't contain any TE framework.
\cite{Optical} suggests that backbone traffic engineering strategies should consider current and past optical layer performance.
Neither \cite{evole} nor \cite{Optical} contains TE frameworks.
 \cite{Stochastic} models demand and risks as uncertainty, but it can't provide availability at the particular level (e.g., 90\%).
 \cite{bi2019uncertainty-aware} can adjust traffic according to demands, but it ignores the link failures.
 Most of the traffic engineering schemes work in the \textit{reactive} manner, such as \cite{jointfailure},\cite{R3}.
The reactive methods could take a long time to recover from faults, while applications have already been hurt\cite{FFC}.
FFC \cite{FFC} and TEAVAR\cite{Teavar} are proactive routing protection methods, but they can't ensure the resources of services under network failures.

%In this case, FFC \cite{FFC} advocates to take link failure into consideration when allocating network resources.
%Traffic are freedom from congestion under arbitrary combinations of up to $k$ faults with FFC.
%However, FFC will lead to low link utilization.
%To relieve this, TEAVAR\cite{Teavar} considers the probability of link failure and tries to maximize link utilization under the particular availability target.
%Although TEAVAR tries to fully utilize the link capacity, it ignores the availability difference of applications.
%Moreover, both FFC and TEAVAR uses connectivity-based availability, which can't ensure the resources of applications under network failures.



\textbf{Bandwidth guarantee.}
Up still now, many existing works have already considered to guarantee the bandwidth of applications over inter or intra DCs.
Deadline-aware schemes such as D$^2$TCP\cite{D2TCP}, LPD \cite{LPD}, Amoeba\cite{Guaranteeings}  try to make flows finish before a fixed time constraints,
but they can't perform finely grained rate limits.
Some cloud bandwidth limit technologies such as CloudMirror \cite{appdriven} and EyeQ \cite{EyeQ}, can provide bandwidth guarantees to applications or tenants, while they ignore the network risks and service availability requirements.
$\mathsf{BATE}$ is different from in two aspects.
Firstly, $\mathsf{BATE}$ considers the network failures and tries to perform rate limit under the assumption.
Secondly, $\mathsf{BATE}$ can guarantee bandwidth at a particular level (e.g., 99\%). 
\fi
%\begin{figure}[t]
%\begin{center}
%\includegraphics [width=0.9\columnwidth] {fig/evaluation/error.pdf}
%\caption{Total profits comparison.}
%\label{sim-error}
%\end{center}
%\end{figure}
\section{Conclusion}
We present $\mathsf{BATE}$, a framework that attempts to satisify the heterogeneous bandwidth demands of different users or applications under network failures. 
$\mathsf{BATE}$ is composed of three core components, i.e., admission control, traffic scheduling and failure recovery. They explicitly take failure probabilities into account, while the last 
component also deals with real failures, all in an efficient way. 
Our extensive evaluations show that, it can achieve close to optimal performance guarantee 
and economic profit. 

\iffalse
$\mathsf{BATE}$ aims to optimize the total profits of service subject availability targets.
We design greedy and network scenario pruning algorithm to derive the solution.
We evaluate the performance of $\mathsf{BATE}$ in real testbed as well as trace-driven simulations.
The evaluation results demonstrate that $\mathsf{BATE}$ can improve  bandwidth-based availability performance by up to 50\%.
\fi






\begin{filecontents}{\jobname.bib}

@article{lamport1998the,
title={The part-time parliament},
author={Lamport, Leslie},
journal={ACM Transactions on Computer Systems},
volume={16},
number={2},
pages={133--169},
year={1998}}
    
    
@inproceedings{ARMA, author = {Bagnall, A. J. and Janacek, G. J.}, title = {Clustering Time Series from ARMA Models with Clipped Data}, year = {2004}, isbn = {1581138881}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1014052.1014061}, doi = {10.1145/1014052.1014061}, booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pages = {49-58}, numpages = {10}, keywords = {clustering, time series, ARMA}, location = {Seattle, WA, USA}, series = {KDD '04} }


@inproceedings{Guaranteeings, author = {Zhang, Hong and Chen, Kai and Bai, Wei and Han, Dongsu and Tian, Chen and Wang, Hao and Guan, Haibing and Zhang, Ming}, title = {Guaranteeing Deadlines for Inter-Datacenter Transfers}, year = {2015}, isbn = {9781450332385}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2741948.2741957}, doi = {10.1145/2741948.2741957}, booktitle = {Proceedings of the Tenth European Conference on Computer Systems}, articleno = {20}, numpages = {14}, location = {Bordeaux, France}, series = {EuroSys '15} }


@INPROCEEDINGS{LPD,
  author={H. {Zhang} and X. {Shi} and X. {Yin} and F. {Ren} and Z. {Wang}},
  booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)}, 
  title={More load, more differentiation-- A design principle for deadline-aware congestion control}, 
  year={2015},
  volume={},
  number={},
  pages={127-135},}
  
  
@inproceedings {EyeQ,
author = {Vimalkumar Jeyakumar and Mohammad Alizadeh and David Mazi{\`e}res and Balaji Prabhakar and Albert Greenberg and Changhoon Kim},
title = {EyeQ: Practical Network Performance Isolation at the Edge},
booktitle = {Presented as part of the 10th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 13)},
year = {2013},
isbn = {978-1-931971-00-3},
address = {Lombard, IL},
pages = {297--311},
url = {https://www.usenix.org/conference/nsdi13/technical-sessions/presentation/jeyakumar},
publisher = {{USENIX}},
}
@article{D2TCP, author = {Vamanan, Balajee and Hasan, Jahangir and Vijaykumar, T.N.}, title = {Deadline-Aware Datacenter Tcp (D2TCP)}, year = {2012}, issue_date = {October 2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {42}, number = {4}, issn = {0146-4833}, url = {https://doi.org/10.1145/2377677.2377709}, doi = {10.1145/2377677.2377709}, journal = {SIGCOMM Comput. Commun. Rev.}, month = aug, pages = {115-126}, numpages = {12}, keywords = {ecn, datacenter, oldi, tcp, deadline, cloud services, sla} }



@inproceedings{R3, author = {Wang, Ye and Wang, Hao and Mahimkar, Ajay and Alimi, Richard and Zhang, Yin and Qiu, Lili and Yang, Yang Richard}, title = {R3: Resilient Routing Reconfiguration}, year = {2010}, isbn = {9781450302012}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1851182.1851218}, doi = {10.1145/1851182.1851218}, booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference}, pages = {291-302}, numpages = {12}, keywords = {routing, routing protection, network resiliency}, location = {New Delhi, India}, series = {SIGCOMM '10} }






@article{bi2019uncertainty-aware,
title={Uncertainty-Aware optimization for Network Provisioning and Routing},
author={Bi, Yingjie and Tang, Ao},
pages={1--6},
year={2019}}
    
    
    
@article{Stochastic, author = {Mitra, Debasis and Wang, Qiong}, title = {Stochastic Traffic Engineering for Demand Uncertainty and Risk-Aware Network Revenue Management}, year = {2005}, issue_date = {April 2005}, publisher = {IEEE Press}, volume = {13}, number = {2}, issn = {1063-6692}, url = {https://doi.org/10.1109/TNET.2005.845527}, doi = {10.1109/TNET.2005.845527}, journal = {IEEE/ACM Trans. Netw.}, month = apr, pages = {221-233}, numpages = {13}, keywords = {risk, economics, traffic engineering, mathematical programming, demand uncertainty} }


@inproceedings{Optical, author = {Ghobadi, Monia and Mahajan, Ratul}, title = {Optical Layer Failures in a Large Backbone}, year = {2016}, isbn = {9781450345262}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2987443.2987483}, doi = {10.1145/2987443.2987483}, booktitle = {Proceedings of the 2016 Internet Measurement Conference}, pages = {461-467}, numpages = {7}, keywords = {wide-area backbone network, optical layer, outage, availability, q-factor}, location = {Santa Monica, California, USA}, series = {IMC '16} }



@ARTICLE{OSPF,
  author={B. {Fortz} and M. {Thorup}},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Optimizing OSPF/IS-IS weights in a changing world}, 
  year={2002},
  volume={20},
  number={4},
  pages={756-767},}
  
  
@INPROCEEDINGS{MATE,
  author={A. {Elwalid} and C. {Jin} and S. {Low} and I. {Widjaja}},
  booktitle={Proceedings IEEE INFOCOM 2001. Conference on Computer Communications. Twentieth Annual Joint Conference of the IEEE Computer and Communications Society (Cat. No.01CH37213)}, 
  title={MATE: MPLS adaptive traffic engineering}, 
  year={2001},
  volume={3},
  number={},
  pages={1300-1309 vol.3},}
  
  @inproceedings{Tightrope, author = {Kandula, Srikanth and Katabi, Dina and Davie, Bruce and Charny, Anna}, title = {Walking the Tightrope: Responsive yet Stable Traffic Engineering}, year = {2005}, isbn = {1595930094}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1080091.1080122}, doi = {10.1145/1080091.1080122}, booktitle = {Proceedings of the 2005 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications}, pages = {253-264}, numpages = {12}, keywords = {distributed, online, responsive, TeXCP, traffic engineering, stable}, location = {Philadelphia, Pennsylvania, USA}, series = {SIGCOMM '05} }
  
  
  
@inproceedings{swan,
author    = {Chi{-}Yao Hong and Srikanth Kandula and  Ratul Mahajan and  Ming Zhang and Vijay Gill and Mohan Nanduri and Roger Wattenhofer},
title     = {Achieving high utilization with software-driven {WAN}},
year = {2013},
booktitle = {{ACM} {SIGCOMM} 2013 Conference, SIGCOMM'13, Hong Kong, China, August 12-16, 2013}
}


@inproceedings{hong2018b4, author = {Hong, Chi-Yao and Mandal, Subhasree and Al-Fares, Mohammad and Zhu, Min and Alimi, Richard and B., Kondapa Naidu and Bhagat, Chandan and Jain, Sourabh and Kaimal, Jay and Liang, Shiyu and Mendelev, Kirill and Padgett, Steve and Rabe, Faro and Ray, Saikat and Tewari, Malveeka and Tierney, Matt and Zahn, Monika and Zolla, Jonathan and Ong, Joon and Vahdat, Amin}, title = {B4 and after: Managing Hierarchy, Partitioning, and Asymmetry for Availability and Scale in Google's Software-Defined WAN}, year = {2018}, isbn = {9781450355674}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230543.3230545}, doi = {10.1145/3230543.3230545}, booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication}, pages = {74-87}, numpages = {14}, keywords = {traffic engineering, software-defined WAN}, location = {Budapest, Hungary}, series = {SIGCOMM '18} }

@article{cloudcost, author = {Greenberg, Albert and Hamilton, James and Maltz, David A. and Patel, Parveen}, title = {The Cost of a Cloud: Research Problems in Data Center Networks}, year = {2009}, issue_date = {January 2009}, publisher = {ACM}, address = {New York, NY, USA}, volume = {39}, number = {1}, issn = {0146-4833}, url = {https://doi.org/10.1145/1496091.1496103}, doi = {10.1145/1496091.1496103}, journal = {SIGCOMM Comput. Commun. Rev.}, month = dec, pages = {68-73}, numpages = {6}, keywords = {cloud-service data centers, costs, network challenges} }


@inproceedings{evole,
 author = {Govindan, Ramesh and Minei, Ina and Kallahalla, Mahesh and Koley, Bikash and Vahdat, Amin}, 
 title = {Evolve or Die: High-Availability Design Principles Drawn from Googles Network Infrastructure}, 
 year = {2016},  
 booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference}, 
 series = {SIGCOMM '16} }

@inproceedings{Teavar,
 author = {Bogle, Jeremy and Bhatia, Nikhil and Ghobadi, Manya and Menache, Ishai and Bj\o{}rner, Nikolaj and Valadarsky, Asaf and Schapira, Michael}, title = {TEAVAR: Striking the Right Utilization-Availability Balance in WAN Traffic Engineering}, year = {2019}, isbn = {9781450359566}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341302.3342069}, doi = {10.1145/3341302.3342069}, booktitle = {Proceedings of the ACM Special Interest Group on Data Communication}, pages = {29-43}, numpages = {15}, keywords = {availability, utilization, traffic engineering, network optimization}, location = {Beijing, China}, series = {SIGCOMM '19} }


 
 
@inproceedings{calendaring, author = {Kandula, Srikanth and Menache, Ishai and Schwartz, Roy and Babbula, Spandana Raj}, title = {Calendaring for Wide Area Networks}, year = {2014}, isbn = {9781450328364}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2619239.2626336}, doi = {10.1145/2619239.2626336}, booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM}, pages = {515-526}, numpages = {12}, keywords = {mixed packing covering, deadlines, software-defined networking, wide area network, online temporal planning, inter-datacenter}, location = {Chicago, Illinois, USA}, series = {SIGCOMM '14} }

@inproceedings{FFC, author = {Liu, Hongqiang Harry and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Gelernter, David}, title = {Traffic Engineering with Forward Fault Correction}, year = {2014}, isbn = {9781450328364}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2619239.2626314}, doi = {10.1145/2619239.2626314}, booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM}, pages = {527-538}, numpages = {12}, keywords = {congestion-free, traffic engineering, fault tolerance}, location = {Chicago, Illinois, USA}, series = {SIGCOMM '14} }

@inproceedings{bwe, author = {Kumar, Alok and Jain, Sushant and Naik, Uday and Raghuraman, Anand and Kasinadhuni, Nikhil and Zermeno, Enrique Cauich and Gunn, C. Stephen and Ai, Jing and Carlin, Bj\"{o}rn and Amarandei-Stavila, Mihai and Robin, Mathieu and Siganporia, Aspi and Stuart, Stephen and Vahdat, Amin}, title = {BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing}, year = {2015}, isbn = {9781450335423}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2785956.2787478}, doi = {10.1145/2785956.2787478}, booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication}, pages = {1-14}, numpages = {14}, keywords = {software-defined network, wide-area networks, max-min fair, bandwidth allocation}, location = {London, United Kingdom}, series = {SIGCOMM '15} }

@inproceedings{dynamic, author = {Jalaparti, Virajith and Bliznets, Ivan and Kandula, Srikanth and Lucier, Brendan and Menache, Ishai}, title = {Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers}, year = {2016}, isbn = {9781450341936}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2934872.2934893}, doi = {10.1145/2934872.2934893}, booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference}, pages = {73-86}, numpages = {14}, keywords = {Inter-datacenter networks;, percentile pricing;, deadline scheduling, dynamic pricing;}, location = {Florianopolis, Brazil}, series = {SIGCOMM '16} }

@inproceedings {SMORE,
author = {Praveen Kumar and Yang Yuan and Chris Yu and Nate Foster and Robert Kleinberg and Petr Lapukhov and Chiun Lin Lim and Robert Soul{\'e}},
title = {Semi-Oblivious Traffic Engineering: The Road Not Taken},
booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {157--170},
url = {https://www.usenix.org/conference/nsdi18/presentation/kumar},
publisher = {{USENIX} Association},
month = apr,
}


@inproceedings{B4, author = {Jain, Sushant and Kumar, Alok and Mandal, Subhasree and Ong, Joon and Poutievski, Leon and Singh, Arjun and Venkata, Subbaiah and Wanderer, Jim and Zhou, Junlan and Zhu, Min and Zolla, Jon and H\"{o}lzle, Urs and Stuart, Stephen and Vahdat, Amin}, title = {B4: Experience with a Globally-Deployed Software Defined Wan}, year = {2013}, isbn = {9781450320566}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2486001.2486019}, doi = {10.1145/2486001.2486019}, booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM}, pages = {3-14}, numpages = {12}, keywords = {wide-area networks, openflow, centralized traffic engineering, routing, software- defined networking}, location = {Hong Kong, China}, series = {SIGCOMM '13} }
 
@inproceedings{OWAN, author = {Jin, Xin and Li, Yiran and Wei, Da and Li, Siming and Gao, Jie and Xu, Lei and Li, Guangzhi and Xu, Wei and Rexford, Jennifer}, title = {Optimizing Bulk Transfers with Software-Defined Optical WAN}, year = {2016}, isbn = {9781450341936}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2934872.2934904}, doi = {10.1145/2934872.2934904}, booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference}, pages = {87-100}, numpages = {14}, keywords = {bulk transfers, Software-defined networking, wide area networks, optical networks, cross-layer network management}, location = {Florianopolis, Brazil}, series = {SIGCOMM '16} }

@ARTICLE{cui,  author={Z. {Yang} and Y. {Cui} and X. {Wang} and Y. {Liu} and M. {Li} and S. {Xiao} and C. {Li}},  journal={IEEE/ACM Transactions on Networking},   title={Cost-Efficient Scheduling of Bulk Transfers in Inter-Datacenter WANs},   year={2019},  volume={27},  number={5},  pages={1973-1986},}

@inproceedings{appdriven, author = {Lee, Jeongkeun and Turner, Yoshio and Lee, Myungjin and Popa, Lucian and Banerjee, Sujata and Kang, Joon-Myung and Sharma, Puneet}, title = {Application-Driven Bandwidth Guarantees in Datacenters}, year = {2014}, isbn = {9781450328364}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2619239.2626326}, doi = {10.1145/2619239.2626326}, booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM}, pages = {467-478}, numpages = {12}, keywords = {availability, virtual network, datacenter, application, bandwidth, cloud}, location = {Chicago, Illinois, USA}, series = {SIGCOMM '14} }

@article{tail, author = {Dean, Jeffrey and Barroso, Luiz Andr\'{e}}, title = {The Tail at Scale}, year = {2013}, issue_date = {February 2013}, publisher = {ACM}, address = {New York, NY, USA}, volume = {56}, number = {2}, issn = {0001-0782}, url = {https://doi.org/10.1145/2408776.2408794}, doi = {10.1145/2408776.2408794}, journal = {Commun. ACM}, month = feb, pages = {74-80}, numpages = {7} }


@inproceedings{riskbased, author = {Alipourfard, Omid and Gao, Jiaqi and Koenig, Jeremie and Harshaw, Chris and Vahdat, Amin and Yu, Minlan}, title = {Risk Based Planning of Network Changes in Evolving Data Centers}, year = {2019}, isbn = {9781450368735}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341301.3359664}, doi = {10.1145/3341301.3359664}, booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles}, pages = {414-429}, numpages = {16}, keywords = {network change planning, network compression, network simulations}, location = {Huntsville, Ontario, Canada}, series = {SOSP '19} }

@inproceedings{analysis-osdi, author = {Alquraan, Ahmed and Takruri, Hatem and Alfatafta, Mohammed and Al-Kiswany, Samer}, title = {An Analysis of Network-Partitioning Failures in Cloud Systems}, year = {2018}, isbn = {9781931971478}, publisher = {USENIX Association}, address = {USA}, booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation}, pages = {51-68}, numpages = {18}, location = {Carlsbad, CA, USA}, series = {OSDI'18} }
@Misc{Cato, year = 2020, author = {{cato}}, title = {Cato Managed Services}, howpublished = {\url{https://www.catonetworks.com}}}
@Misc{Aryaka, year = 2020, author = {{aryaka}}, title = {Aryaka Private WAN}, howpublished = {\url{https://www.aryaka.com}}}

@Misc{amazon, year = 2019, author = {{Amazon}}, title = {Amazon Compute Service Level Agreement (2019)}, howpublished = {\url{https://aws.amazon.com/compute/sla/}}}
@Misc{azure, year = 2020, author = {{Azure}}, title = {Microsoft Azure Service Level Agreements (2020)}, howpublished = {\url{https://azure.microsoft.com/en-us/support/legal/sla/summary/}}} 
@Misc{amazon2, year = 2020, author = {{Amazon}}, title = {AWS Database Migration Service (AWS DMS) Service Level Agreement}, howpublished = {\url{https://aws.amazon.com/cn/dms/sla/}}} 

@Misc{amazon3, year = 2020, author = {{Amazon}},title = {Amazon AppFlow Service Level Agreement}, howpublished = {\url{https://aws.amazon.com/cn/appflow/sla/}}} 

@Misc{alibaba2, year = 2020, author = {{Aliababa}},title = {Data Transmission Service Level Agreement}, howpublished = {\url{https://www.alibabacloud.com/help/zh/doc-detail/50079.htm}}} 
@Misc{alibaba3, year = 2020, author = {{Aliababa}},title = {Short Message Service (SMS) Service Level Agreement}, howpublished = {\url{https://www.alibabacloud.com/help/zh/doc-detail/155130.htm}}} 
@Misc{olfd, year = 2020, author = {{Openvswitch}},title = {Openvswitch link failure detection}, howpublished = {\url{https://manpages.debian.org/jessie/openvswitch-switch/ovs-vswitchd.conf.db.5.en.html}}} 
@Misc{floodlight, year = 2020, author = {{floodlight}},title = {Floodlight controller}, howpublished = {\url{https://github.com/floodlight/floodlight}}} 

@Misc{TCM, year = 2020, author = {{Linux}}, title = {Linux Traffic Control}, howpublished = {\url{https://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html}}} 



@Misc{gurobi,year = 2020, author = {{Gurobi}},title = {Gurobi is a powerful mathematical optimization solver}, howpublished = {\url{https://www.gurobi.com}}}
@Misc{openflow, year = 2020, author = {{Openflow}},title = {sdn and openflow}, howpublished = {\url{https://tools.ietf.org/html/rfc7426\#page-23}}}

@Misc{ryu,title = {Ryu}, howpublished = {\url{https://ryu.readthedocs.io/en/latest/ryu_app_api.html}}}

@Misc{random,title = {random}, howpublished = {\url{http://www.cplusplus.com/reference/random/}}}
@Misc{Kermel ,year = 2020, author = {{Kermel}},title = {Linux Kernel}, howpublished = {\url{http://cdn.kernel.org/pub/linux/kernel/v4.x/}}}

@Misc{FDT ,title = {Link Failure Detection}, howpublished = {\url{https://www.juniper.net/documentation/en_US/junos/topics/concept/uplink-failure-detection.html
}}}

@inproceedings{failureslarge, author = {Ghobadi, Monia and Mahajan, Ratul}, title = {Optical Layer Failures in a Large Backbone}, year = {2016}, isbn = {9781450345262}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2987443.2987483}, doi = {10.1145/2987443.2987483}, booktitle = {Proceedings of the 2016 Internet Measurement Conference}, pages = {461-467}, numpages = {7}, keywords = {availability, q-factor, wide-area backbone network, optical layer, outage}, location = {Santa Monica, California, USA}, series = {IMC '16} }

@inproceedings{understanding, author = {Gill, Phillipa and Jain, Navendu and Nagappan, Nachiappan}, title = {Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications}, year = {2011}, isbn = {9781450307970}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2018436.2018477}, doi = {10.1145/2018436.2018477}, booktitle = {Proceedings of the ACM SIGCOMM 2011 Conference}, pages = {350-361}, numpages = {12}, keywords = {data centers, network reliability}, location = {Toronto, Ontario, Canada}, series = {SIGCOMM '11} }

@inproceedings{california, author = {Turner, Daniel and Levchenko, Kirill and Snoeren, Alex C. and Savage, Stefan}, title = {California Fault Lines: Understanding the Causes and Impact of Network Failures}, year = {2010}, isbn = {9781450302012}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1851182.1851220}, doi = {10.1145/1851182.1851220}, booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference}, pages = {315-326}, numpages = {12}, keywords = {failure}, location = {New Delhi, India}, series = {SIGCOMM '10} }

@inproceedings{D3, author = {Wilson, Christo and Ballani, Hitesh and Karagiannis, Thomas and Rowtron, Ant}, title = {Better Never than Late: Meeting Deadlines in Datacenter Networks}, year = {2011}, isbn = {9781450307970}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2018436.2018443}, doi = {10.1145/2018436.2018443}, booktitle = {Proceedings of the ACM SIGCOMM 2011 Conference}, pages = {50-61}, numpages = {12}, keywords = {sla, deadline, online services, datacenter, rate control}, location = {Toronto, Ontario, Canada}, series = {SIGCOMM '11} }

@ARTICLE{hanzhang,
  author={Z. {Wang} and H. {Zhang} and X. {Shi} and X. {Yin} and Y. {Li} and H. {Geng} and Q. {Wu} and J. {Liu}},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Efficient Scheduling of Weighted Coflows in Data Centers}, 
  year={2019},
  volume={30},
  number={9},
  pages={2003-2017},}
  
@inproceedings{jointfailure, author = {Suchara, Martin and Xu, Dahai and Doverspike, Robert and Johnson, David and Rexford, Jennifer}, title = {Network Architecture for Joint Failure Recovery and Traffic Engineering}, year = {2011}, isbn = {9781450308144}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1993744.1993756}, doi = {10.1145/1993744.1993756}, booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems}, pages = {97-108}, numpages = {12}, keywords = {traffic engineering, network architecture, failure recovery, simulation, optimization}, location = {San Jose, California, USA}, series = {SIGMETRICS '11} }

%  @article{overview, author = {Kuipers, F. A.}, title = {An Overview of Algorithms for Network Survivability}, year = {2012}, issue_date = {January 2012}, publisher = {Hindawi Limited}, address = {London, GBR}, volume = {2012}, issn = {2090-4355}, url = {https://doi.org/10.5402/2012/932456}, doi = {10.5402/2012/932456}, journal = {CN}, month = jan, articleno = {24}, numpages = {1} }
%  
%  }



@article{inproceedings2,
title = "The all-or-nothing multicommodity flow problem",
author = "Chandra Chekuri and Sanjeev Khanna and Shepherd, {F. Bruce}",
year = "2004",
month = sep,
day = "29",
language = "English (US)",
pages = "156--165",
journal = "Conference Proceedings of the Annual ACM Symposium on Theory of Computing",
issn = "0734-9025",
publisher = "ACM",
note = "Proceedings of the 36th Annual ACM Symposium on Theory of Computing ; Conference date: 13-06-2004 Through 15-06-2004",
}
@ARTICLE{Characterization,
  author={A. {Markopoulou} and G. {Iannaccone} and S. {Bhattacharyya} and C. {Chuah} and Y. {Ganjali} and C. {Diot}},
  journal={IEEE/ACM Transactions on Networking}, 
  title={Characterization of Failures in an Operational IP Backbone Network}, 
  year={2008},
  volume={16},
  number={4},
  pages={749-762},}

@article{Genova2011Linear,
  title={Linear Integer Programming Methods and Approaches-A Survey},
  author={Genova, Krasimira and Guliashki, Vassil},
  journal={Cybernetics \& Information Technologies},
  volume={11},
  number={1},
  year={2011},
}

@inproceedings{Bruno2013Dynamic,
title={Dynamic risk-aware routing for OSPF networks},
 author={Bruno Vidalenc and Ludovic Noirie and Laurent Ciavaglia and Eric RENAULT},
 booktitle={IEEE International Symposium on Integrated Network Management},
 year={2013},
}

@inproceedings{video1, author = {Krishnan, S. Shunmuga and Sitaraman, Ramesh K.}, title = {Video Stream Quality Impacts Viewer Behavior: Inferring Causality Using Quasi-Experimental Designs}, year = {2012}, isbn = {9781450317054}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2398776.2398799}, doi = {10.1145/2398776.2398799}, booktitle = {Proceedings of the 2012 Internet Measurement Conference}, pages = {211-224}, numpages = {14}, keywords = {causal inference, quasi-experimental design, multimedia, user behavior, internet content delivery, video quality, streaming video}, location = {Boston, Massachusetts, USA}, series = {IMC '12} }

@inproceedings{video2, author = {Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad}, title = {Neural Adaptive Video Streaming with Pensieve}, year = {2017}, isbn = {9781450346535}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3098822.3098843}, doi = {10.1145/3098822.3098843}, booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication}, pages = {197-210}, numpages = {14}, keywords = {reinforcement learning, video streaming, bitrate adaptation}, location = {Los Angeles, CA, USA}, series = {SIGCOMM '17} }

@INPROCEEDINGS{video3,
  author={K. {Spiteri} and R. {Urgaonkar} and R. K. {Sitaraman}},
  booktitle={IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications}, 
  title={BOLA: Near-optimal bitrate adaptation for online videos}, 
  year={2016},
  volume={},
  number={},
  pages={1-9},}
@inproceedings{Pensieve, author = {Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad}, title = {Neural Adaptive Video Streaming with Pensieve}, year = {2017}, isbn = {9781450346535}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3098822.3098843}, doi = {10.1145/3098822.3098843}, booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication}, pages = {197-210}, numpages = {14}, keywords = {video streaming, reinforcement learning, bitrate adaptation}, location = {Los Angeles, CA, USA}, series = {SIGCOMM '17} }

@inproceedings {openvswitch,
author = {Ben Pfaff and Justin Pettit and Teemu Koponen and Ethan Jackson and Andy Zhou and Jarno Rajahalme and Jesse Gross and Alex Wang and Joe Stringer and Pravin Shelar and Keith Amidon and Martin Casado},
title = {The Design and Implementation of Open vSwitch},
booktitle = {12th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 15)},
year = {2015},
isbn = {978-1-931971-218},
address = {Oakland, CA},
pages = {117--130},
url = {https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/pfaff},
publisher = {{USENIX} Association},
month = may,
}


@article{reactive1, author = {Francois, Pierre and Filsfils, Clarence and Evans, John and Bonaventure, Olivier}, title = {Achieving Sub-Second IGP Convergence in Large IP Networks}, year = {2005}, issue_date = {July 2005}, publisher = {ACM}, address = {New York, NY, USA}, volume = {35}, number = {3}, issn = {0146-4833}, url = {https://doi.org/10.1145/1070873.1070877}, doi = {10.1145/1070873.1070877}, abstract = {We describe and analyse in details the various factors that influence the convergence time of intradomain link state routing protocols. This convergence time reflects the time required by a network to react to the failure of a link or a router. To characterise the convergence process, we first use detailed measurements to determine the time required to perform the various operations of a link state protocol on currently deployed routers. We then build a simulation model based on those measurements and use it to study the convergence time in large networks. Our measurements and simulations indicate that sub-second link-state IGP convergence can be easily met on an ISP network without any compromise on stability.}, journal = {SIGCOMM Comput. Commun. Rev.}, month = jul, pages = {35-44}, numpages = {10}, keywords = {convergence time, intradomain routing, IS-IS, OSPF} }


@inproceedings{reactive2, author = {Motiwala, Murtaza and Elmore, Megan and Feamster, Nick and Vempala, Santosh}, title = {Path Splicing}, year = {2008}, isbn = {9781605581750}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1402958.1402963}, doi = {10.1145/1402958.1402963}, abstract = {We present path splicing, a new routing primitive that allows network paths to be constructed by combining multiple routing trees ("slices") to each destination over a single network topology. Path splicing allows traffic to switch trees at any hop en route to the destination. End systems can change the path on which traffic is forwarded by changing a small number of additional bits in the packet header. We evaluate path splicing for intradomain routing using slices generated from perturbed link weights and find that splicing achieves reliability that approaches the best possible using a small number of slices, for only a small increase in latency and no adverse effects on traffic in the network. In the case of interdomain routing, where splicing derives multiple trees from edges in alternate backup routes, path splicing achieves near-optimal reliability and can provide significant benefits even when only a fraction of ASes deploy it. We also describe several other applications of path splicing, as well as various possible deployment paths.}, booktitle = {Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication}, pages = {27-38}, numpages = {12}, keywords = {path diversity, multi-path routing, path splicing}, location = {Seattle, WA, USA}, series = {SIGCOMM '08} }
 



@inproceedings{PCF, author = {Jiang, Chuan and Rao, Sanjay and Tawarmalani, Mohit}, title = {PCF: Provably Resilient Flexible Routing}, year = {2020}, isbn = {9781450379557}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3387514.3405858}, doi = {10.1145/3387514.3405858}, abstract = {Recently, traffic engineering mechanisms have been developed that guarantee that a network (cloud provider WAN, or ISP) does not experience congestion under failures. In this paper, we show that existing congestion-free mechanisms, notably FFC, achieve performance far short of the network's intrinsic capability. We propose PCF, a set of novel congestion-free mechanisms to bridge this gap. PCF achieves these goals by better modeling network structure, and by carefully enhancing the flexibility of network response while ensuring that the performance under failures can be tractably modeled. All of PCF's schemes involve relatively light-weight operations on failures, and many of them can be realized using a local proportional routing scheme similar to FFC. We show PCF's effectiveness through formal theoretical results, and empirical experiments over 21 Internet topologies. PCF's schemes provably out-perform FFC, and in practice, can sustain higher throughput than FFC by a factor of 1.11X to 1.5X on average across the topologies, while providing a benefit of 2.6X in some cases.}, booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication}, pages = {139-153}, numpages = {15}, keywords = {network resilience, network optimization}, location = {Virtual Event, USA}, series = {SIGCOMM '20} }
 
 
 @article{sentinel, author = {Zheng, Jiaqi and Xu, Hong and Zhu, Xiaojun and Chen, Guihai and Geng, Yanhui}, title = {Sentinel: Failure Recovery in Centralized Traffic Engineering}, year = {2019}, issue_date = {October 2019}, publisher = {IEEE Press}, volume = {27}, number = {5}, issn = {1063-6692}, url = {https://doi.org/10.1109/TNET.2019.2931473}, doi = {10.1109/TNET.2019.2931473}, journal = {IEEE/ACM Trans. Netw.}, month = oct, pages = {1859-1872}, numpages = {14} }
 
 @inproceedings{publicore, author = {Harchol, Yotam and Bergemann, Dirk and Feamster, Nick and Friedman, Eric and Krishnamurthy, Arvind and Panda, Aurojit and Ratnasamy, Sylvia and Schapira, Michael and Shenker, Scott}, title = {A Public Option for the Core}, year = {2020}, isbn = {9781450379557}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3387514.3405875}, doi = {10.1145/3387514.3405875}, booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication}, pages = {377–389}, numpages = {13}, keywords = {Network neutrality, Internet infrastructure, Internet transit}, location = {Virtual Event, USA}, series = {SIGCOMM' 20} }

@INPROCEEDINGS{Online-deadline,
  author={L. {Luo} and H. {Yu} and Z. {Ye} and X. {Du}},
  booktitle={IEEE INFOCOM 2018 - IEEE Conference on Computer Communications}, 
  title={Online Deadline-Aware Bulk Transfer Over Inter-Datacenter WANs}, 
  year={2018},
  volume={},
  number={},
  pages={630-638},
  doi={10.1109/INFOCOM.2018.8485828}}
  



 \end{filecontents}
 \bibliographystyle{ACM-Reference-Format}
\bibliography{\jobname}


\clearpage

\begin{appendices}
 \section{The Admission Control Problem} \label{admission_problem}
 
%We now introduce the admission control problem when all the admitted demands can be rescheduled.
%The new arrival demand and all the admitted ones (i.e., $I_a$) constitute request set $I$.
%The Admission control problem  \textit{determines whether availability target of $\forall d \in D$ can be supported simultaneously subject to network failure model}.
%If this is true, the new demand can be admitted, otherwise, it is rejected.
%We begin with the constraints.

For an source-destination pair $k$ of BA demand $d$, let $R_{dk}^{\mathbf{z}}$ denote the ratio of the effective bandwidth under network scenario $\mathbf{z}$ to the demanded bandwidth:
\begin{eqnarray} \label{R}
R_{dk}^{\mathbf{z}}=\frac{\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d}, \quad \forall  d \in D, \mathbf{z}\in \mathbf{z}, k \in K.
\end{eqnarray}
%Figure \ref{Time-fig} gives an illustration on resource allocation return.
%For a given bandwidth allocation vector, it plots the sorted return and the corresponding probability.
where $v_t^{\mathbf{z}}$ represents whether tunnel $t$ is available under network scenario $\mathbf{z}$.

For every source-destination pair $k$, if the total effective bandwidth on all the available tunnels is larger than $\mathbf{b}^k_d$,  then the bandwidth target can be met under $\mathbf{z}$, even some tunnels fail. In this situation, network scenario $\mathbf{z}$ can be regarded as \textit{qualified}.

Let $q_d^{\mathbf{z}}$ denote whether scenario $\mathbf{z}$ is qualified (i.e., $q_d^{\mathbf{z}}=1$) or not (i.e., $q_d^{\mathbf{z}}=0$) for a BA demand $d$:

\iffalse
\begin{equation}
A_d^{\mathbf{z}}=
\begin{cases}
1 &\text{$\forall k \in K:R_{dk}^{\mathbf{z}}\ge 1$}\\
0&\text{Otherwise}
\end{cases}
,\forall  d \in D, \mathbf{z}\in \mathbf{z}.
\label{CA}
\end{equation}
\fi
\begin{equation*}
q_d^{\mathbf{z}}=
\begin{cases}
1 &\text{if $R_{dk} \ge 1$ for every $k \in K$}\\
0 &\text{Otherwise}
\end{cases}
\label{CA}
\end{equation*}

%subject to:
%
%(\ref{constraint-f}), (\ref{constraint-e}),(\ref{R}),  (\ref{CA}),  (\ref{S}), (\ref{availability})
%The optimization problem 
%It attempts to  maximize the bandwidth-based availability for the important applications.
%Therefore, the important applications with large weight would try best to meet the availability targets, while the lax ones will gain as much profit as possible. 


It can be rewritten as
\iffalse
\begin{equation}
\begin{cases}
R_{dk}^{\mathbf{z}} < 1- A_d^{\mathbf{z}}+M\times A_d^{\mathbf{z}},&{\forall d \in D, \mathbf{z} \in Z, k\in K.} \\
R_{dk}^{\mathbf{z}}>=A_d^{\mathbf{z}},&{\forall d \in D, \mathbf{z} \in Z, k\in K.} \\
A_d^{\mathbf{z}} \in\{0,1\},&{\forall d \in D, \mathbf{z} \in Z.} \\
\end{cases}
\label{A-z2}
\end{equation}
\fi
\begin{equation}
\label{A-z2}
\begin{array}{rll}
q_d^{\mathbf{z}} \in &\!\!\{0,1\}, &{\forall d \in \hat{D}, \mathbf{z} \in Z} \\
R_{dk}^{\mathbf{z}} < &\!\! M\times q_d^{\mathbf{z}}+1-q_d^{\mathbf{z}}, &{\forall d \in \hat{D},  k\in K} \\
R_{dk}^{\mathbf{z}} \ge &\!\!  q_d^{\mathbf{z}}, &{\forall d \in \hat{D},  k\in K}
\end{array}
\end{equation}
where $M$ is a constant larger than the upper bound of $R_{dk}^{\mathbf{z}}$.
%,\forall d \in D, k\in K,\mathbf{z} \in Z$.
%It is easily to  prove  (\ref{A-z2}) equals to  (\ref{CA}).
%Here, our consideration is tunnel $t$ might be unavailable (i.e., $v_t^{\mathbf{z}} =0$) under a network scenario, but if $R_{dk}^{\mathbf{z}} \ge 1$ holds, then this scenario is still \textit{qualified}, i.e., 
%$\mathbf{z} \propto <d, \{f_d^t\}> \quad \Leftrightarrow \quad \forall k, A_d^{\mathbf{z}}=1$. 

The achieved bandwidth availability of demand $d$ is the total probabilities of all \textit{qualified} network scenarios, i.e., 
\begin{eqnarray} \label{S}
s_d=\sum_{\mathbf{z}\in \mathbf{z}}q_d^{\mathbf{z}}\times p_{\mathbf{z}},  \quad \forall  d \in D.
\end{eqnarray}

Use $a_d$ to represent whether the BA target of $d$ can be satisfied, which also means $a_d$ can be admitted, then we have 
\iffalse
\begin{equation}
 y_d=
\begin{cases}
1 &\text{$1 >S_d \ge \beta_{d}$}\\
0 &\text{$ \beta_{d} >S_d\ge 0 $}
\end{cases},
\ \ \ \forall  d \in D.
\label{availability4}
\end{equation}
\fi
\begin{equation*}
a_d=
\begin{cases}
1 &\text{if $ \beta_{d} \le s_d \le 1$}\\
0 &\text{if $ 0 \le s_d< \beta_{d} $}
\end{cases}
\label{availability4}
\end{equation*}
%If the achieved bandwidth  availability (i.e., $s_d$) is larger than its desired availability target (i.e., $\beta_d$), the demand can be admitted.
%If it is between $\beta_j^{k}$ and $\beta_j^{k+1}$, then the profit is $\alpha_j^k$, where $1>\alpha_j^1>\alpha_j^2>...>\alpha_j^k$.
%Otherwise, it is rejected, which means the network can't support $d$.
%where $R_j=\sum_{t \in T_j}f_{jt}/d_j$ and $f_{jtab}$.
%\sum_{\forall a,b \in V: t\in T_{ab}}f_{jtab}/d_{jab} 
%$\mu(f_{jtk}) $ aims to ensure that service $j$  will gain as much bandwidth as possible if the network fails to support the availability of service $j$.
which can further written as
\iffalse
\begin{equation}
\begin{cases}
S_d <  \beta_{d}\times  (1-y_d)+y_d,& \forall d \in D. \\
S_d \ge  \beta_{d} \times y_d,& \forall d \in D. \\
y_d\in\{0,1\},& \forall d \in D. \\
\end{cases}
\label{g-2}
\end{equation}
\fi

\begin{equation}
\label{g-2}
\begin{array}{rll}
a_d\in & \!\!\{0,1\},& \forall d \in D \\
s_d < & \!\!\beta_{d}\times  (1-a_d)+a_d,& \forall d \in D \\
s_d \ge & \!\!\beta_{d} \times a_d,& \forall d \in D \\
\end{array}
\end{equation}

%\begin{equation}
%\forall  j \in J : g_j=
%\begin{cases}
%1 &\text{$1>S_j \ge \beta_j^1$}\\
%\alpha_j^1 &\text{$\beta_j^1>S_j \ge \beta_j^2$}\\
%... &\text{$...$}\\
%\mu(R_j)  &\text{$\beta_j^u>S_j \ge0$}
%\end{cases}
%\label{availability}
%\end{equation}
%As TABLE \ref{target} shown, $g_j$ can be used to describe the availability targets defined in SLAs. 
%Applications might have diverse availability demands (i.e., $\beta_j$) and the piecewise formulation also considers this by setting different bound for full profit $\alpha$.
%where $M_2$ is  a big integer that is at least larger than the upper-bound of $S_j,\forall j \in J$.
In addition, the bandwidth allocation result $f_{d}^t$ for BA demand $d$ over tunnel $t$ should be non-negative and limited by link capacities, i.e.,
\begin{equation}
f_{d}^t \ge 0, \quad\forall d \in D ,  k\in K,  t\in T_{k}.
\label{constraint-f}
\end{equation}
and 
\begin{equation}
 \sum_{d \in D}\ \sum_{k\in K, t\in T_{k}}f_{d}^tu_t^e \le c_e, \quad \forall e \in E.
\label{constraint-e}
\end{equation}


Finally, the admission control intends to maximize the total number of accepted demands
with the above constraints, i.e., 
%\begin{eqnarray} \label{goal}
%maximize  \sum_{i\in I} g_i
%\end{eqnarray}
%where $w_j$ presents the importance of service $j$.
%It considers bandwidth-based availability profit and the importance of services simultaneously.
%Note, constraints (\ref{CA}) and  (\ref{availability}) have piecewise functions which are nonlinear.
%We transform them into linear format, then Table \ref{Notation} shows the results.
%
%\begin{lemma}\label{ew}
%The constraints (\ref{constraint-f})-(\ref{availability}) are equivalent to the linear format shown in Table \ref{Notation}.
%\end{lemma}
%The proof details can be found in Appendix \ref{A}.
\begin{equation}
\small
\begin{aligned} \label{P8}
 &maximize  \sum_{d\in D} a_d\\
&\begin{array}{r@{\quad}r@{}l@{\quad}l}
s.t.
(\ref{R}), (\ref{A-z2}), (\ref{S}), (\ref{g-2}), (\ref{constraint-f}), (\ref{constraint-e})
\end{array}
\end{aligned}
\end{equation}
%\begin{cases}
% \sum_{d \in D}\ \sum_{k\in K}\sum_{t\in T_{k}}f_{d}^tu_t^e \le c_e, &\text{$\forall e \in E $.}\\
%R_{dk}^{\mathbf{z}}=\frac{\sum_{t\in T_{k}}f_{d}^tv_t^{\mathbf{z}} }{\mathbf{b}^k_d} , &\text{$\forall  d \in D, \mathbf{z}\in \mathbf{z},  k \in K$.}\\
%S_i=\sum_{\mathbf{z}\in \mathbf{z}}A_d^{\mathbf{z}}\times p_{\mathbf{z}} , & {\forall  d \in D.}\\
%R_{dk}^{\mathbf{z}} < 1- A_d^{\mathbf{z}}+M\times A_d^{\mathbf{z}},&{\forall d \in D, \mathbf{z} \in Z, k\in K.} \\
%R_{dk}^{\mathbf{z}}>=A_d^{\mathbf{z}},&{\forall d \in D, \mathbf{z} \in Z, k\in K.} \\
%S_i <  \beta_{\gamma_{i}}\times  (1-g_i)+g_i,& \forall d \in D. \\
%S_i \ge  \beta_{\gamma_{i}} \times g_i,& \forall d \in D. \\
%f_{d}^t \ge 0,&{\forall d \in D , \forall k\in K, \forall t\in T_{k}}.\\
%A_d^{\mathbf{z}}, g_i\in\{0,1\}, & \forall \mathbf{z} \in \mathbf{z}, d \in D. \\
%\end{cases}\\

\section{Proof of Theorem \ref{NP-222}} \label{appendix-proof}

\begin{proof}
We prove by contradiction. Suppose there is a BA demand that is admitted by Algorithm \ref{greedy-1} but the network is unable to satisfy its bandwidth availability.
There are two possible cases: (i) network bandwidth is insufficient; (ii) The availability provided by the network is not enough. 
Case (i) is impossible, because if bandwidth is insufficient (i.e., $\mathbf{b}^k_d$ is larger than the remaining network capacity for s-d pair $k$) , Algorithm \ref{greedy-1} won't admit the demand (Line 4-5).
Case (ii) is also impossible, because if the bandwidth availability is smaller than its target (i.e., $s_d <   \beta_{d}$) , Algorithm \ref{greedy-1} will reject the demand (Line 14-15).
This completes the proof. 
\end{proof}


\section{Proof of NP-hardness in Failure Recovery} \label{appendix1}

 
\begin{proof}
The all-or-nothing multi-commodity flow problem, which is known to be NP-hard\cite{inproceedings2}, 
can be regarded as a special case of our failure recovery problem (\ref{BATE-recovery}).
Consider an undirected graph $G = (V,E)$ and a set of $k$ source-destination pairs: $s_1t_1, s_2t_2,..., s_kt_k$,
where each pair $s_it_i$ corresponds to a commodity flow to be sent from the source node $s_i$ to the destination node $t_i$ with demand $d_i$.
Let $\mathcal{P}_i$ denote the path set for pair $s_it_i$.
$L_{pe}$ denotes whether path $p$ goes through link $e$ and $f_{ip}$ is the allocation result of commodity $i$ over path $p$.
The  all-or-nothing multi-commodity flow problem tries to find a maximum weight routable set:
\begin{equation}
\begin{aligned} \label{P}
&maximize  \sum_{i=1}^k w_i \times y_i\\
&\begin{array}{r@{\quad}r@{}l@{\quad}l}
s.t. &\forall e \in E: \sum_{i=1}^k\sum_{p\in \mathcal{P}_i}f_{ip}L_{pe} \le c_e \\
     &\forall  1\le i \le k  : y_i=
\begin{cases}
1 &\text{$\sum_{p \in \mathcal{P}_i}f_{ip} \ge d_i$}\\
0 &\text{$\sum_{p\in \mathcal{P}_i}f_{ip} < d_i$}
\end{cases}\\
\end{array}
\end{aligned}
\end{equation}
Where $y_i$ denotes whether commodity flow $i$ is routable.
Consider a special case of the failure recovery problem, where $\mu_d=0$ for every $d$.
This means, if the allocated bandwidth is no less than the demand, 
then the profit is 1, or the profit is 0 otherwise.
We can transform the all-or-nothing multi-commodity flow problem to a special of our failure recovery problem by 
regarding the commodities as the BA demands.
%\textbf{TBD:
%We consider setting the admitted demand from tenants and their bandwidth target in 
%the failure recovery as the multi-commodities and their bandwidth demand in the all-or-nothing multi-commodity flow problem.}
%If we can solve the special case of the admission control problem with a polynomial time algorithm, we would obtain the routable multi-commodity flow set  in the all-or-nothing multi-commodity flow problem.  
Therefore, the failure recovery problem is at least as hard as the all-or-nothing multi-commodity flow
problem, which is known to be NP-hard. This completes the proof.
%If we regard $j \in J$ as a multi-commodity flow pair and the topology as the undirected graph.
%$g_j$ denotes whether multi-commodity flow $j$ is able to be routed.
%Then WAPM-S problem can be seen as a variant of all-or-nothing multi-commodity flow problem and it is NP-hardness.
\end{proof}

\end{appendices}

 
%    
%\begin{proof}
%Constraints  (\ref{S}),and (\ref{availability}) are all step functions.
%Firstly, we change (\ref{S}) into linear format.
%We can rewrite (\ref{S}) as:
%
% \begin{eqnarray} \label{S-2}
%\forall  j \in J :  S_j=\sum_{z\in \mathbf{z}}A_j(\mathbf{z})\times p_{\mathbf{z}}
%\end{eqnarray}
%$A(\mathbf{z})$ is defined as:
%
%\begin{equation}
%\forall  j\in J, z\in \mathbf{z} : A_j(\mathbf{z})=
%\begin{cases}
%1 &\text{$1\le R_j(\mathbf{z})\le M$}\\
%0 &\text{$0\le R_j(\mathbf{z})<1$}
%\end{cases}
%\label{A-z}
%\end{equation}
%where $M$ is larger than the upper bound of $R(\mathbf{z})$.
%(\ref{A-z}) is equivalent to the following format:
%
%\begin{equation}
%\begin{cases}
%A_j(\mathbf{z})+B_j(\mathbf{z}) =1&{\forall j \in J, \mathbf{z} \in Z} \\
%R_j(\mathbf{z}) < B_j(\mathbf{z})+M\times A_j(\mathbf{z})&{\forall j \in J, \mathbf{z} \in Z} \\
%R_j(\mathbf{z})>=A_j(\mathbf{z})&{\forall j \in J, \mathbf{z} \in Z} \\
%A_j(\mathbf{z}),B_j(\mathbf{z})\in\{0,1\}&{\forall j \in J, \mathbf{z} \in Z} \\
%\end{cases}
%\label{A-z2}
%\end{equation}
%
%This is easy to prove since the two cases:
%(1) $A_j(\mathbf{z})=0, B_j(\mathbf{z})=1$ and (2)  $A_j(\mathbf{z})=1, B_j(\mathbf{z})=0$ all hold true.
%
%Secondly, we can change  (\ref{availability}) to:
%
%\begin{equation}
%\begin{cases}
%g_j=\alpha\times k_j+y_j&\forall j \in J \\
% k_j+y_j=1&\forall j \in J \\
%S_j < \beta_j\times k_j+M\times y_j& \forall j \in J \\
%S_j \ge \beta_j \times y_j& \forall j \in J \\
%k_j, y_j\in\{0,1\}& \forall j \in J \\
%\end{cases}
%\label{g-2}
%\end{equation}
%(\ref{g-2}) is equivalent to (\ref{availability}) as $y_j=1$ and $y_j=0$ all hold.
%Take (\ref{S-2}) into (\ref{g-2}), and  combine with (\ref{A-z2}), (\ref{constraint-f}), (\ref{constraint-e}), (\ref{R}), we can derive the following mixed integer linear optimization problem shown in Table \ref{Notation}.
%\end{proof}
%
%\clearpage

     

\begin{appendices}
     \section{Greedy algorithm for failure recovery} \label{B}
     \begin{algorithm}
\KwIn{Input parameters shown in Table \ref{Notation}, a failure scenario $\bm{z}$} 
\KwOut{ $\{f^t_{d}\},F$}
Sort $d\in \hat{D}$ in non-decreasing order with $\frac{g_d}{\sum_{k\in K}\mathbf{b}^k_d}$;\\
$h_d=0, \forall d \in \hat{D}$; \\
$F=\{\}$;\\
\For{$d \in \hat{D}$}{
%$\{f'_{ikt}\},h_i,\{D_e\}\leftarrow$Allocation($F,i,B$);\\
%$h_d \leftarrow$ ;\\
\If { $\bm{z}$' s remaining capacity can support $d$}{
$h_d=1$;\\
    $F=F\cup {d}$;\\
    Update $\{f^t_{d}\}$;\\
   Update $\bm{z}$' s remaining network capacity;\\
}
\Else{
\If{$\sum_{d'\in F}g_{d'} <g_d$}{
\If {network resource allocated to demands in $F$ is able to support $d$}{
	release network resource allocated to demands in $F$;\\
     $F= \{d\}$;\\
    Update $\{f^t_{d}\},\forall d \in \hat{D}$;\\
    Update z's remaining network capacity;\\
$\bm{break}$; \\
}
}
\Else{
$\bm{break}$;
}

}

}
\Return $\{f^t_{d}\},F$
  \caption{Greedy algorithm for failure recovery}
  \label{greedy}
 \end{algorithm}
 
 
%  \begin{algorithm}
%\KwIn{$j,K,\{T_k\},\mathbf{Z},B_e$} 
%\KwOut{ $\{f_{jtk}\},g_j,B_e$}
%$S_j=g_j=0$;\\
%$f_{jtk}=0, \forall k\in K: t \in T_{k}$; \\
%\For{$\mathbf{z} \in \mathbf{Z}$}{
%\If{$S_j \ge \beta_j$}{
%$g_j=1$;\\
%\Return $\{f_{jtk}\},g_j,B_e$;\\
%}
%\Else{
%$r_{jk}=d_{jk}-\sum_{t\in T_{k}:x_t(\mathbf{z})=1}f_{jtk}, \forall k \in K$;\\
%$b_{jk}=\sum_{t\in T_k}\min_{e\in E}u_t^eB_e x_t{(\mathbf{z})},\forall k \in K$;\\
%\If{$\forall k: r_{jk} <=b_{jk}$}{
%\For{$k \in K$}{
%\For{$t\in T_k$}{
%$\gamma_t=\min_{e\in E}u_t^eB_e$ ;\\
%$f_{jkt}=f_{jkt}+\min\{\gamma_t,r_{jk}\}$;\\
%$r_{jk}=r_{jk}-\min\{\gamma_t,r_{jk}\}$;\\
%$B_e=B_e-\min\{\gamma_t,r_{jk}\}u_t^ex_t{(\mathbf{z})}, \forall e$;\\
%}
%
%}
%
%$S_j=S_j+p(\mathbf{z})$;\\
%}
%}
%}
%
%\Return $\{f_{jtk}\},g_j,B_e$;\\
%  \caption{Allocation}
%  \label{bandwidth-allocation}
% \end{algorithm}



Our greedy algorithm to solve the MILP failure recovery problem (\ref{BATE-recovery}) 
is  shown in Algorithm \ref{greedy}, which works as follows. 
Let $F$ denote the BA demands set that derive full profit (i.e. $h_d=1$).
Firstly, it sorts all the accepted demands  $d\in \hat{D}$ in non-decreasing order according to the ratio of demand profit to aggregate bandwidth demands, where the aggregate bandwidth demands are derived as ${\sum_{k\in K}\mathbf{b}^k_d}$ (Line 1).
The ordered sequence prefers demands that have large profit and  small bandwidth.
The algorithm then loops all the ordered admitted demands and tries to allocate resources with remaining network capacity (Line 5-9).
If the network is able to support current demand, then add to $F$ (Line 7).
If the network is unable to support current demand  but  it has larger profit than the total profit of previous ones in $F$,  the algorithm will try to recycle total resources that are allocated to $F$ and test that if allocating total network resources can support current demand  (Line 12-16).
If this is true, then algorithm will prefer current demand, otherwise, the algorithm finishes the iteration (Line 17-18).
%\begin{equation}
%\begin{aligned} \label{LP}
%\begin{array}{r@{\quad}r@{}l@{\quad}l}
%& \quad maximize {\sum_{j\in J:g_j=0} w_j \times\mu(f_{jtk})}\\
%s.t. &{\sum_{j \in J:g_j=0}\sum_{k\in K: t\in T_k}f_{jtk}\times u_t^e \le B_e, \forall e \in E} \\
% \\\end{array}
%\end{aligned}
%\end{equation}
%
%Algorithm \ref{bandwidth-allocation} demonstrates how to allocate resource to service $j$ with remaining capacity $B_e, \forall e \in E$. 
%It loops each network scenario and stops until its achieved bandwidth-based availability is larger than target (Line 4-6).
%In each iteration, it checks whether the network is able to support the bandwidth demand of $j$ under network scenario $\mathbf{z}$ (Line 8-10).
%If this is true, then bandwidth is allocated to this service according to the bottleneck link (Line 12-16).
Compared with the bruce force algorithm, Algorithm \ref{greedy} can derive solution in $O(|\hat{D}||T_k||E|)$ , which is Polynomial time.
However, it achieves this at the cost of performance loss, which is proven as follows.
%Analyzing the performance loss is not focus of this paper and we just take a special case, in which links/nodes are available and $\mu(f_{jtk})=0,\forall j \in J, k\in K: t\in T_k$, as the example and deploy it under a simple network $G(V,E)=\{v_1,v_2, e\}$ with only one link to perform the analysis.
%The following lemma proves the upper approximation bound of the special case.
\begin{lemma}\label{greedy-app}
Algorithm \ref{greedy} achieves 2-approximation for the MILP failure recovery problem.
\end{lemma}

\begin{proof}
Algorithm \ref{greedy} prefers accepted demands according to the following sequence:
 \begin{eqnarray} \label{B-2}
\frac{g_1}{\sum_{k\in K}\mathbf{b}^k_1} \ge \frac{g_2}{\sum_{k\in K}\mathbf{b}^k_2} \ge ....
\end{eqnarray}

(\ref{B-2})  means the priority of flow pair is decided by the unit value.
Withou loss of generality, assume that the network can't transfer the $n+1$ demand,  
Algorithm \ref{greedy} will choose $max\{g_{n+1},\sum_{i=1}^{n}g_i\}$ as the value.
Let $OPT$ denote the optimal solution and it is obvious that  $\sum_{i=1}^{n} g_i\le OPT$.
Also, we have  $\sum_{i=1}^{n+1} g_i\ge OPT$.
This holds, since we've already made the density of network as high as possible by the greedy method.
If we violate the link capacity constraint and put the $n+1$ demand into links, then  links are fulfilled.
There is no other way that the density of links are greater than this, that is, the value is greater than $OPT$. 
 $\sum_{i=1}^{n+1} g_i/2 \le max\{\sum_{i=1}^{n} g_i,g_{n+1}\}$.
 Therefore, $OPT/2 \le max\{\sum_{i=1}^{n} g_i,g_{n+1}\}$. This completes the
proof.
\end{proof}

 \end{appendices}
 
\begin{appendices}
     \section{More evaluation details} \label{testbed-appendix}
     \begin{figure}
\centering
\subfigure[Measured and demand ratio]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/testbed-public-2.pdf}}
\subfigure[Data loss comparison]{
\includegraphics[width=0.23\textwidth]{fig/evaluation/testbed-data-loss-2.pdf}}
%\subfigure[Scheduled results (SWAN)]{
%\includegraphics[width=0.19\textwidth]{fig/testbed6.pdf}}
%\subfigure[Measured bandwidth (DC1 to DC3)]{
%\includegraphics[width=0.19\textwidth]{fig/evaluation/fake/throughput.pdf}}
%\subfigure[Measured bandwidth (DC1 to DC3)]{
%\includegraphics[width=0.19\textwidth]{fig/evaluation/fake/throughput.pdf}}
%\subfigure[Data loss ratio]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/data-loss.pdf}}
%%\subfigure[After L4 failure ($\mathsf{BATE}$-1)]{
%%\includegraphics[width=0.24\textwidth]{fig/evaluation/time2.pdf}}
%\subfigure[Events after L3 failure]{
%\includegraphics[width=0.24\textwidth]{fig/evaluation/fake-failure.pdf}}
\caption{More details of testbed evaluations}
\label{appendix-e1}
\end{figure}

we plot in Figure \ref{appendix-e1}(a), for each algorithm, the ratio of the allocated bandwidth 
to the demanded bandwidth. 
The CDF curve shows that FFC is too conservative in bandwidth allocation, 
and fails to allocate proper bandwidth in almost 60\% time.   
On the other hand, although TEAVAR provides bandwidth fairly well, 
it ignores the diverse availability requirements of different users, 
and achieves a lower satisfaction ratio than $\mathsf{BATE}$. 

%\textbf{TBD: This is because $\mathsf{BATE}$ pre-computes backup allocation with maximal profit and they can be put into practice as far as failures are detected.}

Data loss due to failures is measured according to statistics reported by \textit{iperf} and switches.
As shown in Figure  \ref{appendix-e1}(b),
$\mathsf{BATE}$ and FFC have a slight loss caused by scheduling when failure occurs.
 while TEAVAR has the highest loss, because it might have congestion when rescaling.

\begin{figure}[t]
\begin{center}
\includegraphics [width=0.8\columnwidth] {fig/evaluation/time-consuming.pdf}
\caption{Time ratio}
\label{sim-time}
\end{center}
\end{figure}




We compute the time ratio of the optimal solution and our greedy failure recovery algorithm for each scenario.
Figure \ref{sim-time}  shows that, under normal load (mean arrival rate=5$\sim$6), driving the optimal solution by bruce force is at least 100$\times$ slower than our greedy algorithm.
\end{appendices}






\clearpage



\end{document}
\endinput
